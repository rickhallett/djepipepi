{
  "original_file": "data/audio/secret_to_prompt_engineering_20250326_220619.mp3",
  "text": "So, on that topic, the way to produce creativity or something special, you mentioned writing prompts, and I've heard you talk about the science and the art of prompt engineering. Could you just speak to what it takes to write great prompts? I really do think that philosophy has been weirdly helpful for me here, more than in many other respects. In philosophy, what you're trying to do is convey these very hard concepts. One of the things you are taught is, and I think it is an anti-bullshit device in philosophy. Philosophy is an area where you could have people bullshitting, and you don't want that. It's this desire for extreme clarity. Anyone could just pick up your paper, read it, and know exactly what you're talking about. It's why it can almost be kind of dry. All of the terms are defined, every objection's kind of gone through methodically. It makes sense to me, because when you're in such an a priori domain, clarity is sort of this way that you can prevent people from just kind of making stuff up. I think that's sort of what you have to do with language models. Very often, I actually find myself doing sort of mini versions of philosophy. Suppose that you give me a task, or I have a task for the model, and I want it to pick out a certain kind of question, or identify whether an answer has a certain property. I'll actually sit and be like, let's just give this a name, this property. Suppose I'm trying to tell it, oh, I want you to identify whether this response was rude or polite. That's a whole philosophical question in and of itself, so I have to do as much philosophy as I can in the moment to be like, here's what I mean by rudeness, and here's what I mean by politeness. Then there's another element that's a bit more, I guess, I don't know if this is scientific or empirical. I think it's empirical. I take that description, and then what I want to do is, again, probe the model many times. times. Prompting is very iterative. I think a lot of people, if a prompt is important, they'll iterate on it hundreds or thousands of times. And so you give it the instructions, and then I'm like, what are the edge cases? So if I looked at this, so I try and almost see myself from the position of the model and be like, what is the exact case that I would misunderstand? Or where I would just be like, I don't know what to do in this case. And then I give that case to the model and I see how it responds. And if I think I got it wrong, I add more instructions, or I even add that in as an example. So these very like taking the examples that are right at the edge of what you want and don't want, and putting those into your prompt as like an additional kind of way of describing the thing. And so yeah, in many ways, it just feels like this mix of like, it's really just trying to do clear exposition. And I think I do that because that's how I get clear on things myself. So in many ways, clear prompting for me is often just me understanding what I want. It's like half the task. So I guess that's quite challenging. There's like a laziness that overtakes me if I'm talking to Claude, where I hope Claude just figures it out. So for example, I asked Claude for today to ask some interesting questions. Okay. And the questions that came up, and I think I listed a few sort of interesting counterintuitive and or funny or something like this. All right. And it gave me some pretty good, like it was okay. But I think what I'm hearing you say is like, all right, well, I have to be more rigorous here. I should probably give examples of what I mean by interesting and what I mean by funny or counterintuitive and iteratively build that prompt to better, to get it like what feels like is the right, because it's really, it's a creative act. I'm not asking for factional information. I'm asking to together write with Claude. So I almost have to program using natural language. Yeah, I think that prompting does feel a lot like the kind of the programming using natural language and experimentation or something. It's an odd blend of the two. I do think that for most tasks, so if I just want Claude to do a thing, I think that I am probably more used to knowing how to ask it to avoid common pitfalls or issues that it has. I think these are decreasing a lot over time. But it's also very fine to just ask it for the thing that you want. I think that prompting actually only really becomes relevant when you're really trying to eke out the top 2% of model performance. So for a lot of tasks, I might just, if it gives me an initial list back and there's something I don't like about it, it's kind of generic. For that kind of task, I'd probably just take a bunch of questions that I've had in the past that I've thought worked really well, and I would just give it to the model and then be like, now here's this person I'm talking with. Give me questions of at least that quality. Or I might just ask it for some questions. And then if I was like, oh, these are kind of trite or like, you know, I would just give it that feedback and then hopefully produces a better list. I think that kind of iterative prompting, at that point, your prompt is like a tool that you're going to get so much value out of that you're willing to put in the work. If I was a company making prompts for models, I'm just like, if you're willing to spend a lot of time and resources on the engineering behind what you're building, then the prompt is not something that you should be spending an hour on. It's like, that's a big part of your system. Make sure it's working really well. And so it's only things like that. If I'm using a prompt to classify things or to create data, that's when you're like, it's actually worth just spending a lot of time really thinking it through. What other advice would you give to people that are talking to Claude, sort of generally, more general, because right now we're talking about maybe the edge cases like eking out the 2%. But what in general advice would you give when they show up to Claude trying it for the first time? You know, there's a concern that people over-anthropomorphize models. And I think that's like a very valid concern. I also think that people often under-anthropomorphize them, because sometimes when I see like issues that people have run into with Claude, you know, say Claude is like refusing a task that it shouldn't refuse. But then I look at the text and like the specific wording of what they wrote, and I'm like, I see why Claude did that. And I'm like, if you think through how that looks to Claude, you probably could have just written it in a way that wouldn't evoke such a response. Especially, this is more relevant if you see failures or if you  you see issues. It's sort of like, think about what the model failed at. What did it do wrong? And then maybe that will give you a sense of why. So is it the way that I phrased the thing? And obviously, as models get smarter, you're going to need less of this. And I already see people needing less of it. But that's probably the advice is try to have empathy for the model. Read what you wrote as if you were a person just encountering this for the first time. How does it look to you? And what would have made you behave in the way that the model behaved? So if it misunderstood, what kind of like, what coding language you wanted to use? Is that because like, it was just very ambiguous, and it kind of had to take a guess, in which case next time, you could just be like, hey, make sure this is in Python. Or, I mean, that's the kind of mistake I think models are much less likely to make now. But you know, if you if you do see that kind of mistake, that's, that's probably the advice I'd have. And maybe sort of, I guess, ask questions, why? Or what other details can I provide to help you answer better? Yeah, is that work or no? Yeah, I mean, I've done this with the models, like, it doesn't always work. But like, sometimes I'll just be like, why did you do that? I mean, people underestimate the decrease, which you can really interact with, with models. Like, like, yeah, I'm just like, and sometimes it's really cool word for word, the part that made you and you don't know that it's like fully accurate. But sometimes you do that, and then you change a thing. I mean, also use the models to help me with all of this stuff, I should say, like prompting can end up being a little factory where you're actually building prompts to generate prompts. And so like, yeah, anything where you're like having an issue, asking for suggestions, sometimes just do that. Like you made that error. What could I have said? That's actually not uncommon for me to do. What could I have said that would make you not make that error? Write that out as an instruction. And I'm going to give it to model, I'm going to try it. Sometimes I do that. I give that to the model in another context window. Often, I take the response, I give it to Claude and I'm like, hmm, didn't work. Can you think of anything else? You can play around with these things quite a lot. you",
  "segments": [
    {
      "id": 0,
      "avg_logprob": -0.2817142605781555,
      "compression_ratio": 1.525821566581726,
      "end": 8.84000015258789,
      "no_speech_prob": 0.0006563410861417651,
      "seek": 0,
      "start": 0.0,
      "temperature": 0.0,
      "text": " So, on that topic, the way to produce creativity or something special, you mentioned writing",
      "tokens": [
        50364,
        407,
        11,
        322,
        300,
        4829,
        11,
        264,
        636,
        281,
        5258,
        12915,
        420,
        746,
        2121,
        11,
        291,
        2835,
        3579,
        50806
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_001.mp3"
    },
    {
      "id": 1,
      "avg_logprob": -0.2817142605781555,
      "compression_ratio": 1.525821566581726,
      "end": 15.239999771118164,
      "no_speech_prob": 0.0006563410861417651,
      "seek": 0,
      "start": 8.84000015258789,
      "temperature": 0.0,
      "text": " prompts, and I've heard you talk about the science and the art of prompt engineering.",
      "tokens": [
        50806,
        41095,
        11,
        293,
        286,
        600,
        2198,
        291,
        751,
        466,
        264,
        3497,
        293,
        264,
        1523,
        295,
        12391,
        7043,
        13,
        51126
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_001.mp3"
    },
    {
      "id": 2,
      "avg_logprob": -0.2817142605781555,
      "compression_ratio": 1.525821566581726,
      "end": 20.959999084472656,
      "no_speech_prob": 0.0006563410861417651,
      "seek": 0,
      "start": 15.239999771118164,
      "temperature": 0.0,
      "text": " Could you just speak to what it takes to write great prompts?",
      "tokens": [
        51126,
        7497,
        291,
        445,
        1710,
        281,
        437,
        309,
        2516,
        281,
        2464,
        869,
        41095,
        30,
        51412
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_001.mp3"
    },
    {
      "id": 3,
      "avg_logprob": -0.2817142605781555,
      "compression_ratio": 1.525821566581726,
      "end": 28.200000762939453,
      "no_speech_prob": 0.0006563410861417651,
      "seek": 0,
      "start": 20.959999084472656,
      "temperature": 0.0,
      "text": " I really do think that philosophy has been weirdly helpful for me here, more than in",
      "tokens": [
        51412,
        286,
        534,
        360,
        519,
        300,
        10675,
        575,
        668,
        48931,
        4961,
        337,
        385,
        510,
        11,
        544,
        813,
        294,
        51774
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_001.mp3"
    },
    {
      "id": 4,
      "avg_logprob": -0.3566923141479492,
      "compression_ratio": 1.577114462852478,
      "end": 32.58000183105469,
      "no_speech_prob": 0.00034062250051647425,
      "seek": 2820,
      "start": 28.200000762939453,
      "temperature": 0.0,
      "text": " many other respects.",
      "tokens": [
        50364,
        867,
        661,
        24126,
        13,
        50583
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_001.mp3"
    },
    {
      "id": 5,
      "avg_logprob": -0.3566923141479492,
      "compression_ratio": 1.577114462852478,
      "end": 36.720001220703125,
      "no_speech_prob": 0.00034062250051647425,
      "seek": 2820,
      "start": 32.58000183105469,
      "temperature": 0.0,
      "text": " In philosophy, what you're trying to do is convey these very hard concepts.",
      "tokens": [
        50583,
        682,
        7012,
        29087,
        11,
        437,
        291,
        434,
        1382,
        281,
        360,
        307,
        16965,
        613,
        588,
        1152,
        10392,
        13,
        50790
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_001.mp3"
    },
    {
      "id": 6,
      "avg_logprob": -0.3566923141479492,
      "compression_ratio": 1.577114462852478,
      "end": 43.959999084472656,
      "no_speech_prob": 0.00034062250051647425,
      "seek": 2820,
      "start": 36.720001220703125,
      "temperature": 0.0,
      "text": " One of the things you are taught is, and I think it is an anti-bullshit device in philosophy.",
      "tokens": [
        50790,
        1485,
        295,
        264,
        721,
        291,
        366,
        5928,
        307,
        11,
        293,
        286,
        519,
        309,
        307,
        364,
        6061,
        12,
        37290,
        19186,
        4302,
        294,
        10675,
        13,
        51152
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_001.mp3"
    },
    {
      "id": 7,
      "avg_logprob": -0.3566923141479492,
      "compression_ratio": 1.577114462852478,
      "end": 50.47999954223633,
      "no_speech_prob": 0.00034062250051647425,
      "seek": 2820,
      "start": 43.959999084472656,
      "temperature": 0.0,
      "text": " Philosophy is an area where you could have people bullshitting, and you don't want that.",
      "tokens": [
        51152,
        43655,
        307,
        364,
        1859,
        689,
        291,
        727,
        362,
        561,
        4693,
        2716,
        2414,
        11,
        293,
        291,
        500,
        380,
        528,
        300,
        13,
        51478
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_001.mp3"
    },
    {
      "id": 8,
      "avg_logprob": -0.3566923141479492,
      "compression_ratio": 1.577114462852478,
      "end": 54.91999816894531,
      "no_speech_prob": 0.00034062250051647425,
      "seek": 2820,
      "start": 50.47999954223633,
      "temperature": 0.0,
      "text": " It's this desire for extreme clarity.",
      "tokens": [
        51478,
        467,
        311,
        341,
        7516,
        337,
        8084,
        16992,
        13,
        51700
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_001.mp3"
    },
    {
      "id": 9,
      "avg_logprob": -0.2789285182952881,
      "compression_ratio": 1.6343283653259277,
      "end": 58.2400016784668,
      "no_speech_prob": 0.0006263078539632261,
      "seek": 5492,
      "start": 54.91999816894531,
      "temperature": 0.0,
      "text": " Anyone could just pick up your paper, read it, and know exactly what you're talking about.",
      "tokens": [
        50364,
        14643,
        727,
        445,
        1888,
        493,
        428,
        3035,
        11,
        1401,
        309,
        11,
        293,
        458,
        2293,
        437,
        291,
        434,
        1417,
        466,
        13,
        50530
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_001.mp3"
    },
    {
      "id": 10,
      "avg_logprob": -0.2789285182952881,
      "compression_ratio": 1.6343283653259277,
      "end": 60.08000183105469,
      "no_speech_prob": 0.0006263078539632261,
      "seek": 5492,
      "start": 58.2400016784668,
      "temperature": 0.0,
      "text": " It's why it can almost be kind of dry.",
      "tokens": [
        50530,
        467,
        311,
        983,
        309,
        393,
        1920,
        312,
        733,
        295,
        4016,
        13,
        50622
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_001.mp3"
    },
    {
      "id": 11,
      "avg_logprob": -0.2789285182952881,
      "compression_ratio": 1.6343283653259277,
      "end": 65.5999984741211,
      "no_speech_prob": 0.0006263078539632261,
      "seek": 5492,
      "start": 60.08000183105469,
      "temperature": 0.0,
      "text": " All of the terms are defined, every objection's kind of gone through methodically.",
      "tokens": [
        50622,
        1057,
        295,
        264,
        2115,
        366,
        7642,
        11,
        633,
        35756,
        311,
        733,
        295,
        2780,
        807,
        3170,
        984,
        13,
        50898
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_001.mp3"
    },
    {
      "id": 12,
      "avg_logprob": -0.2789285182952881,
      "compression_ratio": 1.6343283653259277,
      "end": 71.55999755859375,
      "no_speech_prob": 0.0006263078539632261,
      "seek": 5492,
      "start": 65.5999984741211,
      "temperature": 0.0,
      "text": " It makes sense to me, because when you're in such an a priori domain, clarity is sort",
      "tokens": [
        50898,
        467,
        1669,
        2020,
        281,
        385,
        11,
        570,
        562,
        291,
        434,
        294,
        1270,
        364,
        257,
        4059,
        72,
        9274,
        11,
        16992,
        307,
        1333,
        51196
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_001.mp3"
    },
    {
      "id": 13,
      "avg_logprob": -0.2789285182952881,
      "compression_ratio": 1.6343283653259277,
      "end": 79.36000061035156,
      "no_speech_prob": 0.0006263078539632261,
      "seek": 5492,
      "start": 71.55999755859375,
      "temperature": 0.0,
      "text": " of this way that you can prevent people from just kind of making stuff up.",
      "tokens": [
        51196,
        295,
        341,
        636,
        300,
        291,
        393,
        4871,
        561,
        490,
        445,
        733,
        295,
        1455,
        1507,
        493,
        13,
        51586
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_001.mp3"
    },
    {
      "id": 14,
      "avg_logprob": -0.2789285182952881,
      "compression_ratio": 1.6343283653259277,
      "end": 82.63999938964844,
      "no_speech_prob": 0.0006263078539632261,
      "seek": 5492,
      "start": 79.36000061035156,
      "temperature": 0.0,
      "text": " I think that's sort of what you have to do with language models.",
      "tokens": [
        51586,
        286,
        519,
        300,
        311,
        1333,
        295,
        437,
        291,
        362,
        281,
        360,
        365,
        2856,
        5245,
        13,
        51750
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_001.mp3"
    },
    {
      "id": 15,
      "avg_logprob": -0.2572021186351776,
      "compression_ratio": 1.7854671478271484,
      "end": 87.44000244140625,
      "no_speech_prob": 0.033581387251615524,
      "seek": 8264,
      "start": 82.63999938964844,
      "temperature": 0.0,
      "text": " Very often, I actually find myself doing sort of mini versions of philosophy.",
      "tokens": [
        50364,
        4372,
        2049,
        11,
        286,
        767,
        915,
        2059,
        884,
        1333,
        295,
        8382,
        9606,
        295,
        10675,
        13,
        50604
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_001.mp3"
    },
    {
      "id": 16,
      "avg_logprob": -0.2572021186351776,
      "compression_ratio": 1.7854671478271484,
      "end": 91.68000030517578,
      "no_speech_prob": 0.033581387251615524,
      "seek": 8264,
      "start": 87.44000244140625,
      "temperature": 0.0,
      "text": " Suppose that you give me a task, or I have a task for the model, and I want it to pick",
      "tokens": [
        50604,
        21360,
        300,
        291,
        976,
        385,
        257,
        5633,
        11,
        420,
        286,
        362,
        257,
        5633,
        337,
        264,
        2316,
        11,
        293,
        286,
        528,
        309,
        281,
        1888,
        50816
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_001.mp3"
    },
    {
      "id": 17,
      "avg_logprob": -0.2572021186351776,
      "compression_ratio": 1.7854671478271484,
      "end": 96.5999984741211,
      "no_speech_prob": 0.033581387251615524,
      "seek": 8264,
      "start": 91.68000030517578,
      "temperature": 0.0,
      "text": " out a certain kind of question, or identify whether an answer has a certain property.",
      "tokens": [
        50816,
        484,
        257,
        1629,
        733,
        295,
        1168,
        11,
        420,
        5876,
        1968,
        364,
        1867,
        575,
        257,
        1629,
        4707,
        13,
        51062
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_001.mp3"
    },
    {
      "id": 18,
      "avg_logprob": -0.2572021186351776,
      "compression_ratio": 1.7854671478271484,
      "end": 101.91999816894531,
      "no_speech_prob": 0.033581387251615524,
      "seek": 8264,
      "start": 96.5999984741211,
      "temperature": 0.0,
      "text": " I'll actually sit and be like, let's just give this a name, this property.",
      "tokens": [
        51062,
        286,
        603,
        767,
        1394,
        293,
        312,
        411,
        11,
        718,
        311,
        445,
        976,
        341,
        257,
        1315,
        11,
        341,
        4707,
        13,
        51328
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_001.mp3"
    },
    {
      "id": 19,
      "avg_logprob": -0.2572021186351776,
      "compression_ratio": 1.7854671478271484,
      "end": 105.0,
      "no_speech_prob": 0.033581387251615524,
      "seek": 8264,
      "start": 101.91999816894531,
      "temperature": 0.0,
      "text": " Suppose I'm trying to tell it, oh, I want you to identify whether this response was",
      "tokens": [
        51328,
        21360,
        286,
        478,
        1382,
        281,
        980,
        309,
        11,
        1954,
        11,
        286,
        528,
        291,
        281,
        5876,
        1968,
        341,
        4134,
        390,
        51482
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_001.mp3"
    },
    {
      "id": 20,
      "avg_logprob": -0.2572021186351776,
      "compression_ratio": 1.7854671478271484,
      "end": 107.0,
      "no_speech_prob": 0.033581387251615524,
      "seek": 8264,
      "start": 105.0,
      "temperature": 0.0,
      "text": " rude or polite.",
      "tokens": [
        51482,
        18895,
        420,
        25171,
        13,
        51582
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_001.mp3"
    },
    {
      "id": 21,
      "avg_logprob": -0.2572021186351776,
      "compression_ratio": 1.7854671478271484,
      "end": 110.44000244140625,
      "no_speech_prob": 0.033581387251615524,
      "seek": 8264,
      "start": 107.0,
      "temperature": 0.0,
      "text": " That's a whole philosophical question in and of itself, so I have to do as much philosophy",
      "tokens": [
        51582,
        663,
        311,
        257,
        1379,
        25066,
        1168,
        294,
        293,
        295,
        2564,
        11,
        370,
        286,
        362,
        281,
        360,
        382,
        709,
        10675,
        51754
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_001.mp3"
    },
    {
      "id": 22,
      "avg_logprob": -0.23599937558174133,
      "compression_ratio": 1.6839377880096436,
      "end": 113.55999755859375,
      "no_speech_prob": 0.004398670047521591,
      "seek": 11044,
      "start": 110.44000244140625,
      "temperature": 0.0,
      "text": " as I can in the moment to be like, here's what I mean by rudeness, and here's what",
      "tokens": [
        50364,
        382,
        286,
        393,
        294,
        264,
        1623,
        281,
        312,
        411,
        11,
        510,
        311,
        437,
        286,
        914,
        538,
        32109,
        15264,
        11,
        293,
        510,
        311,
        437,
        50520
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_001.mp3"
    },
    {
      "id": 23,
      "avg_logprob": -0.23599937558174133,
      "compression_ratio": 1.6839377880096436,
      "end": 115.91999816894531,
      "no_speech_prob": 0.004398670047521591,
      "seek": 11044,
      "start": 113.55999755859375,
      "temperature": 0.0,
      "text": " I mean by politeness.",
      "tokens": [
        50520,
        286,
        914,
        538,
        2453,
        15264,
        13,
        50638
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_001.mp3"
    },
    {
      "id": 24,
      "avg_logprob": -0.23599937558174133,
      "compression_ratio": 1.6839377880096436,
      "end": 123.55999755859375,
      "no_speech_prob": 0.004398670047521591,
      "seek": 11044,
      "start": 115.91999816894531,
      "temperature": 0.0,
      "text": " Then there's another element that's a bit more, I guess, I don't know if this is scientific",
      "tokens": [
        50638,
        1396,
        456,
        311,
        1071,
        4478,
        300,
        311,
        257,
        857,
        544,
        11,
        286,
        2041,
        11,
        286,
        500,
        380,
        458,
        498,
        341,
        307,
        8134,
        51020
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_001.mp3"
    },
    {
      "id": 25,
      "avg_logprob": -0.23599937558174133,
      "compression_ratio": 1.6839377880096436,
      "end": 124.55999755859375,
      "no_speech_prob": 0.004398670047521591,
      "seek": 11044,
      "start": 123.55999755859375,
      "temperature": 0.0,
      "text": " or empirical.",
      "tokens": [
        51020,
        420,
        31886,
        13,
        51070
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_001.mp3"
    },
    {
      "id": 26,
      "avg_logprob": -0.23599937558174133,
      "compression_ratio": 1.6839377880096436,
      "end": 125.55999755859375,
      "no_speech_prob": 0.004398670047521591,
      "seek": 11044,
      "start": 124.55999755859375,
      "temperature": 0.0,
      "text": " I think it's empirical.",
      "tokens": [
        51070,
        286,
        519,
        309,
        311,
        31886,
        13,
        51120
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_001.mp3"
    },
    {
      "id": 27,
      "avg_logprob": -0.23599937558174133,
      "compression_ratio": 1.6839377880096436,
      "end": 131.0800018310547,
      "no_speech_prob": 0.004398670047521591,
      "seek": 11044,
      "start": 125.55999755859375,
      "temperature": 0.0,
      "text": " I take that description, and then what I want to do is, again, probe the model many times.",
      "tokens": [
        51120,
        286,
        747,
        300,
        3855,
        11,
        293,
        550,
        437,
        286,
        528,
        281,
        360,
        307,
        11,
        797,
        11,
        22715,
        264,
        2316,
        867,
        1413,
        13,
        51396
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_001.mp3"
    },
    {
      "id": 0,
      "avg_logprob": -0.2601698637008667,
      "compression_ratio": 1.7625417709350586,
      "end": 136.29000282287598,
      "no_speech_prob": 0.00045118702109903097,
      "seek": 0,
      "start": 131.0500030517578,
      "temperature": 0.0,
      "text": " times. Prompting is very iterative. I think a lot of people, if a prompt is important,",
      "tokens": [
        50364,
        1413,
        13,
        15833,
        662,
        278,
        307,
        588,
        17138,
        1166,
        13,
        286,
        519,
        257,
        688,
        295,
        561,
        11,
        498,
        257,
        12391,
        307,
        566,
        2707,
        394,
        11,
        50626
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_002.mp3"
    },
    {
      "id": 1,
      "avg_logprob": -0.2601698637008667,
      "compression_ratio": 1.7625417709350586,
      "end": 141.85000324249268,
      "no_speech_prob": 0.00045118702109903097,
      "seek": 0,
      "start": 136.29000282287598,
      "temperature": 0.0,
      "text": " they'll iterate on it hundreds or thousands of times. And so you give it the instructions,",
      "tokens": [
        50626,
        436,
        603,
        44497,
        322,
        309,
        6779,
        420,
        5383,
        295,
        1413,
        13,
        400,
        370,
        291,
        976,
        309,
        264,
        9415,
        11,
        50904
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_002.mp3"
    },
    {
      "id": 2,
      "avg_logprob": -0.2601698637008667,
      "compression_ratio": 1.7625417709350586,
      "end": 147.9300022125244,
      "no_speech_prob": 0.00045118702109903097,
      "seek": 0,
      "start": 141.85000324249268,
      "temperature": 0.0,
      "text": " and then I'm like, what are the edge cases? So if I looked at this, so I try and almost",
      "tokens": [
        50904,
        293,
        550,
        286,
        478,
        411,
        11,
        437,
        366,
        264,
        4691,
        3331,
        30,
        407,
        498,
        286,
        2956,
        412,
        341,
        11,
        370,
        286,
        853,
        293,
        1920,
        51208
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_002.mp3"
    },
    {
      "id": 3,
      "avg_logprob": -0.2601698637008667,
      "compression_ratio": 1.7625417709350586,
      "end": 151.81000328063965,
      "no_speech_prob": 0.00045118702109903097,
      "seek": 0,
      "start": 147.9300022125244,
      "temperature": 0.0,
      "text": " see myself from the position of the model and be like, what is the exact case that I",
      "tokens": [
        51208,
        536,
        2059,
        490,
        264,
        2535,
        295,
        264,
        2316,
        293,
        312,
        411,
        11,
        437,
        307,
        264,
        1900,
        1389,
        300,
        286,
        51402
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_002.mp3"
    },
    {
      "id": 4,
      "avg_logprob": -0.2601698637008667,
      "compression_ratio": 1.7625417709350586,
      "end": 155.37000274658203,
      "no_speech_prob": 0.00045118702109903097,
      "seek": 0,
      "start": 151.81000328063965,
      "temperature": 0.0,
      "text": " would misunderstand? Or where I would just be like, I don't know what to do in this case.",
      "tokens": [
        51402,
        576,
        35736,
        30,
        1610,
        689,
        286,
        576,
        445,
        312,
        411,
        11,
        286,
        500,
        380,
        458,
        437,
        281,
        360,
        294,
        341,
        1389,
        13,
        51580
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_002.mp3"
    },
    {
      "id": 5,
      "avg_logprob": -0.2601698637008667,
      "compression_ratio": 1.7625417709350586,
      "end": 158.85000228881836,
      "no_speech_prob": 0.00045118702109903097,
      "seek": 0,
      "start": 155.37000274658203,
      "temperature": 0.0,
      "text": " And then I give that case to the model and I see how it responds. And if I think I got",
      "tokens": [
        51580,
        400,
        550,
        286,
        976,
        300,
        1389,
        281,
        264,
        2316,
        293,
        286,
        536,
        577,
        309,
        27331,
        13,
        400,
        498,
        286,
        519,
        286,
        658,
        51754
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_002.mp3"
    },
    {
      "id": 6,
      "avg_logprob": -0.239130437374115,
      "compression_ratio": 1.8108108043670654,
      "end": 163.8900032043457,
      "no_speech_prob": 0.00038595980731770396,
      "seek": 2780,
      "start": 158.85000228881836,
      "temperature": 0.0,
      "text": " it wrong, I add more instructions, or I even add that in as an example. So these very like",
      "tokens": [
        50364,
        309,
        2085,
        11,
        286,
        909,
        544,
        9415,
        11,
        420,
        286,
        754,
        909,
        300,
        294,
        382,
        364,
        1365,
        13,
        407,
        613,
        588,
        411,
        50616
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_002.mp3"
    },
    {
      "id": 7,
      "avg_logprob": -0.239130437374115,
      "compression_ratio": 1.8108108043670654,
      "end": 167.6500015258789,
      "no_speech_prob": 0.00038595980731770396,
      "seek": 2780,
      "start": 163.8900032043457,
      "temperature": 0.0,
      "text": " taking the examples that are right at the edge of what you want and don't want, and",
      "tokens": [
        50616,
        1940,
        264,
        5110,
        300,
        366,
        558,
        412,
        264,
        4691,
        295,
        437,
        291,
        528,
        293,
        500,
        380,
        528,
        11,
        293,
        50804
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_002.mp3"
    },
    {
      "id": 8,
      "avg_logprob": -0.239130437374115,
      "compression_ratio": 1.8108108043670654,
      "end": 172.69000244140625,
      "no_speech_prob": 0.00038595980731770396,
      "seek": 2780,
      "start": 167.6500015258789,
      "temperature": 0.0,
      "text": " putting those into your prompt as like an additional kind of way of describing the thing.",
      "tokens": [
        50804,
        3372,
        729,
        666,
        428,
        12391,
        382,
        411,
        364,
        4497,
        733,
        295,
        636,
        295,
        16141,
        264,
        551,
        13,
        51056
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_002.mp3"
    },
    {
      "id": 9,
      "avg_logprob": -0.239130437374115,
      "compression_ratio": 1.8108108043670654,
      "end": 177.01000213623047,
      "no_speech_prob": 0.00038595980731770396,
      "seek": 2780,
      "start": 172.69000244140625,
      "temperature": 0.0,
      "text": " And so yeah, in many ways, it just feels like this mix of like, it's really just trying",
      "tokens": [
        51056,
        400,
        370,
        1338,
        11,
        294,
        867,
        2098,
        11,
        309,
        445,
        3417,
        411,
        341,
        2890,
        295,
        411,
        11,
        309,
        311,
        534,
        445,
        1382,
        51272
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_002.mp3"
    },
    {
      "id": 10,
      "avg_logprob": -0.239130437374115,
      "compression_ratio": 1.8108108043670654,
      "end": 183.25000381469727,
      "no_speech_prob": 0.00038595980731770396,
      "seek": 2780,
      "start": 177.01000213623047,
      "temperature": 0.0,
      "text": " to do clear exposition. And I think I do that because that's how I get clear on things myself.",
      "tokens": [
        51272,
        281,
        360,
        1850,
        1278,
        5830,
        13,
        400,
        286,
        519,
        286,
        360,
        300,
        570,
        300,
        311,
        577,
        286,
        483,
        1850,
        322,
        721,
        2059,
        13,
        51584
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_002.mp3"
    },
    {
      "id": 11,
      "avg_logprob": -0.239130437374115,
      "compression_ratio": 1.8108108043670654,
      "end": 188.4900016784668,
      "no_speech_prob": 0.00038595980731770396,
      "seek": 2780,
      "start": 183.25000381469727,
      "temperature": 0.0,
      "text": " So in many ways, clear prompting for me is often just me understanding what I want. It's",
      "tokens": [
        51584,
        407,
        294,
        867,
        2098,
        11,
        1850,
        12391,
        278,
        337,
        385,
        307,
        2049,
        445,
        385,
        3701,
        437,
        286,
        528,
        13,
        467,
        311,
        51846
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_002.mp3"
    },
    {
      "id": 12,
      "avg_logprob": -0.23972675204277039,
      "compression_ratio": 1.686619758605957,
      "end": 189.77000427246094,
      "no_speech_prob": 0.0028448128141462803,
      "seek": 5744,
      "start": 188.4900016784668,
      "temperature": 0.0,
      "text": " like half the task.",
      "tokens": [
        50364,
        411,
        1922,
        264,
        5633,
        13,
        50428
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_002.mp3"
    },
    {
      "id": 13,
      "avg_logprob": -0.23972675204277039,
      "compression_ratio": 1.686619758605957,
      "end": 194.45000457763672,
      "no_speech_prob": 0.0028448128141462803,
      "seek": 5744,
      "start": 189.77000427246094,
      "temperature": 0.0,
      "text": " So I guess that's quite challenging. There's like a laziness that overtakes me if I'm talking",
      "tokens": [
        50428,
        407,
        286,
        2041,
        300,
        311,
        1596,
        7595,
        13,
        821,
        311,
        411,
        257,
        19320,
        1324,
        300,
        17038,
        3419,
        385,
        498,
        286,
        478,
        1417,
        50662
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_002.mp3"
    },
    {
      "id": 14,
      "avg_logprob": -0.23972675204277039,
      "compression_ratio": 1.686619758605957,
      "end": 200.45000457763672,
      "no_speech_prob": 0.0028448128141462803,
      "seek": 5744,
      "start": 194.45000457763672,
      "temperature": 0.0,
      "text": " to Claude, where I hope Claude just figures it out. So for example, I asked Claude for",
      "tokens": [
        50662,
        281,
        12947,
        2303,
        11,
        689,
        286,
        1454,
        12947,
        2303,
        445,
        9624,
        309,
        484,
        13,
        407,
        337,
        1365,
        11,
        286,
        2351,
        12947,
        2303,
        337,
        50962
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_002.mp3"
    },
    {
      "id": 15,
      "avg_logprob": -0.23972675204277039,
      "compression_ratio": 1.686619758605957,
      "end": 205.69000244140625,
      "no_speech_prob": 0.0028448128141462803,
      "seek": 5744,
      "start": 200.45000457763672,
      "temperature": 0.0,
      "text": " today to ask some interesting questions. Okay. And the questions that came up, and I think",
      "tokens": [
        50962,
        965,
        281,
        1029,
        512,
        1880,
        1651,
        13,
        1033,
        13,
        400,
        264,
        1651,
        300,
        1361,
        493,
        11,
        293,
        286,
        519,
        51224
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_002.mp3"
    },
    {
      "id": 16,
      "avg_logprob": -0.23972675204277039,
      "compression_ratio": 1.686619758605957,
      "end": 212.88999938964844,
      "no_speech_prob": 0.0028448128141462803,
      "seek": 5744,
      "start": 205.69000244140625,
      "temperature": 0.0,
      "text": " I listed a few sort of interesting counterintuitive and or funny or something like this. All right.",
      "tokens": [
        51224,
        286,
        10052,
        257,
        1326,
        1333,
        295,
        1880,
        5682,
        686,
        48314,
        293,
        420,
        4074,
        420,
        746,
        411,
        341,
        13,
        1057,
        558,
        13,
        51584
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_002.mp3"
    },
    {
      "id": 17,
      "avg_logprob": -0.23972675204277039,
      "compression_ratio": 1.686619758605957,
      "end": 217.77000427246094,
      "no_speech_prob": 0.0028448128141462803,
      "seek": 5744,
      "start": 212.88999938964844,
      "temperature": 0.0,
      "text": " And it gave me some pretty good, like it was okay. But I think what I'm hearing you say",
      "tokens": [
        51584,
        400,
        309,
        2729,
        385,
        512,
        1238,
        665,
        11,
        411,
        309,
        390,
        1392,
        13,
        583,
        286,
        519,
        437,
        286,
        478,
        4763,
        291,
        584,
        51828
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_002.mp3"
    },
    {
      "id": 18,
      "avg_logprob": -0.22104805707931519,
      "compression_ratio": 1.69140625,
      "end": 222.17000579833984,
      "no_speech_prob": 0.0006262886454351246,
      "seek": 8672,
      "start": 217.85000610351562,
      "temperature": 0.0,
      "text": " is like, all right, well, I have to be more rigorous here. I should probably give examples",
      "tokens": [
        50368,
        307,
        411,
        11,
        439,
        558,
        11,
        731,
        11,
        286,
        362,
        281,
        312,
        544,
        29882,
        510,
        13,
        286,
        820,
        1391,
        976,
        5110,
        50584
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_002.mp3"
    },
    {
      "id": 19,
      "avg_logprob": -0.22104805707931519,
      "compression_ratio": 1.69140625,
      "end": 229.2100067138672,
      "no_speech_prob": 0.0006262886454351246,
      "seek": 8672,
      "start": 222.17000579833984,
      "temperature": 0.0,
      "text": " of what I mean by interesting and what I mean by funny or counterintuitive and iteratively",
      "tokens": [
        50584,
        295,
        437,
        286,
        914,
        538,
        1880,
        293,
        437,
        286,
        914,
        538,
        4074,
        420,
        5682,
        686,
        48314,
        293,
        17138,
        19020,
        50936
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_002.mp3"
    },
    {
      "id": 20,
      "avg_logprob": -0.22104805707931519,
      "compression_ratio": 1.69140625,
      "end": 238.6500015258789,
      "no_speech_prob": 0.0006262886454351246,
      "seek": 8672,
      "start": 230.81000518798828,
      "temperature": 0.0,
      "text": " build that prompt to better, to get it like what feels like is the right, because it's really,",
      "tokens": [
        51016,
        1322,
        300,
        12391,
        281,
        1101,
        11,
        281,
        483,
        309,
        411,
        437,
        3417,
        411,
        307,
        264,
        558,
        11,
        570,
        309,
        311,
        534,
        11,
        51408
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_002.mp3"
    },
    {
      "id": 21,
      "avg_logprob": -0.22104805707931519,
      "compression_ratio": 1.69140625,
      "end": 242.88999938964844,
      "no_speech_prob": 0.0006262886454351246,
      "seek": 8672,
      "start": 238.6500015258789,
      "temperature": 0.0,
      "text": " it's a creative act. I'm not asking for factional information. I'm asking to together",
      "tokens": [
        51408,
        309,
        311,
        257,
        5880,
        605,
        13,
        286,
        478,
        406,
        3365,
        337,
        1186,
        1966,
        1589,
        13,
        286,
        478,
        3365,
        281,
        1214,
        51620
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_002.mp3"
    },
    {
      "id": 22,
      "avg_logprob": -0.22104805707931519,
      "compression_ratio": 1.69140625,
      "end": 247.0500030517578,
      "no_speech_prob": 0.0006262886454351246,
      "seek": 8672,
      "start": 242.88999938964844,
      "temperature": 0.0,
      "text": " write with Claude. So I almost have to program using natural language.",
      "tokens": [
        51620,
        2464,
        365,
        12947,
        2303,
        13,
        407,
        286,
        1920,
        362,
        281,
        1461,
        1228,
        3303,
        2856,
        13,
        51828
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_002.mp3"
    },
    {
      "id": 23,
      "avg_logprob": -0.2123252898454666,
      "compression_ratio": 1.5491329431533813,
      "end": 253.29000091552734,
      "no_speech_prob": 0.00353781646117568,
      "seek": 11600,
      "start": 247.61000061035156,
      "temperature": 0.0,
      "text": " Yeah, I think that prompting does feel a lot like the kind of the programming using natural language",
      "tokens": [
        50392,
        865,
        11,
        286,
        519,
        300,
        12391,
        278,
        775,
        841,
        257,
        688,
        411,
        264,
        733,
        295,
        264,
        9410,
        1228,
        3303,
        2856,
        50676
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_002.mp3"
    },
    {
      "id": 24,
      "avg_logprob": -0.2123252898454666,
      "compression_ratio": 1.5491329431533813,
      "end": 258.8100051879883,
      "no_speech_prob": 0.00353781646117568,
      "seek": 11600,
      "start": 254.01000213623047,
      "temperature": 0.0,
      "text": " and experimentation or something. It's an odd blend of the two. I do think that for most tasks,",
      "tokens": [
        50712,
        293,
        37142,
        420,
        746,
        13,
        467,
        311,
        364,
        7401,
        10628,
        295,
        264,
        732,
        13,
        286,
        360,
        519,
        300,
        337,
        881,
        9608,
        11,
        50952
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_002.mp3"
    },
    {
      "id": 25,
      "avg_logprob": -0.2123252898454666,
      "compression_ratio": 1.5491329431533813,
      "end": 261.9300079345703,
      "no_speech_prob": 0.00353781646117568,
      "seek": 11600,
      "start": 258.8100051879883,
      "temperature": 0.0,
      "text": " so if I just want Claude to do a thing, I think that I am probably more",
      "tokens": [
        50952,
        370,
        498,
        286,
        445,
        528,
        12947,
        2303,
        281,
        360,
        257,
        551,
        11,
        286,
        519,
        300,
        286,
        669,
        1391,
        544,
        51108
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_002.mp3"
    },
    {
      "id": 0,
      "avg_logprob": -0.23027482628822327,
      "compression_ratio": 1.6759259700775146,
      "end": 267.5800061225891,
      "no_speech_prob": 0.0007096226909197867,
      "seek": 0,
      "start": 262.1000061035156,
      "temperature": 0.0,
      "text": " used to knowing how to ask it to avoid common pitfalls or issues that it has. I think these",
      "tokens": [
        50364,
        1143,
        281,
        5276,
        577,
        281,
        1029,
        309,
        281,
        5042,
        2689,
        10147,
        18542,
        420,
        2663,
        300,
        309,
        575,
        13,
        286,
        519,
        613,
        50638
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_003.mp3"
    },
    {
      "id": 1,
      "avg_logprob": -0.23027482628822327,
      "compression_ratio": 1.6759259700775146,
      "end": 273.3400058746338,
      "no_speech_prob": 0.0007096226909197867,
      "seek": 0,
      "start": 267.5800061225891,
      "temperature": 0.0,
      "text": " are decreasing a lot over time. But it's also very fine to just ask it for the thing that",
      "tokens": [
        50638,
        366,
        23223,
        257,
        688,
        670,
        565,
        13,
        583,
        309,
        311,
        611,
        588,
        2489,
        281,
        445,
        1029,
        309,
        337,
        264,
        551,
        300,
        50926
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_003.mp3"
    },
    {
      "id": 2,
      "avg_logprob": -0.23027482628822327,
      "compression_ratio": 1.6759259700775146,
      "end": 277.4400062561035,
      "no_speech_prob": 0.0007096226909197867,
      "seek": 0,
      "start": 273.3400058746338,
      "temperature": 0.0,
      "text": " you want. I think that prompting actually only really becomes relevant when you're really",
      "tokens": [
        50926,
        291,
        528,
        13,
        286,
        519,
        300,
        12391,
        278,
        767,
        787,
        534,
        3643,
        7340,
        562,
        291,
        434,
        534,
        51131
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_003.mp3"
    },
    {
      "id": 3,
      "avg_logprob": -0.23027482628822327,
      "compression_ratio": 1.6759259700775146,
      "end": 283.0600051879883,
      "no_speech_prob": 0.0007096226909197867,
      "seek": 0,
      "start": 277.4400062561035,
      "temperature": 0.0,
      "text": " trying to eke out the top 2% of model performance. So for a lot of tasks, I might just, if it",
      "tokens": [
        51131,
        1382,
        281,
        308,
        330,
        484,
        264,
        1192,
        568,
        4,
        295,
        2316,
        3389,
        13,
        407,
        337,
        257,
        688,
        295,
        9608,
        11,
        286,
        1062,
        445,
        11,
        498,
        309,
        51412
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_003.mp3"
    },
    {
      "id": 4,
      "avg_logprob": -0.23027482628822327,
      "compression_ratio": 1.6759259700775146,
      "end": 286.14000701904297,
      "no_speech_prob": 0.0007096226909197867,
      "seek": 0,
      "start": 283.0600051879883,
      "temperature": 0.0,
      "text": " gives me an initial list back and there's something I don't like about it, it's kind",
      "tokens": [
        51412,
        2709,
        385,
        364,
        5883,
        1329,
        646,
        293,
        456,
        311,
        746,
        286,
        500,
        380,
        411,
        466,
        309,
        11,
        309,
        311,
        733,
        51566
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_003.mp3"
    },
    {
      "id": 5,
      "avg_logprob": -0.23027482628822327,
      "compression_ratio": 1.6759259700775146,
      "end": 291.58000564575195,
      "no_speech_prob": 0.0007096226909197867,
      "seek": 0,
      "start": 286.14000701904297,
      "temperature": 0.0,
      "text": " of generic. For that kind of task, I'd probably just take a bunch of questions that I've had",
      "tokens": [
        51566,
        295,
        19577,
        13,
        1171,
        300,
        733,
        295,
        5633,
        11,
        286,
        1116,
        1391,
        445,
        747,
        257,
        3840,
        295,
        1651,
        300,
        286,
        600,
        632,
        51838
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_003.mp3"
    },
    {
      "id": 6,
      "avg_logprob": -0.24558737874031067,
      "compression_ratio": 1.7852349281311035,
      "end": 295.02000427246094,
      "no_speech_prob": 0.0008426222484558821,
      "seek": 2948,
      "start": 291.58000564575195,
      "temperature": 0.0,
      "text": " in the past that I've thought worked really well, and I would just give it to the model",
      "tokens": [
        50364,
        294,
        264,
        1791,
        300,
        286,
        600,
        1194,
        2732,
        534,
        731,
        11,
        293,
        286,
        576,
        445,
        976,
        309,
        281,
        264,
        2316,
        50536
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_003.mp3"
    },
    {
      "id": 7,
      "avg_logprob": -0.24558737874031067,
      "compression_ratio": 1.7852349281311035,
      "end": 299.6200065612793,
      "no_speech_prob": 0.0008426222484558821,
      "seek": 2948,
      "start": 295.02000427246094,
      "temperature": 0.0,
      "text": " and then be like, now here's this person I'm talking with. Give me questions of at least",
      "tokens": [
        50536,
        293,
        550,
        312,
        411,
        11,
        586,
        510,
        311,
        341,
        954,
        286,
        478,
        1417,
        365,
        13,
        5303,
        385,
        1651,
        295,
        412,
        1935,
        50766
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_003.mp3"
    },
    {
      "id": 8,
      "avg_logprob": -0.24558737874031067,
      "compression_ratio": 1.7852349281311035,
      "end": 305.1000061035156,
      "no_speech_prob": 0.0008426222484558821,
      "seek": 2948,
      "start": 299.6200065612793,
      "temperature": 0.0,
      "text": " that quality. Or I might just ask it for some questions. And then if I was like, oh, these",
      "tokens": [
        50766,
        300,
        3125,
        13,
        1610,
        286,
        1062,
        445,
        1029,
        309,
        337,
        512,
        1651,
        13,
        400,
        550,
        498,
        286,
        390,
        411,
        11,
        1954,
        11,
        613,
        51040
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_003.mp3"
    },
    {
      "id": 9,
      "avg_logprob": -0.24558737874031067,
      "compression_ratio": 1.7852349281311035,
      "end": 308.9400062561035,
      "no_speech_prob": 0.0008426222484558821,
      "seek": 2948,
      "start": 305.1000061035156,
      "temperature": 0.0,
      "text": " are kind of trite or like, you know, I would just give it that feedback and then hopefully",
      "tokens": [
        51040,
        366,
        733,
        295,
        504,
        642,
        420,
        411,
        11,
        291,
        458,
        11,
        286,
        576,
        445,
        976,
        309,
        300,
        5824,
        293,
        550,
        4696,
        51232
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_003.mp3"
    },
    {
      "id": 10,
      "avg_logprob": -0.24558737874031067,
      "compression_ratio": 1.7852349281311035,
      "end": 314.42000579833984,
      "no_speech_prob": 0.0008426222484558821,
      "seek": 2948,
      "start": 308.9400062561035,
      "temperature": 0.0,
      "text": " produces a better list. I think that kind of iterative prompting, at that point, your",
      "tokens": [
        51232,
        14725,
        257,
        1101,
        1329,
        13,
        286,
        519,
        300,
        733,
        295,
        17138,
        1166,
        12391,
        278,
        11,
        412,
        300,
        935,
        11,
        428,
        51506
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_003.mp3"
    },
    {
      "id": 11,
      "avg_logprob": -0.24558737874031067,
      "compression_ratio": 1.7852349281311035,
      "end": 317.3000068664551,
      "no_speech_prob": 0.0008426222484558821,
      "seek": 2948,
      "start": 314.42000579833984,
      "temperature": 0.0,
      "text": " prompt is like a tool that you're going to get so much value out of that you're willing",
      "tokens": [
        51506,
        12391,
        307,
        411,
        257,
        2290,
        300,
        291,
        434,
        516,
        281,
        483,
        370,
        709,
        2158,
        484,
        295,
        300,
        291,
        434,
        4950,
        51650
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_003.mp3"
    },
    {
      "id": 12,
      "avg_logprob": -0.23007084429264069,
      "compression_ratio": 1.8379310369491577,
      "end": 321.9400062561035,
      "no_speech_prob": 0.016913503408432007,
      "seek": 5520,
      "start": 317.3000068664551,
      "temperature": 0.0,
      "text": " to put in the work. If I was a company making prompts for models, I'm just like, if you're",
      "tokens": [
        50364,
        281,
        829,
        294,
        264,
        589,
        13,
        759,
        286,
        390,
        257,
        2237,
        1455,
        41095,
        337,
        5245,
        11,
        286,
        478,
        445,
        411,
        11,
        498,
        291,
        434,
        50596
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_003.mp3"
    },
    {
      "id": 13,
      "avg_logprob": -0.23007084429264069,
      "compression_ratio": 1.8379310369491577,
      "end": 327.02000427246094,
      "no_speech_prob": 0.016913503408432007,
      "seek": 5520,
      "start": 321.9400062561035,
      "temperature": 0.0,
      "text": " willing to spend a lot of time and resources on the engineering behind what you're building,",
      "tokens": [
        50596,
        4950,
        281,
        3496,
        257,
        688,
        295,
        565,
        293,
        3593,
        322,
        264,
        7043,
        2261,
        437,
        291,
        434,
        2390,
        11,
        50850
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_003.mp3"
    },
    {
      "id": 14,
      "avg_logprob": -0.23007084429264069,
      "compression_ratio": 1.8379310369491577,
      "end": 331.3800048828125,
      "no_speech_prob": 0.016913503408432007,
      "seek": 5520,
      "start": 327.02000427246094,
      "temperature": 0.0,
      "text": " then the prompt is not something that you should be spending an hour on. It's like,",
      "tokens": [
        50850,
        550,
        264,
        12391,
        307,
        406,
        746,
        300,
        291,
        820,
        312,
        6434,
        364,
        1773,
        322,
        13,
        467,
        311,
        411,
        11,
        51068
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_003.mp3"
    },
    {
      "id": 15,
      "avg_logprob": -0.23007084429264069,
      "compression_ratio": 1.8379310369491577,
      "end": 334.90000915527344,
      "no_speech_prob": 0.016913503408432007,
      "seek": 5520,
      "start": 331.3800048828125,
      "temperature": 0.0,
      "text": " that's a big part of your system. Make sure it's working really well. And so it's only",
      "tokens": [
        51068,
        300,
        311,
        257,
        955,
        644,
        295,
        428,
        1185,
        13,
        4387,
        988,
        309,
        311,
        1364,
        534,
        731,
        13,
        400,
        370,
        309,
        311,
        787,
        51244
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_003.mp3"
    },
    {
      "id": 16,
      "avg_logprob": -0.23007084429264069,
      "compression_ratio": 1.8379310369491577,
      "end": 339.74000549316406,
      "no_speech_prob": 0.016913503408432007,
      "seek": 5520,
      "start": 334.90000915527344,
      "temperature": 0.0,
      "text": " things like that. If I'm using a prompt to classify things or to create data, that's",
      "tokens": [
        51244,
        721,
        411,
        300,
        13,
        759,
        286,
        478,
        1228,
        257,
        12391,
        281,
        33872,
        721,
        420,
        281,
        1884,
        1412,
        11,
        300,
        311,
        51486
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_003.mp3"
    },
    {
      "id": 17,
      "avg_logprob": -0.23007084429264069,
      "compression_ratio": 1.8379310369491577,
      "end": 343.42000579833984,
      "no_speech_prob": 0.016913503408432007,
      "seek": 5520,
      "start": 339.74000549316406,
      "temperature": 0.0,
      "text": " when you're like, it's actually worth just spending a lot of time really thinking it",
      "tokens": [
        51486,
        562,
        291,
        434,
        411,
        11,
        309,
        311,
        767,
        3163,
        445,
        6434,
        257,
        688,
        295,
        565,
        534,
        1953,
        309,
        51670
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_003.mp3"
    },
    {
      "id": 18,
      "avg_logprob": -0.23007084429264069,
      "compression_ratio": 1.8379310369491577,
      "end": 344.42000579833984,
      "no_speech_prob": 0.016913503408432007,
      "seek": 5520,
      "start": 343.42000579833984,
      "temperature": 0.0,
      "text": " through.",
      "tokens": [
        51670,
        807,
        13,
        51720
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_003.mp3"
    },
    {
      "id": 19,
      "avg_logprob": -0.2759844660758972,
      "compression_ratio": 1.882165551185608,
      "end": 349.94000244140625,
      "no_speech_prob": 0.03114001452922821,
      "seek": 8232,
      "start": 344.42000579833984,
      "temperature": 0.0,
      "text": " What other advice would you give to people that are talking to Claude, sort of generally,",
      "tokens": [
        50364,
        708,
        661,
        5192,
        576,
        291,
        976,
        281,
        561,
        300,
        366,
        1417,
        281,
        12947,
        2303,
        11,
        1333,
        295,
        5101,
        11,
        50640
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_003.mp3"
    },
    {
      "id": 20,
      "avg_logprob": -0.2759844660758972,
      "compression_ratio": 1.882165551185608,
      "end": 352.7800064086914,
      "no_speech_prob": 0.03114001452922821,
      "seek": 8232,
      "start": 349.94000244140625,
      "temperature": 0.0,
      "text": " more general, because right now we're talking about maybe the edge cases like eking out",
      "tokens": [
        50640,
        544,
        2674,
        11,
        570,
        558,
        586,
        321,
        434,
        1417,
        466,
        1310,
        264,
        4691,
        3331,
        411,
        308,
        5092,
        484,
        50782
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_003.mp3"
    },
    {
      "id": 21,
      "avg_logprob": -0.2759844660758972,
      "compression_ratio": 1.882165551185608,
      "end": 358.5400085449219,
      "no_speech_prob": 0.03114001452922821,
      "seek": 8232,
      "start": 352.7800064086914,
      "temperature": 0.0,
      "text": " the 2%. But what in general advice would you give when they show up to Claude trying it",
      "tokens": [
        50782,
        264,
        568,
        6856,
        583,
        437,
        294,
        2674,
        5192,
        576,
        291,
        976,
        562,
        436,
        855,
        493,
        281,
        12947,
        2303,
        1382,
        309,
        51070
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_003.mp3"
    },
    {
      "id": 22,
      "avg_logprob": -0.2759844660758972,
      "compression_ratio": 1.882165551185608,
      "end": 359.5400085449219,
      "no_speech_prob": 0.03114001452922821,
      "seek": 8232,
      "start": 358.5400085449219,
      "temperature": 0.0,
      "text": " for the first time?",
      "tokens": [
        51070,
        337,
        264,
        700,
        565,
        30,
        51120
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_003.mp3"
    },
    {
      "id": 23,
      "avg_logprob": -0.2759844660758972,
      "compression_ratio": 1.882165551185608,
      "end": 363.6600036621094,
      "no_speech_prob": 0.03114001452922821,
      "seek": 8232,
      "start": 359.5400085449219,
      "temperature": 0.0,
      "text": " You know, there's a concern that people over-anthropomorphize models. And I think that's like a very valid",
      "tokens": [
        51120,
        509,
        458,
        11,
        456,
        311,
        257,
        3136,
        300,
        561,
        670,
        12,
        282,
        14222,
        32702,
        1125,
        5245,
        13,
        400,
        286,
        519,
        300,
        311,
        411,
        257,
        588,
        7363,
        51326
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_003.mp3"
    },
    {
      "id": 24,
      "avg_logprob": -0.2759844660758972,
      "compression_ratio": 1.882165551185608,
      "end": 369.7000045776367,
      "no_speech_prob": 0.03114001452922821,
      "seek": 8232,
      "start": 363.6600036621094,
      "temperature": 0.0,
      "text": " concern. I also think that people often under-anthropomorphize them, because sometimes when I see like issues",
      "tokens": [
        51326,
        3136,
        13,
        286,
        611,
        519,
        300,
        561,
        2049,
        833,
        12,
        282,
        14222,
        32702,
        1125,
        552,
        11,
        570,
        2171,
        562,
        286,
        536,
        411,
        2663,
        51628
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_003.mp3"
    },
    {
      "id": 25,
      "avg_logprob": -0.2759844660758972,
      "compression_ratio": 1.882165551185608,
      "end": 373.22000885009766,
      "no_speech_prob": 0.03114001452922821,
      "seek": 8232,
      "start": 369.7000045776367,
      "temperature": 0.0,
      "text": " that people have run into with Claude, you know, say Claude is like refusing a task that",
      "tokens": [
        51628,
        300,
        561,
        362,
        1190,
        666,
        365,
        12947,
        2303,
        11,
        291,
        458,
        11,
        584,
        12947,
        2303,
        307,
        411,
        37289,
        257,
        5633,
        300,
        51804
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_003.mp3"
    },
    {
      "id": 26,
      "avg_logprob": -0.3016120493412018,
      "compression_ratio": 1.6081081628799438,
      "end": 377.9800033569336,
      "no_speech_prob": 0.030669568106532097,
      "seek": 11112,
      "start": 373.22000885009766,
      "temperature": 0.0,
      "text": " it shouldn't refuse. But then I look at the text and like the specific wording of what",
      "tokens": [
        50364,
        309,
        4659,
        380,
        16791,
        13,
        583,
        550,
        286,
        574,
        412,
        264,
        2487,
        293,
        411,
        264,
        2685,
        47602,
        295,
        437,
        50602
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_003.mp3"
    },
    {
      "id": 27,
      "avg_logprob": -0.3016120493412018,
      "compression_ratio": 1.6081081628799438,
      "end": 384.94000244140625,
      "no_speech_prob": 0.030669568106532097,
      "seek": 11112,
      "start": 377.9800033569336,
      "temperature": 0.0,
      "text": " they wrote, and I'm like, I see why Claude did that. And I'm like, if you think through",
      "tokens": [
        50602,
        436,
        4114,
        11,
        293,
        286,
        478,
        411,
        11,
        286,
        536,
        983,
        12947,
        2303,
        630,
        300,
        13,
        400,
        286,
        478,
        411,
        11,
        498,
        291,
        519,
        807,
        50950
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_003.mp3"
    },
    {
      "id": 28,
      "avg_logprob": -0.3016120493412018,
      "compression_ratio": 1.6081081628799438,
      "end": 388.5400085449219,
      "no_speech_prob": 0.030669568106532097,
      "seek": 11112,
      "start": 384.94000244140625,
      "temperature": 0.0,
      "text": " how that looks to Claude, you probably could have just written it in a way that wouldn't",
      "tokens": [
        50950,
        577,
        300,
        1542,
        281,
        12947,
        2303,
        11,
        291,
        1391,
        727,
        362,
        445,
        3720,
        309,
        294,
        257,
        636,
        300,
        2759,
        380,
        51130
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_003.mp3"
    },
    {
      "id": 29,
      "avg_logprob": -0.3016120493412018,
      "compression_ratio": 1.6081081628799438,
      "end": 393.1800079345703,
      "no_speech_prob": 0.030669568106532097,
      "seek": 11112,
      "start": 388.5400085449219,
      "temperature": 0.0,
      "text": " evoke such a response. Especially, this is more relevant if you see failures or if you",
      "tokens": [
        51130,
        1073,
        2949,
        1270,
        257,
        4134,
        13,
        8545,
        11,
        341,
        307,
        544,
        7340,
        498,
        291,
        536,
        20774,
        420,
        498,
        291,
        51362
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_003.mp3"
    },
    {
      "id": 30,
      "avg_logprob": -0.3016120493412018,
      "compression_ratio": 1.6081081628799438,
      "end": 393.1800079345703,
      "no_speech_prob": 0.030669568106532097,
      "seek": 11112,
      "start": 393.1800079345703,
      "temperature": 0.0,
      "text": "",
      "tokens": [],
      "words": [],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_003.mp3"
    },
    {
      "id": 0,
      "avg_logprob": -0.2980593740940094,
      "compression_ratio": 1.638686180114746,
      "end": 399.15000915527344,
      "no_speech_prob": 0.00018522109894547611,
      "seek": 0,
      "start": 393.15000915527344,
      "temperature": 0.0,
      "text": " you see issues. It's sort of like, think about what the model failed at. What did it do wrong?",
      "tokens": [
        50364,
        291,
        536,
        2663,
        13,
        467,
        311,
        1333,
        295,
        411,
        11,
        519,
        466,
        437,
        264,
        2316,
        7612,
        412,
        13,
        708,
        630,
        309,
        360,
        2085,
        30,
        50664
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_004.mp3"
    },
    {
      "id": 1,
      "avg_logprob": -0.2980593740940094,
      "compression_ratio": 1.638686180114746,
      "end": 405.5500087738037,
      "no_speech_prob": 0.00018522109894547611,
      "seek": 0,
      "start": 399.15000915527344,
      "temperature": 0.0,
      "text": " And then maybe that will give you a sense of why. So is it the way that I phrased the",
      "tokens": [
        50664,
        400,
        550,
        1310,
        300,
        486,
        976,
        291,
        257,
        2020,
        295,
        983,
        13,
        407,
        307,
        309,
        264,
        636,
        300,
        286,
        7636,
        1937,
        264,
        50984
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_004.mp3"
    },
    {
      "id": 2,
      "avg_logprob": -0.2980593740940094,
      "compression_ratio": 1.638686180114746,
      "end": 410.2300090789795,
      "no_speech_prob": 0.00018522109894547611,
      "seek": 0,
      "start": 405.5500087738037,
      "temperature": 0.0,
      "text": " thing? And obviously, as models get smarter, you're going to need less of this. And I already",
      "tokens": [
        50984,
        551,
        30,
        400,
        2745,
        11,
        382,
        5245,
        483,
        20294,
        11,
        291,
        434,
        516,
        281,
        643,
        1570,
        295,
        341,
        13,
        400,
        286,
        1217,
        51218
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_004.mp3"
    },
    {
      "id": 3,
      "avg_logprob": -0.2980593740940094,
      "compression_ratio": 1.638686180114746,
      "end": 416.3500099182129,
      "no_speech_prob": 0.00018522109894547611,
      "seek": 0,
      "start": 410.2300090789795,
      "temperature": 0.0,
      "text": " see people needing less of it. But that's probably the advice is try to have empathy",
      "tokens": [
        51218,
        536,
        561,
        18006,
        1570,
        295,
        309,
        13,
        583,
        300,
        311,
        1391,
        264,
        5192,
        307,
        853,
        281,
        362,
        18701,
        51524
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_004.mp3"
    },
    {
      "id": 4,
      "avg_logprob": -0.2980593740940094,
      "compression_ratio": 1.638686180114746,
      "end": 421.510009765625,
      "no_speech_prob": 0.00018522109894547611,
      "seek": 0,
      "start": 416.3500099182129,
      "temperature": 0.0,
      "text": " for the model. Read what you wrote as if you were a person just encountering this for the",
      "tokens": [
        51524,
        337,
        264,
        2316,
        13,
        17604,
        437,
        291,
        4114,
        382,
        498,
        291,
        645,
        257,
        954,
        445,
        8593,
        278,
        341,
        337,
        264,
        51782
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_004.mp3"
    },
    {
      "id": 5,
      "avg_logprob": -0.25653257966041565,
      "compression_ratio": 1.7766990661621094,
      "end": 425.3500099182129,
      "no_speech_prob": 0.001867490354925394,
      "seek": 2836,
      "start": 421.510009765625,
      "temperature": 0.0,
      "text": " first time. How does it look to you? And what would have made you behave in the way that",
      "tokens": [
        50364,
        700,
        565,
        13,
        1012,
        775,
        309,
        574,
        281,
        291,
        30,
        400,
        437,
        576,
        362,
        1027,
        291,
        15158,
        294,
        264,
        636,
        300,
        50556
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_004.mp3"
    },
    {
      "id": 6,
      "avg_logprob": -0.25653257966041565,
      "compression_ratio": 1.7766990661621094,
      "end": 429.15000915527344,
      "no_speech_prob": 0.001867490354925394,
      "seek": 2836,
      "start": 425.3500099182129,
      "temperature": 0.0,
      "text": " the model behaved? So if it misunderstood, what kind of like, what coding language you",
      "tokens": [
        50556,
        264,
        2316,
        48249,
        30,
        407,
        498,
        309,
        33870,
        11,
        437,
        733,
        295,
        411,
        11,
        437,
        17720,
        2856,
        291,
        50746
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_004.mp3"
    },
    {
      "id": 7,
      "avg_logprob": -0.25653257966041565,
      "compression_ratio": 1.7766990661621094,
      "end": 432.63000869750977,
      "no_speech_prob": 0.001867490354925394,
      "seek": 2836,
      "start": 429.15000915527344,
      "temperature": 0.0,
      "text": " wanted to use? Is that because like, it was just very ambiguous, and it kind of had to",
      "tokens": [
        50746,
        1415,
        281,
        764,
        30,
        1119,
        300,
        570,
        411,
        11,
        309,
        390,
        445,
        588,
        39465,
        11,
        293,
        309,
        733,
        295,
        632,
        281,
        50920
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_004.mp3"
    },
    {
      "id": 8,
      "avg_logprob": -0.25653257966041565,
      "compression_ratio": 1.7766990661621094,
      "end": 435.9900093078613,
      "no_speech_prob": 0.001867490354925394,
      "seek": 2836,
      "start": 432.63000869750977,
      "temperature": 0.0,
      "text": " take a guess, in which case next time, you could just be like, hey, make sure this is",
      "tokens": [
        50920,
        747,
        257,
        2041,
        11,
        294,
        597,
        1389,
        958,
        565,
        11,
        291,
        727,
        445,
        312,
        411,
        11,
        4177,
        11,
        652,
        988,
        341,
        307,
        51088
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_004.mp3"
    },
    {
      "id": 9,
      "avg_logprob": -0.25653257966041565,
      "compression_ratio": 1.7766990661621094,
      "end": 439.39001083374023,
      "no_speech_prob": 0.001867490354925394,
      "seek": 2836,
      "start": 435.9900093078613,
      "temperature": 0.0,
      "text": " in Python. Or, I mean, that's the kind of mistake I think models are much less likely",
      "tokens": [
        51088,
        294,
        15329,
        13,
        1610,
        11,
        286,
        914,
        11,
        300,
        311,
        264,
        733,
        295,
        6146,
        286,
        519,
        5245,
        366,
        709,
        1570,
        3700,
        51258
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_004.mp3"
    },
    {
      "id": 10,
      "avg_logprob": -0.25653257966041565,
      "compression_ratio": 1.7766990661621094,
      "end": 443.75000762939453,
      "no_speech_prob": 0.001867490354925394,
      "seek": 2836,
      "start": 439.39001083374023,
      "temperature": 0.0,
      "text": " to make now. But you know, if you if you do see that kind of mistake, that's, that's probably",
      "tokens": [
        51258,
        281,
        652,
        586,
        13,
        583,
        291,
        458,
        11,
        498,
        291,
        498,
        291,
        360,
        536,
        300,
        733,
        295,
        6146,
        11,
        300,
        311,
        11,
        300,
        311,
        1391,
        51476
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_004.mp3"
    },
    {
      "id": 11,
      "avg_logprob": -0.25653257966041565,
      "compression_ratio": 1.7766990661621094,
      "end": 444.75000762939453,
      "no_speech_prob": 0.001867490354925394,
      "seek": 2836,
      "start": 443.75000762939453,
      "temperature": 0.0,
      "text": " the advice I'd have.",
      "tokens": [
        51476,
        264,
        5192,
        286,
        1116,
        362,
        13,
        51526
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_004.mp3"
    },
    {
      "id": 12,
      "avg_logprob": -0.27847832441329956,
      "compression_ratio": 1.708185076713562,
      "end": 451.8300094604492,
      "no_speech_prob": 0.007345489226281643,
      "seek": 5160,
      "start": 445.31000900268555,
      "temperature": 0.0,
      "text": " And maybe sort of, I guess, ask questions, why? Or what other details can I provide to",
      "tokens": [
        50392,
        400,
        1310,
        1333,
        295,
        11,
        286,
        2041,
        11,
        1029,
        1651,
        11,
        983,
        30,
        1610,
        437,
        661,
        4365,
        393,
        286,
        2893,
        281,
        50718
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_004.mp3"
    },
    {
      "id": 13,
      "avg_logprob": -0.27847832441329956,
      "compression_ratio": 1.708185076713562,
      "end": 454.6700096130371,
      "no_speech_prob": 0.007345489226281643,
      "seek": 5160,
      "start": 451.8300094604492,
      "temperature": 0.0,
      "text": " help you answer better? Yeah, is that work or no?",
      "tokens": [
        50718,
        854,
        291,
        1867,
        1101,
        30,
        865,
        11,
        307,
        300,
        589,
        420,
        572,
        30,
        50860
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_004.mp3"
    },
    {
      "id": 14,
      "avg_logprob": -0.27847832441329956,
      "compression_ratio": 1.708185076713562,
      "end": 458.7100067138672,
      "no_speech_prob": 0.007345489226281643,
      "seek": 5160,
      "start": 454.87001037597656,
      "temperature": 0.0,
      "text": " Yeah, I mean, I've done this with the models, like, it doesn't always work. But like,",
      "tokens": [
        50870,
        865,
        11,
        286,
        914,
        11,
        286,
        600,
        1096,
        341,
        365,
        264,
        5245,
        11,
        411,
        11,
        309,
        1177,
        380,
        1009,
        589,
        13,
        583,
        411,
        11,
        51062
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_004.mp3"
    },
    {
      "id": 15,
      "avg_logprob": -0.27847832441329956,
      "compression_ratio": 1.708185076713562,
      "end": 463.8300094604492,
      "no_speech_prob": 0.007345489226281643,
      "seek": 5160,
      "start": 459.1900100708008,
      "temperature": 0.0,
      "text": " sometimes I'll just be like, why did you do that? I mean, people underestimate the",
      "tokens": [
        51086,
        2171,
        286,
        603,
        445,
        312,
        411,
        11,
        983,
        630,
        291,
        360,
        300,
        30,
        286,
        914,
        11,
        561,
        35826,
        264,
        51318
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_004.mp3"
    },
    {
      "id": 16,
      "avg_logprob": -0.27847832441329956,
      "compression_ratio": 1.708185076713562,
      "end": 469.27001190185547,
      "no_speech_prob": 0.007345489226281643,
      "seek": 5160,
      "start": 463.8300094604492,
      "temperature": 0.0,
      "text": " decrease, which you can really interact with, with models. Like, like, yeah, I'm just",
      "tokens": [
        51318,
        11514,
        11,
        597,
        291,
        393,
        534,
        4648,
        365,
        11,
        365,
        5245,
        13,
        1743,
        11,
        411,
        11,
        1338,
        11,
        286,
        478,
        445,
        51590
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_004.mp3"
    },
    {
      "id": 17,
      "avg_logprob": -0.27847832441329956,
      "compression_ratio": 1.708185076713562,
      "end": 473.07000732421875,
      "no_speech_prob": 0.007345489226281643,
      "seek": 5160,
      "start": 469.27001190185547,
      "temperature": 0.0,
      "text": " like, and sometimes it's really cool word for word, the part that made you and you don't",
      "tokens": [
        51590,
        411,
        11,
        293,
        2171,
        309,
        311,
        534,
        1627,
        1349,
        337,
        1349,
        11,
        264,
        644,
        300,
        1027,
        291,
        293,
        291,
        500,
        380,
        51780
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_004.mp3"
    },
    {
      "id": 18,
      "avg_logprob": -0.2274792194366455,
      "compression_ratio": 1.815047025680542,
      "end": 476.1900100708008,
      "no_speech_prob": 0.002672943752259016,
      "seek": 7992,
      "start": 473.07000732421875,
      "temperature": 0.0,
      "text": " know that it's like fully accurate. But sometimes you do that, and then you change a",
      "tokens": [
        50364,
        458,
        300,
        309,
        311,
        411,
        4498,
        8559,
        13,
        583,
        2171,
        291,
        360,
        300,
        11,
        293,
        550,
        291,
        1319,
        257,
        50520
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_004.mp3"
    },
    {
      "id": 19,
      "avg_logprob": -0.2274792194366455,
      "compression_ratio": 1.815047025680542,
      "end": 479.7100067138672,
      "no_speech_prob": 0.002672943752259016,
      "seek": 7992,
      "start": 476.1900100708008,
      "temperature": 0.0,
      "text": " thing. I mean, also use the models to help me with all of this stuff, I should say, like",
      "tokens": [
        50520,
        551,
        13,
        286,
        914,
        11,
        611,
        764,
        264,
        5245,
        281,
        854,
        385,
        365,
        439,
        295,
        341,
        1507,
        11,
        286,
        820,
        584,
        11,
        411,
        50696
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_004.mp3"
    },
    {
      "id": 20,
      "avg_logprob": -0.2274792194366455,
      "compression_ratio": 1.815047025680542,
      "end": 483.7900085449219,
      "no_speech_prob": 0.002672943752259016,
      "seek": 7992,
      "start": 479.7100067138672,
      "temperature": 0.0,
      "text": " prompting can end up being a little factory where you're actually building prompts to",
      "tokens": [
        50696,
        12391,
        278,
        393,
        917,
        493,
        885,
        257,
        707,
        9265,
        689,
        291,
        434,
        767,
        2390,
        41095,
        281,
        50900
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_004.mp3"
    },
    {
      "id": 21,
      "avg_logprob": -0.2274792194366455,
      "compression_ratio": 1.815047025680542,
      "end": 488.8300094604492,
      "no_speech_prob": 0.002672943752259016,
      "seek": 7992,
      "start": 483.7900085449219,
      "temperature": 0.0,
      "text": " generate prompts. And so like, yeah, anything where you're like having an issue,",
      "tokens": [
        50900,
        8460,
        41095,
        13,
        400,
        370,
        411,
        11,
        1338,
        11,
        1340,
        689,
        291,
        434,
        411,
        1419,
        364,
        2734,
        11,
        51152
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_004.mp3"
    },
    {
      "id": 22,
      "avg_logprob": -0.2274792194366455,
      "compression_ratio": 1.815047025680542,
      "end": 492.510009765625,
      "no_speech_prob": 0.002672943752259016,
      "seek": 7992,
      "start": 490.1100082397461,
      "temperature": 0.0,
      "text": " asking for suggestions, sometimes just do that.",
      "tokens": [
        51216,
        3365,
        337,
        13396,
        11,
        2171,
        445,
        360,
        300,
        13,
        51336
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_004.mp3"
    },
    {
      "id": 23,
      "avg_logprob": -0.2274792194366455,
      "compression_ratio": 1.815047025680542,
      "end": 493.95001220703125,
      "no_speech_prob": 0.002672943752259016,
      "seek": 7992,
      "start": 492.55001068115234,
      "temperature": 0.0,
      "text": " Like you made that error.",
      "tokens": [
        51338,
        1743,
        291,
        1027,
        300,
        6713,
        13,
        51408
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_004.mp3"
    },
    {
      "id": 24,
      "avg_logprob": -0.2274792194366455,
      "compression_ratio": 1.815047025680542,
      "end": 496.5900115966797,
      "no_speech_prob": 0.002672943752259016,
      "seek": 7992,
      "start": 494.2300109863281,
      "temperature": 0.0,
      "text": " What could I have said? That's actually not uncommon for me to do.",
      "tokens": [
        51422,
        708,
        727,
        286,
        362,
        848,
        30,
        663,
        311,
        767,
        406,
        29289,
        337,
        385,
        281,
        360,
        13,
        51540
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_004.mp3"
    },
    {
      "id": 25,
      "avg_logprob": -0.2274792194366455,
      "compression_ratio": 1.815047025680542,
      "end": 499.15000915527344,
      "no_speech_prob": 0.002672943752259016,
      "seek": 7992,
      "start": 496.5900115966797,
      "temperature": 0.0,
      "text": " What could I have said that would make you not make that error?",
      "tokens": [
        51540,
        708,
        727,
        286,
        362,
        848,
        300,
        576,
        652,
        291,
        406,
        652,
        300,
        6713,
        30,
        51668
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_004.mp3"
    },
    {
      "id": 26,
      "avg_logprob": -0.2274792194366455,
      "compression_ratio": 1.815047025680542,
      "end": 500.7100067138672,
      "no_speech_prob": 0.002672943752259016,
      "seek": 7992,
      "start": 499.39000701904297,
      "temperature": 0.0,
      "text": " Write that out as an instruction.",
      "tokens": [
        51680,
        23499,
        300,
        484,
        382,
        364,
        10951,
        13,
        51746
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_004.mp3"
    },
    {
      "id": 27,
      "avg_logprob": -0.25857335329055786,
      "compression_ratio": 1.515625,
      "end": 502.95001220703125,
      "no_speech_prob": 0.0035379095934331417,
      "seek": 10756,
      "start": 501.15000915527344,
      "temperature": 0.0,
      "text": " And I'm going to give it to model, I'm going to try it.",
      "tokens": [
        50386,
        400,
        286,
        478,
        516,
        281,
        976,
        309,
        281,
        2316,
        11,
        286,
        478,
        516,
        281,
        853,
        309,
        13,
        50476
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_004.mp3"
    },
    {
      "id": 28,
      "avg_logprob": -0.25857335329055786,
      "compression_ratio": 1.515625,
      "end": 507.5900115966797,
      "no_speech_prob": 0.0035379095934331417,
      "seek": 10756,
      "start": 503.07000732421875,
      "temperature": 0.0,
      "text": " Sometimes I do that. I give that to the model in another context window.",
      "tokens": [
        50482,
        4803,
        286,
        360,
        300,
        13,
        286,
        976,
        300,
        281,
        264,
        2316,
        294,
        1071,
        4319,
        4910,
        13,
        50708
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_004.mp3"
    },
    {
      "id": 29,
      "avg_logprob": -0.25857335329055786,
      "compression_ratio": 1.515625,
      "end": 510.87001037597656,
      "no_speech_prob": 0.0035379095934331417,
      "seek": 10756,
      "start": 507.5900115966797,
      "temperature": 0.0,
      "text": " Often, I take the response, I give it to Claude and I'm like, hmm, didn't work.",
      "tokens": [
        50708,
        20043,
        11,
        286,
        747,
        264,
        4134,
        11,
        286,
        976,
        309,
        281,
        12947,
        2303,
        293,
        286,
        478,
        411,
        11,
        16478,
        11,
        994,
        380,
        589,
        13,
        50872
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_004.mp3"
    },
    {
      "id": 30,
      "avg_logprob": -0.25857335329055786,
      "compression_ratio": 1.515625,
      "end": 512.2300109863281,
      "no_speech_prob": 0.0035379095934331417,
      "seek": 10756,
      "start": 511.07000732421875,
      "temperature": 0.0,
      "text": " Can you think of anything else?",
      "tokens": [
        50882,
        1664,
        291,
        519,
        295,
        1340,
        1646,
        30,
        50940
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_004.mp3"
    },
    {
      "id": 31,
      "avg_logprob": -0.25857335329055786,
      "compression_ratio": 1.515625,
      "end": 515.1900100708008,
      "no_speech_prob": 0.0035379095934331417,
      "seek": 10756,
      "start": 513.6700057983398,
      "temperature": 0.0,
      "text": " You can play around with these things quite a lot.",
      "tokens": [
        51012,
        509,
        393,
        862,
        926,
        365,
        613,
        721,
        1596,
        257,
        688,
        13,
        51088
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_004.mp3"
    },
    {
      "id": 0,
      "avg_logprob": -0.5156269073486328,
      "compression_ratio": 0.27272728085517883,
      "end": 526.2600121498108,
      "no_speech_prob": 0.9428696036338806,
      "seek": 0,
      "start": 524.2000122070312,
      "temperature": 0.0,
      "text": " you",
      "tokens": [
        50364,
        291,
        50467
      ],
      "chunk_source": "secret_to_prompt_engineering_20250326_220619_chunk_005.mp3"
    }
  ],
  "created_at": 1743026839.054326,
  "num_chunks": 5,
  "language": "english"
}