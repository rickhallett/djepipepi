{
  "audio_path": "data/chunks/my_top_6_claude_code_pro_tips_for_ai_coding_mcp_20250324_151153_chunk_008.mp3",
  "text": "tokens are priced at I think $15 per million, right? So this is really good actually. We want to be saving on the output token. So this is an example of how several tips, several techniques can come together, right? Context is king. You want to be paying attention to what your AI coding assistant can see, what's actively inside its context window. And then we also looked at the slash release. So slash release notes tells you what are the Cloud Code engineers cooking up, right? This is super, super important. We also then tapped into Claude's thinking tokens, right? Claude 3.7's reasoning capabilities. So this is super, super powerful. So we've now updated our agent. If I search for, you know, dash dash efficiency, we can see we have that brand new efficiency flag that enables, you can see here, if use token efficiency, we then add this new cool data flag, right? So that looks great. It did save us, you know, some five, 6% of tokens that will definitely add up over time. All right. So tip number five, if I type slash and hit down a couple of times, you're going to see something pretty cool. If I scroll to the bottom, you'll see I have a couple of prompts, right? So a couple of my MCP servers have built-in prompts that I can just run, okay? So you can see fetch has, you know, the obvious one that you would think if I hit enter here, it's going to set up Claude code to have that URL that I can pass in, right? So the key thing I want to show off here is not the fetch tool again. It's the fact that you can use these slash commands to quickly activate one of the prompts. If we look at the model context protocol servers, if we go into fetch again, we can see that there's this prompt, right? And this is really cool. I think this is a pretty underused element of MCP servers. You can create predefined prompts where you have your MCP host pass in specific variables, right? So a simple example here is just fetch, right? So we type slash fetch, and this is a prompt that we now have access to, right? If we also look at, there's another one here for SQLite. Let's hop into something a little bit newer. You can see there's a prompt here that provides a demonstration of how to use SQLite with MCP, right? So if we clear this out and I type MCP, you can see that we do have our",
  "segments": [
    {
      "id": 0,
      "avg_logprob": -0.22187691926956177,
      "compression_ratio": 1.6631579399108887,
      "end": 4.960000038146973,
      "no_speech_prob": 0.07055321335792542,
      "seek": 0,
      "start": 0.0,
      "temperature": 0.0,
      "text": " tokens are priced at I think $15 per million, right? So this is really good actually. We want",
      "tokens": [
        50364,
        22667,
        366,
        30349,
        412,
        286,
        519,
        1848,
        5211,
        680,
        2459,
        11,
        558,
        30,
        407,
        341,
        307,
        534,
        665,
        767,
        13,
        492,
        528,
        50612
      ]
    },
    {
      "id": 1,
      "avg_logprob": -0.22187691926956177,
      "compression_ratio": 1.6631579399108887,
      "end": 9.520000457763672,
      "no_speech_prob": 0.07055321335792542,
      "seek": 0,
      "start": 4.960000038146973,
      "temperature": 0.0,
      "text": " to be saving on the output token. So this is an example of how several tips, several techniques",
      "tokens": [
        50612,
        281,
        312,
        6816,
        322,
        264,
        5598,
        14862,
        13,
        407,
        341,
        307,
        364,
        1365,
        295,
        577,
        2940,
        6082,
        11,
        2940,
        7512,
        50840
      ]
    },
    {
      "id": 2,
      "avg_logprob": -0.22187691926956177,
      "compression_ratio": 1.6631579399108887,
      "end": 14.960000038146973,
      "no_speech_prob": 0.07055321335792542,
      "seek": 0,
      "start": 9.520000457763672,
      "temperature": 0.0,
      "text": " can come together, right? Context is king. You want to be paying attention to what your AI coding",
      "tokens": [
        50840,
        393,
        808,
        1214,
        11,
        558,
        30,
        4839,
        3828,
        307,
        4867,
        13,
        509,
        528,
        281,
        312,
        6229,
        3202,
        281,
        437,
        428,
        7318,
        17720,
        51112
      ]
    },
    {
      "id": 3,
      "avg_logprob": -0.22187691926956177,
      "compression_ratio": 1.6631579399108887,
      "end": 20.31999969482422,
      "no_speech_prob": 0.07055321335792542,
      "seek": 0,
      "start": 14.960000038146973,
      "temperature": 0.0,
      "text": " assistant can see, what's actively inside its context window. And then we also looked at the",
      "tokens": [
        51112,
        10994,
        393,
        536,
        11,
        437,
        311,
        13022,
        1854,
        1080,
        4319,
        4910,
        13,
        400,
        550,
        321,
        611,
        2956,
        412,
        264,
        51380
      ]
    },
    {
      "id": 4,
      "avg_logprob": -0.22187691926956177,
      "compression_ratio": 1.6631579399108887,
      "end": 25.600000381469727,
      "no_speech_prob": 0.07055321335792542,
      "seek": 0,
      "start": 20.31999969482422,
      "temperature": 0.0,
      "text": " slash release. So slash release notes tells you what are the Cloud Code engineers cooking up,",
      "tokens": [
        51380,
        17330,
        4374,
        13,
        407,
        17330,
        4374,
        5570,
        5112,
        291,
        437,
        366,
        264,
        8061,
        15549,
        11955,
        6361,
        493,
        11,
        51644
      ]
    },
    {
      "id": 5,
      "avg_logprob": -0.21966859698295593,
      "compression_ratio": 1.7343173027038574,
      "end": 30.479999542236328,
      "no_speech_prob": 0.20179365575313568,
      "seek": 2560,
      "start": 25.600000381469727,
      "temperature": 0.0,
      "text": " right? This is super, super important. We also then tapped into Claude's thinking tokens, right?",
      "tokens": [
        50364,
        558,
        30,
        639,
        307,
        1687,
        11,
        1687,
        1021,
        13,
        492,
        611,
        550,
        38693,
        666,
        12947,
        2303,
        311,
        1953,
        22667,
        11,
        558,
        30,
        50608
      ]
    },
    {
      "id": 6,
      "avg_logprob": -0.21966859698295593,
      "compression_ratio": 1.7343173027038574,
      "end": 34.720001220703125,
      "no_speech_prob": 0.20179365575313568,
      "seek": 2560,
      "start": 30.479999542236328,
      "temperature": 0.0,
      "text": " Claude 3.7's reasoning capabilities. So this is super, super powerful. So we've now updated our",
      "tokens": [
        50608,
        12947,
        2303,
        805,
        13,
        22,
        311,
        21577,
        10862,
        13,
        407,
        341,
        307,
        1687,
        11,
        1687,
        4005,
        13,
        407,
        321,
        600,
        586,
        493,
        32164,
        527,
        50820
      ]
    },
    {
      "id": 7,
      "avg_logprob": -0.21966859698295593,
      "compression_ratio": 1.7343173027038574,
      "end": 39.20000076293945,
      "no_speech_prob": 0.20179365575313568,
      "seek": 2560,
      "start": 34.720001220703125,
      "temperature": 0.0,
      "text": " agent. If I search for, you know, dash dash efficiency, we can see we have that brand new",
      "tokens": [
        50820,
        9461,
        13,
        759,
        286,
        3164,
        337,
        11,
        291,
        458,
        11,
        8240,
        8240,
        10493,
        11,
        321,
        393,
        536,
        321,
        362,
        300,
        3360,
        777,
        51044
      ]
    },
    {
      "id": 8,
      "avg_logprob": -0.21966859698295593,
      "compression_ratio": 1.7343173027038574,
      "end": 46.880001068115234,
      "no_speech_prob": 0.20179365575313568,
      "seek": 2560,
      "start": 39.20000076293945,
      "temperature": 0.0,
      "text": " efficiency flag that enables, you can see here, if use token efficiency, we then add this new",
      "tokens": [
        51044,
        10493,
        7166,
        300,
        17077,
        11,
        291,
        393,
        536,
        510,
        11,
        498,
        764,
        14862,
        10493,
        11,
        321,
        550,
        909,
        341,
        777,
        51428
      ]
    },
    {
      "id": 9,
      "avg_logprob": -0.21966859698295593,
      "compression_ratio": 1.7343173027038574,
      "end": 53.119998931884766,
      "no_speech_prob": 0.20179365575313568,
      "seek": 2560,
      "start": 46.880001068115234,
      "temperature": 0.0,
      "text": " cool data flag, right? So that looks great. It did save us, you know, some five, 6% of tokens",
      "tokens": [
        51428,
        1627,
        1412,
        7166,
        11,
        558,
        30,
        407,
        300,
        1542,
        869,
        13,
        467,
        630,
        3155,
        505,
        11,
        291,
        458,
        11,
        512,
        1732,
        11,
        1386,
        4,
        295,
        22667,
        51740
      ]
    },
    {
      "id": 10,
      "avg_logprob": -0.17939814925193787,
      "compression_ratio": 1.7073171138763428,
      "end": 59.119998931884766,
      "no_speech_prob": 0.0026316449511796236,
      "seek": 5312,
      "start": 53.119998931884766,
      "temperature": 0.0,
      "text": " that will definitely add up over time. All right. So tip number five, if I type slash and hit down a",
      "tokens": [
        50364,
        300,
        486,
        2138,
        909,
        493,
        670,
        565,
        13,
        1057,
        558,
        13,
        407,
        4125,
        1230,
        1732,
        11,
        498,
        286,
        2010,
        17330,
        293,
        2045,
        760,
        257,
        50664
      ]
    },
    {
      "id": 11,
      "avg_logprob": -0.17939814925193787,
      "compression_ratio": 1.7073171138763428,
      "end": 63.279998779296875,
      "no_speech_prob": 0.0026316449511796236,
      "seek": 5312,
      "start": 59.119998931884766,
      "temperature": 0.0,
      "text": " couple of times, you're going to see something pretty cool. If I scroll to the bottom, you'll see",
      "tokens": [
        50664,
        1916,
        295,
        1413,
        11,
        291,
        434,
        516,
        281,
        536,
        746,
        1238,
        1627,
        13,
        759,
        286,
        11369,
        281,
        264,
        2767,
        11,
        291,
        603,
        536,
        50872
      ]
    },
    {
      "id": 12,
      "avg_logprob": -0.17939814925193787,
      "compression_ratio": 1.7073171138763428,
      "end": 69.5999984741211,
      "no_speech_prob": 0.0026316449511796236,
      "seek": 5312,
      "start": 63.279998779296875,
      "temperature": 0.0,
      "text": " I have a couple of prompts, right? So a couple of my MCP servers have built-in prompts that I can",
      "tokens": [
        50872,
        286,
        362,
        257,
        1916,
        295,
        41095,
        11,
        558,
        30,
        407,
        257,
        1916,
        295,
        452,
        8797,
        47,
        15909,
        362,
        3094,
        12,
        259,
        41095,
        300,
        286,
        393,
        51188
      ]
    },
    {
      "id": 13,
      "avg_logprob": -0.17939814925193787,
      "compression_ratio": 1.7073171138763428,
      "end": 75.36000061035156,
      "no_speech_prob": 0.0026316449511796236,
      "seek": 5312,
      "start": 69.5999984741211,
      "temperature": 0.0,
      "text": " just run, okay? So you can see fetch has, you know, the obvious one that you would think if I hit",
      "tokens": [
        51188,
        445,
        1190,
        11,
        1392,
        30,
        407,
        291,
        393,
        536,
        23673,
        575,
        11,
        291,
        458,
        11,
        264,
        6322,
        472,
        300,
        291,
        576,
        519,
        498,
        286,
        2045,
        51476
      ]
    },
    {
      "id": 14,
      "avg_logprob": -0.17939814925193787,
      "compression_ratio": 1.7073171138763428,
      "end": 81.36000061035156,
      "no_speech_prob": 0.0026316449511796236,
      "seek": 5312,
      "start": 75.36000061035156,
      "temperature": 0.0,
      "text": " enter here, it's going to set up Claude code to have that URL that I can pass in, right? So the",
      "tokens": [
        51476,
        3242,
        510,
        11,
        309,
        311,
        516,
        281,
        992,
        493,
        12947,
        2303,
        3089,
        281,
        362,
        300,
        12905,
        300,
        286,
        393,
        1320,
        294,
        11,
        558,
        30,
        407,
        264,
        51776
      ]
    },
    {
      "id": 15,
      "avg_logprob": -0.16984248161315918,
      "compression_ratio": 1.6790540218353271,
      "end": 85.19999694824219,
      "no_speech_prob": 0.002396698808297515,
      "seek": 8136,
      "start": 81.36000061035156,
      "temperature": 0.0,
      "text": " key thing I want to show off here is not the fetch tool again. It's the fact that you can use",
      "tokens": [
        50364,
        2141,
        551,
        286,
        528,
        281,
        855,
        766,
        510,
        307,
        406,
        264,
        23673,
        2290,
        797,
        13,
        467,
        311,
        264,
        1186,
        300,
        291,
        393,
        764,
        50556
      ]
    },
    {
      "id": 16,
      "avg_logprob": -0.16984248161315918,
      "compression_ratio": 1.6790540218353271,
      "end": 90.55999755859375,
      "no_speech_prob": 0.002396698808297515,
      "seek": 8136,
      "start": 85.19999694824219,
      "temperature": 0.0,
      "text": " these slash commands to quickly activate one of the prompts. If we look at the model context protocol",
      "tokens": [
        50556,
        613,
        17330,
        16901,
        281,
        2661,
        13615,
        472,
        295,
        264,
        41095,
        13,
        759,
        321,
        574,
        412,
        264,
        2316,
        4319,
        10336,
        50824
      ]
    },
    {
      "id": 17,
      "avg_logprob": -0.16984248161315918,
      "compression_ratio": 1.6790540218353271,
      "end": 97.04000091552734,
      "no_speech_prob": 0.002396698808297515,
      "seek": 8136,
      "start": 90.55999755859375,
      "temperature": 0.0,
      "text": " servers, if we go into fetch again, we can see that there's this prompt, right? And this is really cool.",
      "tokens": [
        50824,
        15909,
        11,
        498,
        321,
        352,
        666,
        23673,
        797,
        11,
        321,
        393,
        536,
        300,
        456,
        311,
        341,
        12391,
        11,
        558,
        30,
        400,
        341,
        307,
        534,
        1627,
        13,
        51148
      ]
    },
    {
      "id": 18,
      "avg_logprob": -0.16984248161315918,
      "compression_ratio": 1.6790540218353271,
      "end": 102.63999938964844,
      "no_speech_prob": 0.002396698808297515,
      "seek": 8136,
      "start": 97.04000091552734,
      "temperature": 0.0,
      "text": " I think this is a pretty underused element of MCP servers. You can create predefined prompts where",
      "tokens": [
        51148,
        286,
        519,
        341,
        307,
        257,
        1238,
        833,
        4717,
        4478,
        295,
        8797,
        47,
        15909,
        13,
        509,
        393,
        1884,
        659,
        37716,
        41095,
        689,
        51428
      ]
    },
    {
      "id": 19,
      "avg_logprob": -0.16984248161315918,
      "compression_ratio": 1.6790540218353271,
      "end": 109.68000030517578,
      "no_speech_prob": 0.002396698808297515,
      "seek": 8136,
      "start": 102.63999938964844,
      "temperature": 0.0,
      "text": " you have your MCP host pass in specific variables, right? So a simple example here is just fetch,",
      "tokens": [
        51428,
        291,
        362,
        428,
        8797,
        47,
        3975,
        1320,
        294,
        2685,
        9102,
        11,
        558,
        30,
        407,
        257,
        2199,
        1365,
        510,
        307,
        445,
        23673,
        11,
        51780
      ]
    },
    {
      "id": 20,
      "avg_logprob": -0.1615290641784668,
      "compression_ratio": 1.6017699241638184,
      "end": 113.83999633789062,
      "no_speech_prob": 0.0018386193551123142,
      "seek": 10968,
      "start": 109.68000030517578,
      "temperature": 0.0,
      "text": " right? So we type slash fetch, and this is a prompt that we now have access to, right?",
      "tokens": [
        50364,
        558,
        30,
        407,
        321,
        2010,
        17330,
        23673,
        11,
        293,
        341,
        307,
        257,
        12391,
        300,
        321,
        586,
        362,
        2105,
        281,
        11,
        558,
        30,
        50572
      ]
    },
    {
      "id": 21,
      "avg_logprob": -0.1615290641784668,
      "compression_ratio": 1.6017699241638184,
      "end": 118.63999938964844,
      "no_speech_prob": 0.0018386193551123142,
      "seek": 10968,
      "start": 113.83999633789062,
      "temperature": 0.0,
      "text": " If we also look at, there's another one here for SQLite. Let's hop into something a little bit",
      "tokens": [
        50572,
        759,
        321,
        611,
        574,
        412,
        11,
        456,
        311,
        1071,
        472,
        510,
        337,
        19200,
        642,
        13,
        961,
        311,
        3818,
        666,
        746,
        257,
        707,
        857,
        50812
      ]
    },
    {
      "id": 22,
      "avg_logprob": -0.1615290641784668,
      "compression_ratio": 1.6017699241638184,
      "end": 125.12000274658203,
      "no_speech_prob": 0.0018386193551123142,
      "seek": 10968,
      "start": 118.63999938964844,
      "temperature": 0.0,
      "text": " newer. You can see there's a prompt here that provides a demonstration of how to use SQLite",
      "tokens": [
        50812,
        17628,
        13,
        509,
        393,
        536,
        456,
        311,
        257,
        12391,
        510,
        300,
        6417,
        257,
        16520,
        295,
        577,
        281,
        764,
        19200,
        642,
        51136
      ]
    },
    {
      "id": 23,
      "avg_logprob": -0.1615290641784668,
      "compression_ratio": 1.6017699241638184,
      "end": 130.63999938964844,
      "no_speech_prob": 0.0018386193551123142,
      "seek": 10968,
      "start": 125.12000274658203,
      "temperature": 0.0,
      "text": " with MCP, right? So if we clear this out and I type MCP, you can see that we do have our",
      "tokens": [
        51136,
        365,
        8797,
        47,
        11,
        558,
        30,
        407,
        498,
        321,
        1850,
        341,
        484,
        293,
        286,
        2010,
        8797,
        47,
        11,
        291,
        393,
        536,
        300,
        321,
        360,
        362,
        527,
        51412
      ]
    }
  ],
  "language": "english",
  "duration": 131.05999755859375,
  "timestamp": 1742834652.311471
}