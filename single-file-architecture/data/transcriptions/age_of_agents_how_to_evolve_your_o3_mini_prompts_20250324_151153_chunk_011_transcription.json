{
  "audio_path": "data/chunks/age_of_agents_how_to_evolve_your_o3_mini_prompts_20250324_151153_chunk_011.mp3",
  "text": "this AI agent, okay? So that's what this looks like. We then have a, you know, classic OpenAI call here. We have reasoning effort if we wanna go high. And then, you know, just classic stuff here, right? We're handling the tokens and making sure that we call the right tool. And so this is the AI agent version. So let's go ahead and take a look at the output, right? We just ran that with O3 mini. Let's see how it performed. So you can see it got the answer correct. Let's see what it did, okay? So I'll close this, O3 mini logging session. Top to bottom, make deletion. It's going to remove a. There's the before and after. So you can see it just pulled out that single a from the beginning, right? We have one deletion. And then if we scroll down here, so you can see these are all creating their own prompts. And then we have a nicer, larger edit. I did change it so that it is making a single edit, but you can still remove, you know, chunks of words, right? Chunks of words at a time, right? You wanna be able to remove phrases. Just like as if I were actually editing this. It's important to make the agent or the prompt chain or your AI tooling mirror the process you would take. It's always removing one idea of an edit at a time, right? One piece of an edit at a time. So you can see it removed a, and then removed this, the scratch pad. We can see here, this, the scratch pad, act, and then we have this, right? The scratch pad, act, and memory pattern is going to be really important. And then you can see, look at this. Once it got to this state, it said, perfect, right? The tool decided it was done. And this is really, really cool, right? It's not that different than the prompt chain, right? But it explicitly here called a different tool. There are all the deletions. There's the final transcript slice. And if we scroll to the bottom, we have the original, the final, and the target, okay? So this is correct. Fantastic. So now let's go ahead and look at the benchmark. Instead of just a single problem running against one model at a time, you know, just like in our previous benchmarking video, in order to understand and solve problems with generative AI at scale, you need to create tests. And tests are just another word for eval or benchmarks. For this tool, I'm starting to build up a suite of benchmarks. Right now, I only have 10 problems, but you can see here, I'm taking it.",
  "segments": [
    {
      "id": 0,
      "avg_logprob": -0.21769393980503082,
      "compression_ratio": 1.7796052694320679,
      "end": 1.440000057220459,
      "no_speech_prob": 0.07475928962230682,
      "seek": 0,
      "start": 0.0,
      "temperature": 0.0,
      "text": " this AI agent, okay?",
      "tokens": [
        50364,
        341,
        7318,
        9461,
        11,
        1392,
        30,
        50436
      ]
    },
    {
      "id": 1,
      "avg_logprob": -0.21769393980503082,
      "compression_ratio": 1.7796052694320679,
      "end": 2.740000009536743,
      "no_speech_prob": 0.07475928962230682,
      "seek": 0,
      "start": 1.440000057220459,
      "temperature": 0.0,
      "text": " So that's what this looks like.",
      "tokens": [
        50436,
        407,
        300,
        311,
        437,
        341,
        1542,
        411,
        13,
        50501
      ]
    },
    {
      "id": 2,
      "avg_logprob": -0.21769393980503082,
      "compression_ratio": 1.7796052694320679,
      "end": 5.820000171661377,
      "no_speech_prob": 0.07475928962230682,
      "seek": 0,
      "start": 2.740000009536743,
      "temperature": 0.0,
      "text": " We then have a, you know, classic OpenAI call here.",
      "tokens": [
        50501,
        492,
        550,
        362,
        257,
        11,
        291,
        458,
        11,
        7230,
        7238,
        48698,
        818,
        510,
        13,
        50655
      ]
    },
    {
      "id": 3,
      "avg_logprob": -0.21769393980503082,
      "compression_ratio": 1.7796052694320679,
      "end": 8.079999923706055,
      "no_speech_prob": 0.07475928962230682,
      "seek": 0,
      "start": 5.820000171661377,
      "temperature": 0.0,
      "text": " We have reasoning effort if we wanna go high.",
      "tokens": [
        50655,
        492,
        362,
        21577,
        4630,
        498,
        321,
        1948,
        352,
        1090,
        13,
        50768
      ]
    },
    {
      "id": 4,
      "avg_logprob": -0.21769393980503082,
      "compression_ratio": 1.7796052694320679,
      "end": 9.84000015258789,
      "no_speech_prob": 0.07475928962230682,
      "seek": 0,
      "start": 8.079999923706055,
      "temperature": 0.0,
      "text": " And then, you know, just classic stuff here, right?",
      "tokens": [
        50768,
        400,
        550,
        11,
        291,
        458,
        11,
        445,
        7230,
        1507,
        510,
        11,
        558,
        30,
        50856
      ]
    },
    {
      "id": 5,
      "avg_logprob": -0.21769393980503082,
      "compression_ratio": 1.7796052694320679,
      "end": 11.359999656677246,
      "no_speech_prob": 0.07475928962230682,
      "seek": 0,
      "start": 9.84000015258789,
      "temperature": 0.0,
      "text": " We're handling the tokens",
      "tokens": [
        50856,
        492,
        434,
        13175,
        264,
        22667,
        50932
      ]
    },
    {
      "id": 6,
      "avg_logprob": -0.21769393980503082,
      "compression_ratio": 1.7796052694320679,
      "end": 12.920000076293945,
      "no_speech_prob": 0.07475928962230682,
      "seek": 0,
      "start": 11.359999656677246,
      "temperature": 0.0,
      "text": " and making sure that we call the right tool.",
      "tokens": [
        50932,
        293,
        1455,
        988,
        300,
        321,
        818,
        264,
        558,
        2290,
        13,
        51010
      ]
    },
    {
      "id": 7,
      "avg_logprob": -0.21769393980503082,
      "compression_ratio": 1.7796052694320679,
      "end": 15.359999656677246,
      "no_speech_prob": 0.07475928962230682,
      "seek": 0,
      "start": 12.920000076293945,
      "temperature": 0.0,
      "text": " And so this is the AI agent version.",
      "tokens": [
        51010,
        400,
        370,
        341,
        307,
        264,
        7318,
        9461,
        3037,
        13,
        51132
      ]
    },
    {
      "id": 8,
      "avg_logprob": -0.21769393980503082,
      "compression_ratio": 1.7796052694320679,
      "end": 17.84000015258789,
      "no_speech_prob": 0.07475928962230682,
      "seek": 0,
      "start": 15.359999656677246,
      "temperature": 0.0,
      "text": " So let's go ahead and take a look at the output, right?",
      "tokens": [
        51132,
        407,
        718,
        311,
        352,
        2286,
        293,
        747,
        257,
        574,
        412,
        264,
        5598,
        11,
        558,
        30,
        51256
      ]
    },
    {
      "id": 9,
      "avg_logprob": -0.21769393980503082,
      "compression_ratio": 1.7796052694320679,
      "end": 19.8799991607666,
      "no_speech_prob": 0.07475928962230682,
      "seek": 0,
      "start": 17.84000015258789,
      "temperature": 0.0,
      "text": " We just ran that with O3 mini.",
      "tokens": [
        51256,
        492,
        445,
        5872,
        300,
        365,
        422,
        18,
        8382,
        13,
        51358
      ]
    },
    {
      "id": 10,
      "avg_logprob": -0.21769393980503082,
      "compression_ratio": 1.7796052694320679,
      "end": 20.920000076293945,
      "no_speech_prob": 0.07475928962230682,
      "seek": 0,
      "start": 19.8799991607666,
      "temperature": 0.0,
      "text": " Let's see how it performed.",
      "tokens": [
        51358,
        961,
        311,
        536,
        577,
        309,
        10332,
        13,
        51410
      ]
    },
    {
      "id": 11,
      "avg_logprob": -0.21769393980503082,
      "compression_ratio": 1.7796052694320679,
      "end": 22.739999771118164,
      "no_speech_prob": 0.07475928962230682,
      "seek": 0,
      "start": 20.920000076293945,
      "temperature": 0.0,
      "text": " So you can see it got the answer correct.",
      "tokens": [
        51410,
        407,
        291,
        393,
        536,
        309,
        658,
        264,
        1867,
        3006,
        13,
        51501
      ]
    },
    {
      "id": 12,
      "avg_logprob": -0.21769393980503082,
      "compression_ratio": 1.7796052694320679,
      "end": 24.799999237060547,
      "no_speech_prob": 0.07475928962230682,
      "seek": 0,
      "start": 22.739999771118164,
      "temperature": 0.0,
      "text": " Let's see what it did, okay?",
      "tokens": [
        51501,
        961,
        311,
        536,
        437,
        309,
        630,
        11,
        1392,
        30,
        51604
      ]
    },
    {
      "id": 13,
      "avg_logprob": -0.21769393980503082,
      "compression_ratio": 1.7796052694320679,
      "end": 28.739999771118164,
      "no_speech_prob": 0.07475928962230682,
      "seek": 0,
      "start": 24.799999237060547,
      "temperature": 0.0,
      "text": " So I'll close this, O3 mini logging session.",
      "tokens": [
        51604,
        407,
        286,
        603,
        1998,
        341,
        11,
        422,
        18,
        8382,
        27991,
        5481,
        13,
        51801
      ]
    },
    {
      "id": 14,
      "avg_logprob": -0.18802152574062347,
      "compression_ratio": 1.769736886024475,
      "end": 30.579999923706055,
      "no_speech_prob": 0.1943558156490326,
      "seek": 2874,
      "start": 28.739999771118164,
      "temperature": 0.0,
      "text": " Top to bottom, make deletion.",
      "tokens": [
        50364,
        8840,
        281,
        2767,
        11,
        652,
        1103,
        302,
        313,
        13,
        50456
      ]
    },
    {
      "id": 15,
      "avg_logprob": -0.18802152574062347,
      "compression_ratio": 1.769736886024475,
      "end": 32.02000045776367,
      "no_speech_prob": 0.1943558156490326,
      "seek": 2874,
      "start": 30.579999923706055,
      "temperature": 0.0,
      "text": " It's going to remove a.",
      "tokens": [
        50456,
        467,
        311,
        516,
        281,
        4159,
        257,
        13,
        50528
      ]
    },
    {
      "id": 16,
      "avg_logprob": -0.18802152574062347,
      "compression_ratio": 1.769736886024475,
      "end": 33.220001220703125,
      "no_speech_prob": 0.1943558156490326,
      "seek": 2874,
      "start": 32.02000045776367,
      "temperature": 0.0,
      "text": " There's the before and after.",
      "tokens": [
        50528,
        821,
        311,
        264,
        949,
        293,
        934,
        13,
        50588
      ]
    },
    {
      "id": 17,
      "avg_logprob": -0.18802152574062347,
      "compression_ratio": 1.769736886024475,
      "end": 36.13999938964844,
      "no_speech_prob": 0.1943558156490326,
      "seek": 2874,
      "start": 33.220001220703125,
      "temperature": 0.0,
      "text": " So you can see it just pulled out that single a",
      "tokens": [
        50588,
        407,
        291,
        393,
        536,
        309,
        445,
        7373,
        484,
        300,
        2167,
        257,
        50734
      ]
    },
    {
      "id": 18,
      "avg_logprob": -0.18802152574062347,
      "compression_ratio": 1.769736886024475,
      "end": 37.099998474121094,
      "no_speech_prob": 0.1943558156490326,
      "seek": 2874,
      "start": 36.13999938964844,
      "temperature": 0.0,
      "text": " from the beginning, right?",
      "tokens": [
        50734,
        490,
        264,
        2863,
        11,
        558,
        30,
        50782
      ]
    },
    {
      "id": 19,
      "avg_logprob": -0.18802152574062347,
      "compression_ratio": 1.769736886024475,
      "end": 38.63999938964844,
      "no_speech_prob": 0.1943558156490326,
      "seek": 2874,
      "start": 37.099998474121094,
      "temperature": 0.0,
      "text": " We have one deletion.",
      "tokens": [
        50782,
        492,
        362,
        472,
        1103,
        302,
        313,
        13,
        50859
      ]
    },
    {
      "id": 20,
      "avg_logprob": -0.18802152574062347,
      "compression_ratio": 1.769736886024475,
      "end": 40.459999084472656,
      "no_speech_prob": 0.1943558156490326,
      "seek": 2874,
      "start": 38.63999938964844,
      "temperature": 0.0,
      "text": " And then if we scroll down here,",
      "tokens": [
        50859,
        400,
        550,
        498,
        321,
        11369,
        760,
        510,
        11,
        50950
      ]
    },
    {
      "id": 21,
      "avg_logprob": -0.18802152574062347,
      "compression_ratio": 1.769736886024475,
      "end": 42.70000076293945,
      "no_speech_prob": 0.1943558156490326,
      "seek": 2874,
      "start": 40.459999084472656,
      "temperature": 0.0,
      "text": " so you can see these are all creating their own prompts.",
      "tokens": [
        50950,
        370,
        291,
        393,
        536,
        613,
        366,
        439,
        4084,
        641,
        1065,
        41095,
        13,
        51062
      ]
    },
    {
      "id": 22,
      "avg_logprob": -0.18802152574062347,
      "compression_ratio": 1.769736886024475,
      "end": 45.29999923706055,
      "no_speech_prob": 0.1943558156490326,
      "seek": 2874,
      "start": 42.70000076293945,
      "temperature": 0.0,
      "text": " And then we have a nicer, larger edit.",
      "tokens": [
        51062,
        400,
        550,
        321,
        362,
        257,
        22842,
        11,
        4833,
        8129,
        13,
        51192
      ]
    },
    {
      "id": 23,
      "avg_logprob": -0.18802152574062347,
      "compression_ratio": 1.769736886024475,
      "end": 47.599998474121094,
      "no_speech_prob": 0.1943558156490326,
      "seek": 2874,
      "start": 45.29999923706055,
      "temperature": 0.0,
      "text": " I did change it so that it is making a single edit,",
      "tokens": [
        51192,
        286,
        630,
        1319,
        309,
        370,
        300,
        309,
        307,
        1455,
        257,
        2167,
        8129,
        11,
        51307
      ]
    },
    {
      "id": 24,
      "avg_logprob": -0.18802152574062347,
      "compression_ratio": 1.769736886024475,
      "end": 50.7400016784668,
      "no_speech_prob": 0.1943558156490326,
      "seek": 2874,
      "start": 47.599998474121094,
      "temperature": 0.0,
      "text": " but you can still remove, you know, chunks of words, right?",
      "tokens": [
        51307,
        457,
        291,
        393,
        920,
        4159,
        11,
        291,
        458,
        11,
        24004,
        295,
        2283,
        11,
        558,
        30,
        51464
      ]
    },
    {
      "id": 25,
      "avg_logprob": -0.18802152574062347,
      "compression_ratio": 1.769736886024475,
      "end": 51.97999954223633,
      "no_speech_prob": 0.1943558156490326,
      "seek": 2874,
      "start": 50.7400016784668,
      "temperature": 0.0,
      "text": " Chunks of words at a time, right?",
      "tokens": [
        51464,
        761,
        17627,
        295,
        2283,
        412,
        257,
        565,
        11,
        558,
        30,
        51526
      ]
    },
    {
      "id": 26,
      "avg_logprob": -0.18802152574062347,
      "compression_ratio": 1.769736886024475,
      "end": 53.380001068115234,
      "no_speech_prob": 0.1943558156490326,
      "seek": 2874,
      "start": 51.97999954223633,
      "temperature": 0.0,
      "text": " You wanna be able to remove phrases.",
      "tokens": [
        51526,
        509,
        1948,
        312,
        1075,
        281,
        4159,
        20312,
        13,
        51596
      ]
    },
    {
      "id": 27,
      "avg_logprob": -0.18802152574062347,
      "compression_ratio": 1.769736886024475,
      "end": 55.97999954223633,
      "no_speech_prob": 0.1943558156490326,
      "seek": 2874,
      "start": 53.380001068115234,
      "temperature": 0.0,
      "text": " Just like as if I were actually editing this.",
      "tokens": [
        51596,
        1449,
        411,
        382,
        498,
        286,
        645,
        767,
        10000,
        341,
        13,
        51726
      ]
    },
    {
      "id": 28,
      "avg_logprob": -0.22940762341022491,
      "compression_ratio": 1.9125474691390991,
      "end": 59.220001220703125,
      "no_speech_prob": 0.0003353485371917486,
      "seek": 5598,
      "start": 55.97999954223633,
      "temperature": 0.0,
      "text": " It's important to make the agent or the prompt chain",
      "tokens": [
        50364,
        467,
        311,
        1021,
        281,
        652,
        264,
        9461,
        420,
        264,
        12391,
        5021,
        50526
      ]
    },
    {
      "id": 29,
      "avg_logprob": -0.22940762341022491,
      "compression_ratio": 1.9125474691390991,
      "end": 62.58000183105469,
      "no_speech_prob": 0.0003353485371917486,
      "seek": 5598,
      "start": 59.220001220703125,
      "temperature": 0.0,
      "text": " or your AI tooling mirror the process you would take.",
      "tokens": [
        50526,
        420,
        428,
        7318,
        46593,
        8013,
        264,
        1399,
        291,
        576,
        747,
        13,
        50694
      ]
    },
    {
      "id": 30,
      "avg_logprob": -0.22940762341022491,
      "compression_ratio": 1.9125474691390991,
      "end": 66.69999694824219,
      "no_speech_prob": 0.0003353485371917486,
      "seek": 5598,
      "start": 62.58000183105469,
      "temperature": 0.0,
      "text": " It's always removing one idea of an edit at a time, right?",
      "tokens": [
        50694,
        467,
        311,
        1009,
        12720,
        472,
        1558,
        295,
        364,
        8129,
        412,
        257,
        565,
        11,
        558,
        30,
        50900
      ]
    },
    {
      "id": 31,
      "avg_logprob": -0.22940762341022491,
      "compression_ratio": 1.9125474691390991,
      "end": 68.41999816894531,
      "no_speech_prob": 0.0003353485371917486,
      "seek": 5598,
      "start": 66.69999694824219,
      "temperature": 0.0,
      "text": " One piece of an edit at a time.",
      "tokens": [
        50900,
        1485,
        2522,
        295,
        364,
        8129,
        412,
        257,
        565,
        13,
        50986
      ]
    },
    {
      "id": 32,
      "avg_logprob": -0.22940762341022491,
      "compression_ratio": 1.9125474691390991,
      "end": 70.05999755859375,
      "no_speech_prob": 0.0003353485371917486,
      "seek": 5598,
      "start": 68.41999816894531,
      "temperature": 0.0,
      "text": " So you can see it removed a,",
      "tokens": [
        50986,
        407,
        291,
        393,
        536,
        309,
        7261,
        257,
        11,
        51068
      ]
    },
    {
      "id": 33,
      "avg_logprob": -0.22940762341022491,
      "compression_ratio": 1.9125474691390991,
      "end": 72.5,
      "no_speech_prob": 0.0003353485371917486,
      "seek": 5598,
      "start": 70.05999755859375,
      "temperature": 0.0,
      "text": " and then removed this, the scratch pad.",
      "tokens": [
        51068,
        293,
        550,
        7261,
        341,
        11,
        264,
        8459,
        6887,
        13,
        51190
      ]
    },
    {
      "id": 34,
      "avg_logprob": -0.22940762341022491,
      "compression_ratio": 1.9125474691390991,
      "end": 74.5,
      "no_speech_prob": 0.0003353485371917486,
      "seek": 5598,
      "start": 72.5,
      "temperature": 0.0,
      "text": " We can see here, this, the scratch pad,",
      "tokens": [
        51190,
        492,
        393,
        536,
        510,
        11,
        341,
        11,
        264,
        8459,
        6887,
        11,
        51290
      ]
    },
    {
      "id": 35,
      "avg_logprob": -0.22940762341022491,
      "compression_ratio": 1.9125474691390991,
      "end": 77.19999694824219,
      "no_speech_prob": 0.0003353485371917486,
      "seek": 5598,
      "start": 74.5,
      "temperature": 0.0,
      "text": " act, and then we have this, right?",
      "tokens": [
        51290,
        605,
        11,
        293,
        550,
        321,
        362,
        341,
        11,
        558,
        30,
        51425
      ]
    },
    {
      "id": 36,
      "avg_logprob": -0.22940762341022491,
      "compression_ratio": 1.9125474691390991,
      "end": 78.45999908447266,
      "no_speech_prob": 0.0003353485371917486,
      "seek": 5598,
      "start": 77.19999694824219,
      "temperature": 0.0,
      "text": " The scratch pad, act, and memory pattern",
      "tokens": [
        51425,
        440,
        8459,
        6887,
        11,
        605,
        11,
        293,
        4675,
        5102,
        51488
      ]
    },
    {
      "id": 37,
      "avg_logprob": -0.22940762341022491,
      "compression_ratio": 1.9125474691390991,
      "end": 79.62000274658203,
      "no_speech_prob": 0.0003353485371917486,
      "seek": 5598,
      "start": 78.45999908447266,
      "temperature": 0.0,
      "text": " is going to be really important.",
      "tokens": [
        51488,
        307,
        516,
        281,
        312,
        534,
        1021,
        13,
        51546
      ]
    },
    {
      "id": 38,
      "avg_logprob": -0.22940762341022491,
      "compression_ratio": 1.9125474691390991,
      "end": 81.62000274658203,
      "no_speech_prob": 0.0003353485371917486,
      "seek": 5598,
      "start": 79.62000274658203,
      "temperature": 0.0,
      "text": " And then you can see, look at this.",
      "tokens": [
        51546,
        400,
        550,
        291,
        393,
        536,
        11,
        574,
        412,
        341,
        13,
        51646
      ]
    },
    {
      "id": 39,
      "avg_logprob": -0.22940762341022491,
      "compression_ratio": 1.9125474691390991,
      "end": 84.45999908447266,
      "no_speech_prob": 0.0003353485371917486,
      "seek": 5598,
      "start": 81.62000274658203,
      "temperature": 0.0,
      "text": " Once it got to this state, it said, perfect, right?",
      "tokens": [
        51646,
        3443,
        309,
        658,
        281,
        341,
        1785,
        11,
        309,
        848,
        11,
        2176,
        11,
        558,
        30,
        51788
      ]
    },
    {
      "id": 40,
      "avg_logprob": -0.19399572908878326,
      "compression_ratio": 1.6829267740249634,
      "end": 86.62000274658203,
      "no_speech_prob": 0.0005884025013074279,
      "seek": 8446,
      "start": 84.45999908447266,
      "temperature": 0.0,
      "text": " The tool decided it was done.",
      "tokens": [
        50364,
        440,
        2290,
        3047,
        309,
        390,
        1096,
        13,
        50472
      ]
    },
    {
      "id": 41,
      "avg_logprob": -0.19399572908878326,
      "compression_ratio": 1.6829267740249634,
      "end": 88.94000244140625,
      "no_speech_prob": 0.0005884025013074279,
      "seek": 8446,
      "start": 86.62000274658203,
      "temperature": 0.0,
      "text": " And this is really, really cool, right?",
      "tokens": [
        50472,
        400,
        341,
        307,
        534,
        11,
        534,
        1627,
        11,
        558,
        30,
        50588
      ]
    },
    {
      "id": 42,
      "avg_logprob": -0.19399572908878326,
      "compression_ratio": 1.6829267740249634,
      "end": 90.83999633789062,
      "no_speech_prob": 0.0005884025013074279,
      "seek": 8446,
      "start": 88.94000244140625,
      "temperature": 0.0,
      "text": " It's not that different than the prompt chain, right?",
      "tokens": [
        50588,
        467,
        311,
        406,
        300,
        819,
        813,
        264,
        12391,
        5021,
        11,
        558,
        30,
        50683
      ]
    },
    {
      "id": 43,
      "avg_logprob": -0.19399572908878326,
      "compression_ratio": 1.6829267740249634,
      "end": 93.69999694824219,
      "no_speech_prob": 0.0005884025013074279,
      "seek": 8446,
      "start": 90.83999633789062,
      "temperature": 0.0,
      "text": " But it explicitly here called a different tool.",
      "tokens": [
        50683,
        583,
        309,
        20803,
        510,
        1219,
        257,
        819,
        2290,
        13,
        50826
      ]
    },
    {
      "id": 44,
      "avg_logprob": -0.19399572908878326,
      "compression_ratio": 1.6829267740249634,
      "end": 95.23999786376953,
      "no_speech_prob": 0.0005884025013074279,
      "seek": 8446,
      "start": 93.69999694824219,
      "temperature": 0.0,
      "text": " There are all the deletions.",
      "tokens": [
        50826,
        821,
        366,
        439,
        264,
        1103,
        302,
        626,
        13,
        50903
      ]
    },
    {
      "id": 45,
      "avg_logprob": -0.19399572908878326,
      "compression_ratio": 1.6829267740249634,
      "end": 97.0199966430664,
      "no_speech_prob": 0.0005884025013074279,
      "seek": 8446,
      "start": 95.23999786376953,
      "temperature": 0.0,
      "text": " There's the final transcript slice.",
      "tokens": [
        50903,
        821,
        311,
        264,
        2572,
        24444,
        13153,
        13,
        50992
      ]
    },
    {
      "id": 46,
      "avg_logprob": -0.19399572908878326,
      "compression_ratio": 1.6829267740249634,
      "end": 98.45999908447266,
      "no_speech_prob": 0.0005884025013074279,
      "seek": 8446,
      "start": 97.0199966430664,
      "temperature": 0.0,
      "text": " And if we scroll to the bottom,",
      "tokens": [
        50992,
        400,
        498,
        321,
        11369,
        281,
        264,
        2767,
        11,
        51064
      ]
    },
    {
      "id": 47,
      "avg_logprob": -0.19399572908878326,
      "compression_ratio": 1.6829267740249634,
      "end": 101.66000366210938,
      "no_speech_prob": 0.0005884025013074279,
      "seek": 8446,
      "start": 98.45999908447266,
      "temperature": 0.0,
      "text": " we have the original, the final, and the target, okay?",
      "tokens": [
        51064,
        321,
        362,
        264,
        3380,
        11,
        264,
        2572,
        11,
        293,
        264,
        3779,
        11,
        1392,
        30,
        51224
      ]
    },
    {
      "id": 48,
      "avg_logprob": -0.19399572908878326,
      "compression_ratio": 1.6829267740249634,
      "end": 102.58000183105469,
      "no_speech_prob": 0.0005884025013074279,
      "seek": 8446,
      "start": 101.66000366210938,
      "temperature": 0.0,
      "text": " So this is correct.",
      "tokens": [
        51224,
        407,
        341,
        307,
        3006,
        13,
        51270
      ]
    },
    {
      "id": 49,
      "avg_logprob": -0.19399572908878326,
      "compression_ratio": 1.6829267740249634,
      "end": 103.41999816894531,
      "no_speech_prob": 0.0005884025013074279,
      "seek": 8446,
      "start": 102.58000183105469,
      "temperature": 0.0,
      "text": " Fantastic.",
      "tokens": [
        51270,
        21320,
        13,
        51312
      ]
    },
    {
      "id": 50,
      "avg_logprob": -0.19399572908878326,
      "compression_ratio": 1.6829267740249634,
      "end": 106.83999633789062,
      "no_speech_prob": 0.0005884025013074279,
      "seek": 8446,
      "start": 103.41999816894531,
      "temperature": 0.0,
      "text": " So now let's go ahead and look at the benchmark.",
      "tokens": [
        51312,
        407,
        586,
        718,
        311,
        352,
        2286,
        293,
        574,
        412,
        264,
        18927,
        13,
        51483
      ]
    },
    {
      "id": 51,
      "avg_logprob": -0.19399572908878326,
      "compression_ratio": 1.6829267740249634,
      "end": 110.94000244140625,
      "no_speech_prob": 0.0005884025013074279,
      "seek": 8446,
      "start": 109.33999633789062,
      "temperature": 0.0,
      "text": " Instead of just a single problem",
      "tokens": [
        51608,
        7156,
        295,
        445,
        257,
        2167,
        1154,
        51688
      ]
    },
    {
      "id": 52,
      "avg_logprob": -0.19399572908878326,
      "compression_ratio": 1.6829267740249634,
      "end": 112.91999816894531,
      "no_speech_prob": 0.0005884025013074279,
      "seek": 8446,
      "start": 110.94000244140625,
      "temperature": 0.0,
      "text": " running against one model at a time, you know,",
      "tokens": [
        51688,
        2614,
        1970,
        472,
        2316,
        412,
        257,
        565,
        11,
        291,
        458,
        11,
        51787
      ]
    },
    {
      "id": 53,
      "avg_logprob": -0.23130349814891815,
      "compression_ratio": 1.539170503616333,
      "end": 115.08000183105469,
      "no_speech_prob": 0.0031235332135111094,
      "seek": 11292,
      "start": 112.91999816894531,
      "temperature": 0.20000000298023224,
      "text": " just like in our previous benchmarking video,",
      "tokens": [
        50364,
        445,
        411,
        294,
        527,
        3894,
        18927,
        278,
        960,
        11,
        50472
      ]
    },
    {
      "id": 54,
      "avg_logprob": -0.23130349814891815,
      "compression_ratio": 1.539170503616333,
      "end": 117.63999938964844,
      "no_speech_prob": 0.0031235332135111094,
      "seek": 11292,
      "start": 115.08000183105469,
      "temperature": 0.20000000298023224,
      "text": " in order to understand and solve problems",
      "tokens": [
        50472,
        294,
        1668,
        281,
        1223,
        293,
        5039,
        2740,
        50600
      ]
    },
    {
      "id": 55,
      "avg_logprob": -0.23130349814891815,
      "compression_ratio": 1.539170503616333,
      "end": 121.30000305175781,
      "no_speech_prob": 0.0031235332135111094,
      "seek": 11292,
      "start": 117.63999938964844,
      "temperature": 0.20000000298023224,
      "text": " with generative AI at scale, you need to create tests.",
      "tokens": [
        50600,
        365,
        1337,
        1166,
        7318,
        412,
        4373,
        11,
        291,
        643,
        281,
        1884,
        6921,
        13,
        50783
      ]
    },
    {
      "id": 56,
      "avg_logprob": -0.23130349814891815,
      "compression_ratio": 1.539170503616333,
      "end": 125.4800033569336,
      "no_speech_prob": 0.0031235332135111094,
      "seek": 11292,
      "start": 121.30000305175781,
      "temperature": 0.20000000298023224,
      "text": " And tests are just another word for eval or benchmarks.",
      "tokens": [
        50783,
        400,
        6921,
        366,
        445,
        1071,
        1349,
        337,
        1073,
        304,
        420,
        43751,
        13,
        50992
      ]
    },
    {
      "id": 57,
      "avg_logprob": -0.23130349814891815,
      "compression_ratio": 1.539170503616333,
      "end": 126.31999969482422,
      "no_speech_prob": 0.0031235332135111094,
      "seek": 11292,
      "start": 125.4800033569336,
      "temperature": 0.20000000298023224,
      "text": " For this tool,",
      "tokens": [
        50992,
        1171,
        341,
        2290,
        11,
        51034
      ]
    },
    {
      "id": 58,
      "avg_logprob": -0.23130349814891815,
      "compression_ratio": 1.539170503616333,
      "end": 128.1199951171875,
      "no_speech_prob": 0.0031235332135111094,
      "seek": 11292,
      "start": 126.31999969482422,
      "temperature": 0.20000000298023224,
      "text": " I'm starting to build up a suite of benchmarks.",
      "tokens": [
        51034,
        286,
        478,
        2891,
        281,
        1322,
        493,
        257,
        14205,
        295,
        43751,
        13,
        51124
      ]
    },
    {
      "id": 59,
      "avg_logprob": -0.23130349814891815,
      "compression_ratio": 1.539170503616333,
      "end": 129.75999450683594,
      "no_speech_prob": 0.0031235332135111094,
      "seek": 11292,
      "start": 128.1199951171875,
      "temperature": 0.20000000298023224,
      "text": " Right now, I only have 10 problems,",
      "tokens": [
        51124,
        1779,
        586,
        11,
        286,
        787,
        362,
        1266,
        2740,
        11,
        51206
      ]
    },
    {
      "id": 60,
      "avg_logprob": -0.23130349814891815,
      "compression_ratio": 1.539170503616333,
      "end": 131.8000030517578,
      "no_speech_prob": 0.0031235332135111094,
      "seek": 11292,
      "start": 129.75999450683594,
      "temperature": 0.20000000298023224,
      "text": " but you can see here, I'm taking it.",
      "tokens": [
        51206,
        457,
        291,
        393,
        536,
        510,
        11,
        286,
        478,
        1940,
        309,
        13,
        51308
      ]
    }
  ],
  "language": "english",
  "duration": 131.05999755859375,
  "timestamp": 1742835440.1825678
}