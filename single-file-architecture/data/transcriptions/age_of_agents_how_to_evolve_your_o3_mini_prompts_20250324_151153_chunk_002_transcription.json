{
  "audio_path": "data/chunks/age_of_agents_how_to_evolve_your_o3_mini_prompts_20250324_151153_chunk_002.mp3",
  "text": "AI agents so you can avoid making the same mistakes. Let me introduce you to my new video editing tool, Akka. Okay, so before we dive in to what exactly this tool does, let's go ahead and go over to the Akka agent file. And I'm going to kick off a benchmark that is going to run a series of problems for us using the prompt, the prompt chain, and the AI agent version. This benchmark is going to tell us which one of these levels we need to actually solve this problem. Spawn, run, Akka agent. And this benchmark is going to run in the background in parallel to us and we'll circle back to this to answer questions like, are reasoning models always better than base models? Do GPT-4 agents beat O3 mini prompts? Does a O3 mini prompt beat a O3 prompt chain? How much more expensive is an AI agent versus a prompt chain? And more importantly, is it worth it? So what's Akka, how does it work, and why did I build it? Akka is the fastest way to remove filler words, repeats, and nonsense out of your videos. So you can go from scripts that, you know, have stuttering in them, that have filler words and repeats, and then it'll end up looking like this. The scratchpad active memory pattern is going to be really important for rolling out useful personal AI assistance. The key here is that it focuses on the transcript. So let's walk through the process. It all starts out with word level transcript. You can use a whisper transcription tool to generate this type of JSON structure, right? We have all the text, and then we have the individual words at specific start and end times. This is the raw script, what I like to call the base edit, that we're going to have our AI agents edit for us. So if we scroll down here, you can see we have, you know, a couple sentences worth. Step two is create slices. So it is too much work to pass off an entire transcription for, you know, a 30, 40, 50 minute long video to any language model. If we open up a transcription here, you can see we have 350,000 tokens in this single transcript, and that includes all the text here.",
  "segments": [
    {
      "id": 0,
      "avg_logprob": -0.21875503659248352,
      "compression_ratio": 1.5862069129943848,
      "end": 6.400000095367432,
      "no_speech_prob": 0.007576774340122938,
      "seek": 0,
      "start": 0.0,
      "temperature": 0.0,
      "text": " AI agents so you can avoid making the same mistakes. Let me introduce you to my new video",
      "tokens": [
        50364,
        7318,
        12554,
        370,
        291,
        393,
        5042,
        1455,
        264,
        912,
        8038,
        13,
        961,
        385,
        5366,
        291,
        281,
        452,
        777,
        960,
        50684
      ]
    },
    {
      "id": 1,
      "avg_logprob": -0.21875503659248352,
      "compression_ratio": 1.5862069129943848,
      "end": 15.84000015258789,
      "no_speech_prob": 0.007576774340122938,
      "seek": 0,
      "start": 6.400000095367432,
      "temperature": 0.0,
      "text": " editing tool, Akka. Okay, so before we dive in to what exactly this tool does, let's go ahead and",
      "tokens": [
        50684,
        10000,
        2290,
        11,
        9629,
        2330,
        13,
        1033,
        11,
        370,
        949,
        321,
        9192,
        294,
        281,
        437,
        2293,
        341,
        2290,
        775,
        11,
        718,
        311,
        352,
        2286,
        293,
        51156
      ]
    },
    {
      "id": 2,
      "avg_logprob": -0.21875503659248352,
      "compression_ratio": 1.5862069129943848,
      "end": 21.440000534057617,
      "no_speech_prob": 0.007576774340122938,
      "seek": 0,
      "start": 15.84000015258789,
      "temperature": 0.0,
      "text": " go over to the Akka agent file. And I'm going to kick off a benchmark that is going to run",
      "tokens": [
        51156,
        352,
        670,
        281,
        264,
        9629,
        2330,
        9461,
        3991,
        13,
        400,
        286,
        478,
        516,
        281,
        4437,
        766,
        257,
        18927,
        300,
        307,
        516,
        281,
        1190,
        51436
      ]
    },
    {
      "id": 3,
      "avg_logprob": -0.21875503659248352,
      "compression_ratio": 1.5862069129943848,
      "end": 28.0,
      "no_speech_prob": 0.007576774340122938,
      "seek": 0,
      "start": 21.440000534057617,
      "temperature": 0.0,
      "text": " a series of problems for us using the prompt, the prompt chain, and the AI agent version.",
      "tokens": [
        51436,
        257,
        2638,
        295,
        2740,
        337,
        505,
        1228,
        264,
        12391,
        11,
        264,
        12391,
        5021,
        11,
        293,
        264,
        7318,
        9461,
        3037,
        13,
        51764
      ]
    },
    {
      "id": 4,
      "avg_logprob": -0.2188090682029724,
      "compression_ratio": 1.6130434274673462,
      "end": 32.880001068115234,
      "no_speech_prob": 0.0005192969110794365,
      "seek": 2800,
      "start": 28.0,
      "temperature": 0.0,
      "text": " This benchmark is going to tell us which one of these levels we need to actually solve this",
      "tokens": [
        50364,
        639,
        18927,
        307,
        516,
        281,
        980,
        505,
        597,
        472,
        295,
        613,
        4358,
        321,
        643,
        281,
        767,
        5039,
        341,
        50608
      ]
    },
    {
      "id": 5,
      "avg_logprob": -0.2188090682029724,
      "compression_ratio": 1.6130434274673462,
      "end": 39.52000045776367,
      "no_speech_prob": 0.0005192969110794365,
      "seek": 2800,
      "start": 32.880001068115234,
      "temperature": 0.0,
      "text": " problem. Spawn, run, Akka agent. And this benchmark is going to run in the background in parallel to",
      "tokens": [
        50608,
        1154,
        13,
        1738,
        11251,
        11,
        1190,
        11,
        9629,
        2330,
        9461,
        13,
        400,
        341,
        18927,
        307,
        516,
        281,
        1190,
        294,
        264,
        3678,
        294,
        8952,
        281,
        50940
      ]
    },
    {
      "id": 6,
      "avg_logprob": -0.2188090682029724,
      "compression_ratio": 1.6130434274673462,
      "end": 44.400001525878906,
      "no_speech_prob": 0.0005192969110794365,
      "seek": 2800,
      "start": 39.52000045776367,
      "temperature": 0.0,
      "text": " us and we'll circle back to this to answer questions like, are reasoning models always",
      "tokens": [
        50940,
        505,
        293,
        321,
        603,
        6329,
        646,
        281,
        341,
        281,
        1867,
        1651,
        411,
        11,
        366,
        21577,
        5245,
        1009,
        51184
      ]
    },
    {
      "id": 7,
      "avg_logprob": -0.2188090682029724,
      "compression_ratio": 1.6130434274673462,
      "end": 52.560001373291016,
      "no_speech_prob": 0.0005192969110794365,
      "seek": 2800,
      "start": 44.400001525878906,
      "temperature": 0.0,
      "text": " better than base models? Do GPT-4 agents beat O3 mini prompts? Does a O3 mini prompt beat a",
      "tokens": [
        51184,
        1101,
        813,
        3096,
        5245,
        30,
        1144,
        26039,
        51,
        12,
        19,
        12554,
        4224,
        422,
        18,
        8382,
        41095,
        30,
        4402,
        257,
        422,
        18,
        8382,
        12391,
        4224,
        257,
        51592
      ]
    },
    {
      "id": 8,
      "avg_logprob": -0.19102822244167328,
      "compression_ratio": 1.6620689630508423,
      "end": 59.040000915527344,
      "no_speech_prob": 0.02096319943666458,
      "seek": 5256,
      "start": 52.560001373291016,
      "temperature": 0.0,
      "text": " O3 prompt chain? How much more expensive is an AI agent versus a prompt chain? And more importantly,",
      "tokens": [
        50364,
        422,
        18,
        12391,
        5021,
        30,
        1012,
        709,
        544,
        5124,
        307,
        364,
        7318,
        9461,
        5717,
        257,
        12391,
        5021,
        30,
        400,
        544,
        8906,
        11,
        50688
      ]
    },
    {
      "id": 9,
      "avg_logprob": -0.19102822244167328,
      "compression_ratio": 1.6620689630508423,
      "end": 66.08000183105469,
      "no_speech_prob": 0.02096319943666458,
      "seek": 5256,
      "start": 59.040000915527344,
      "temperature": 0.0,
      "text": " is it worth it? So what's Akka, how does it work, and why did I build it? Akka is the fastest way",
      "tokens": [
        50688,
        307,
        309,
        3163,
        309,
        30,
        407,
        437,
        311,
        9629,
        2330,
        11,
        577,
        775,
        309,
        589,
        11,
        293,
        983,
        630,
        286,
        1322,
        309,
        30,
        9629,
        2330,
        307,
        264,
        14573,
        636,
        51040
      ]
    },
    {
      "id": 10,
      "avg_logprob": -0.19102822244167328,
      "compression_ratio": 1.6620689630508423,
      "end": 73.44000244140625,
      "no_speech_prob": 0.02096319943666458,
      "seek": 5256,
      "start": 66.08000183105469,
      "temperature": 0.0,
      "text": " to remove filler words, repeats, and nonsense out of your videos. So you can go from scripts that,",
      "tokens": [
        51040,
        281,
        4159,
        34676,
        2283,
        11,
        35038,
        11,
        293,
        14925,
        484,
        295,
        428,
        2145,
        13,
        407,
        291,
        393,
        352,
        490,
        23294,
        300,
        11,
        51408
      ]
    },
    {
      "id": 11,
      "avg_logprob": -0.19102822244167328,
      "compression_ratio": 1.6620689630508423,
      "end": 77.5999984741211,
      "no_speech_prob": 0.02096319943666458,
      "seek": 5256,
      "start": 73.44000244140625,
      "temperature": 0.0,
      "text": " you know, have stuttering in them, that have filler words and repeats, and then it'll end up",
      "tokens": [
        51408,
        291,
        458,
        11,
        362,
        342,
        32224,
        294,
        552,
        11,
        300,
        362,
        34676,
        2283,
        293,
        35038,
        11,
        293,
        550,
        309,
        603,
        917,
        493,
        51616
      ]
    },
    {
      "id": 12,
      "avg_logprob": -0.19102822244167328,
      "compression_ratio": 1.6620689630508423,
      "end": 82.08000183105469,
      "no_speech_prob": 0.02096319943666458,
      "seek": 5256,
      "start": 77.5999984741211,
      "temperature": 0.0,
      "text": " looking like this. The scratchpad active memory pattern is going to be really important for",
      "tokens": [
        51616,
        1237,
        411,
        341,
        13,
        440,
        8459,
        13647,
        4967,
        4675,
        5102,
        307,
        516,
        281,
        312,
        534,
        1021,
        337,
        51840
      ]
    },
    {
      "id": 13,
      "avg_logprob": -0.17516165971755981,
      "compression_ratio": 1.6521738767623901,
      "end": 88.4800033569336,
      "no_speech_prob": 0.0008426300482824445,
      "seek": 8208,
      "start": 82.08000183105469,
      "temperature": 0.0,
      "text": " rolling out useful personal AI assistance. The key here is that it focuses on the transcript. So",
      "tokens": [
        50364,
        9439,
        484,
        4420,
        2973,
        7318,
        9683,
        13,
        440,
        2141,
        510,
        307,
        300,
        309,
        16109,
        322,
        264,
        24444,
        13,
        407,
        50684
      ]
    },
    {
      "id": 14,
      "avg_logprob": -0.17516165971755981,
      "compression_ratio": 1.6521738767623901,
      "end": 94.0,
      "no_speech_prob": 0.0008426300482824445,
      "seek": 8208,
      "start": 88.4800033569336,
      "temperature": 0.0,
      "text": " let's walk through the process. It all starts out with word level transcript. You can use a whisper",
      "tokens": [
        50684,
        718,
        311,
        1792,
        807,
        264,
        1399,
        13,
        467,
        439,
        3719,
        484,
        365,
        1349,
        1496,
        24444,
        13,
        509,
        393,
        764,
        257,
        26018,
        50960
      ]
    },
    {
      "id": 15,
      "avg_logprob": -0.17516165971755981,
      "compression_ratio": 1.6521738767623901,
      "end": 99.19999694824219,
      "no_speech_prob": 0.0008426300482824445,
      "seek": 8208,
      "start": 94.0,
      "temperature": 0.0,
      "text": " transcription tool to generate this type of JSON structure, right? We have all the text, and then",
      "tokens": [
        50960,
        35288,
        2290,
        281,
        8460,
        341,
        2010,
        295,
        31828,
        3877,
        11,
        558,
        30,
        492,
        362,
        439,
        264,
        2487,
        11,
        293,
        550,
        51220
      ]
    },
    {
      "id": 16,
      "avg_logprob": -0.17516165971755981,
      "compression_ratio": 1.6521738767623901,
      "end": 104.72000122070312,
      "no_speech_prob": 0.0008426300482824445,
      "seek": 8208,
      "start": 99.19999694824219,
      "temperature": 0.0,
      "text": " we have the individual words at specific start and end times. This is the raw script, what I like to",
      "tokens": [
        51220,
        321,
        362,
        264,
        2609,
        2283,
        412,
        2685,
        722,
        293,
        917,
        1413,
        13,
        639,
        307,
        264,
        8936,
        5755,
        11,
        437,
        286,
        411,
        281,
        51496
      ]
    },
    {
      "id": 17,
      "avg_logprob": -0.17516165971755981,
      "compression_ratio": 1.6521738767623901,
      "end": 109.27999877929688,
      "no_speech_prob": 0.0008426300482824445,
      "seek": 8208,
      "start": 104.72000122070312,
      "temperature": 0.0,
      "text": " call the base edit, that we're going to have our AI agents edit for us. So if we scroll down here,",
      "tokens": [
        51496,
        818,
        264,
        3096,
        8129,
        11,
        300,
        321,
        434,
        516,
        281,
        362,
        527,
        7318,
        12554,
        8129,
        337,
        505,
        13,
        407,
        498,
        321,
        11369,
        760,
        510,
        11,
        51724
      ]
    },
    {
      "id": 18,
      "avg_logprob": -0.18231964111328125,
      "compression_ratio": 1.6168224811553955,
      "end": 114.63999938964844,
      "no_speech_prob": 0.002148580737411976,
      "seek": 10928,
      "start": 109.27999877929688,
      "temperature": 0.0,
      "text": " you can see we have, you know, a couple sentences worth. Step two is create slices. So it is too",
      "tokens": [
        50364,
        291,
        393,
        536,
        321,
        362,
        11,
        291,
        458,
        11,
        257,
        1916,
        16579,
        3163,
        13,
        5470,
        732,
        307,
        1884,
        19793,
        13,
        407,
        309,
        307,
        886,
        50632
      ]
    },
    {
      "id": 19,
      "avg_logprob": -0.18231964111328125,
      "compression_ratio": 1.6168224811553955,
      "end": 120.4000015258789,
      "no_speech_prob": 0.002148580737411976,
      "seek": 10928,
      "start": 114.63999938964844,
      "temperature": 0.0,
      "text": " much work to pass off an entire transcription for, you know, a 30, 40, 50 minute long video",
      "tokens": [
        50632,
        709,
        589,
        281,
        1320,
        766,
        364,
        2302,
        35288,
        337,
        11,
        291,
        458,
        11,
        257,
        2217,
        11,
        3356,
        11,
        2625,
        3456,
        938,
        960,
        50920
      ]
    },
    {
      "id": 20,
      "avg_logprob": -0.18231964111328125,
      "compression_ratio": 1.6168224811553955,
      "end": 127.36000061035156,
      "no_speech_prob": 0.002148580737411976,
      "seek": 10928,
      "start": 120.4000015258789,
      "temperature": 0.0,
      "text": " to any language model. If we open up a transcription here, you can see we have 350,000",
      "tokens": [
        50920,
        281,
        604,
        2856,
        2316,
        13,
        759,
        321,
        1269,
        493,
        257,
        35288,
        510,
        11,
        291,
        393,
        536,
        321,
        362,
        18065,
        11,
        1360,
        51268
      ]
    },
    {
      "id": 21,
      "avg_logprob": -0.18231964111328125,
      "compression_ratio": 1.6168224811553955,
      "end": 131.0399932861328,
      "no_speech_prob": 0.002148580737411976,
      "seek": 10928,
      "start": 127.36000061035156,
      "temperature": 0.0,
      "text": " tokens in this single transcript, and that includes all the text here.",
      "tokens": [
        51268,
        22667,
        294,
        341,
        2167,
        24444,
        11,
        293,
        300,
        5974,
        439,
        264,
        2487,
        510,
        13,
        51452
      ]
    }
  ],
  "language": "english",
  "duration": 131.05999755859375,
  "timestamp": 1742835316.211422
}