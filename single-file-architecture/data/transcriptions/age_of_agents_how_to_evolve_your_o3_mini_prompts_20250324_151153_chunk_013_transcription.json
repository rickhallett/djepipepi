{
  "audio_path": "data/chunks/age_of_agents_how_to_evolve_your_o3_mini_prompts_20250324_151153_chunk_013.mp3",
  "text": "Interestingly, the O3 mini gets the individual prompt wrong, but you can see here the prompt chain and the AI agent get this problem correct. Okay, so very cool to see that. Now we're moving to problem two in GPT 4.0. So we can see here this problem, every single version of GPT 4.0 got it incorrect, right? False, false, false. So this is a longer, more interesting edit. Let's see how O3 mini has performed. True, true, and then interestingly, false. So this is a case where more compute is not better. Interesting to see that. And we can go on down the line here, right? I'm not gonna bore you with every single thing here. You can see here that this prompt chain over edited. So it ended up cutting out way too much text, right? It just kept going down. This is the, you know, one of the problems with just a prompt chain that just runs rampant. It just edited everything down, okay? And it didn't have the tools, right? It didn't have the tools available to reset or fix the edit. So if we continue down the line here, it's that same problem with O3 mini. You can see here, prompt was wrong. The chain was correct, but the AI agent was not correct. And so you can see here, it looks close, right? If we pull this text out, right? And we can just highlight this to see. So let's go ahead and open up everyone's favorite local model tool. We're going to be using Olama and here's where it branched. And we're going to be using a tool I'm building out called Benchy. And what we wanted was, and a tool I'm building out called Benchy, okay? So this is a better failure in the AI agent. You know, it made a clean edit, but it wasn't the exact edit we're looking for. And this is where making the prompt more specific to the exact editing decisions and subjective editing tastes is going to be more and more important, okay? There are many of these in here that could have been marked right, specifically with the O3 mini model that could be marked true, could be marked correct, but it doesn't exactly hit the target text, right? This is where something like 11 scene distance or some rough string comparison framework can come in, okay? So we're going to skip through these, right? There's a lot going on. If we just scroll all the way to the bottom, we can get some high level overarching benchmark results, okay? Here's everything.",
  "segments": [
    {
      "id": 0,
      "avg_logprob": -0.2537037134170532,
      "compression_ratio": 1.7186311483383179,
      "end": 5.0,
      "no_speech_prob": 0.0850805938243866,
      "seek": 0,
      "start": 0.0,
      "temperature": 0.0,
      "text": " Interestingly, the O3 mini gets the individual prompt wrong,",
      "tokens": [
        50364,
        30564,
        11,
        264,
        422,
        18,
        8382,
        2170,
        264,
        2609,
        12391,
        2085,
        11,
        50614
      ]
    },
    {
      "id": 1,
      "avg_logprob": -0.2537037134170532,
      "compression_ratio": 1.7186311483383179,
      "end": 7.139999866485596,
      "no_speech_prob": 0.0850805938243866,
      "seek": 0,
      "start": 5.519999980926514,
      "temperature": 0.0,
      "text": " but you can see here the prompt chain",
      "tokens": [
        50640,
        457,
        291,
        393,
        536,
        510,
        264,
        12391,
        5021,
        50721
      ]
    },
    {
      "id": 2,
      "avg_logprob": -0.2537037134170532,
      "compression_ratio": 1.7186311483383179,
      "end": 9.84000015258789,
      "no_speech_prob": 0.0850805938243866,
      "seek": 0,
      "start": 7.139999866485596,
      "temperature": 0.0,
      "text": " and the AI agent get this problem correct.",
      "tokens": [
        50721,
        293,
        264,
        7318,
        9461,
        483,
        341,
        1154,
        3006,
        13,
        50856
      ]
    },
    {
      "id": 3,
      "avg_logprob": -0.2537037134170532,
      "compression_ratio": 1.7186311483383179,
      "end": 11.800000190734863,
      "no_speech_prob": 0.0850805938243866,
      "seek": 0,
      "start": 9.84000015258789,
      "temperature": 0.0,
      "text": " Okay, so very cool to see that.",
      "tokens": [
        50856,
        1033,
        11,
        370,
        588,
        1627,
        281,
        536,
        300,
        13,
        50954
      ]
    },
    {
      "id": 4,
      "avg_logprob": -0.2537037134170532,
      "compression_ratio": 1.7186311483383179,
      "end": 14.640000343322754,
      "no_speech_prob": 0.0850805938243866,
      "seek": 0,
      "start": 11.800000190734863,
      "temperature": 0.0,
      "text": " Now we're moving to problem two in GPT 4.0.",
      "tokens": [
        50954,
        823,
        321,
        434,
        2684,
        281,
        1154,
        732,
        294,
        26039,
        51,
        1017,
        13,
        15,
        13,
        51096
      ]
    },
    {
      "id": 5,
      "avg_logprob": -0.2537037134170532,
      "compression_ratio": 1.7186311483383179,
      "end": 16.920000076293945,
      "no_speech_prob": 0.0850805938243866,
      "seek": 0,
      "start": 14.640000343322754,
      "temperature": 0.0,
      "text": " So we can see here this problem,",
      "tokens": [
        51096,
        407,
        321,
        393,
        536,
        510,
        341,
        1154,
        11,
        51210
      ]
    },
    {
      "id": 6,
      "avg_logprob": -0.2537037134170532,
      "compression_ratio": 1.7186311483383179,
      "end": 20.15999984741211,
      "no_speech_prob": 0.0850805938243866,
      "seek": 0,
      "start": 16.920000076293945,
      "temperature": 0.0,
      "text": " every single version of GPT 4.0 got it incorrect, right?",
      "tokens": [
        51210,
        633,
        2167,
        3037,
        295,
        26039,
        51,
        1017,
        13,
        15,
        658,
        309,
        18424,
        11,
        558,
        30,
        51372
      ]
    },
    {
      "id": 7,
      "avg_logprob": -0.2537037134170532,
      "compression_ratio": 1.7186311483383179,
      "end": 22.440000534057617,
      "no_speech_prob": 0.0850805938243866,
      "seek": 0,
      "start": 20.15999984741211,
      "temperature": 0.0,
      "text": " False, false, false.",
      "tokens": [
        51372,
        50040,
        11,
        7908,
        11,
        7908,
        13,
        51486
      ]
    },
    {
      "id": 8,
      "avg_logprob": -0.2537037134170532,
      "compression_ratio": 1.7186311483383179,
      "end": 24.440000534057617,
      "no_speech_prob": 0.0850805938243866,
      "seek": 0,
      "start": 22.440000534057617,
      "temperature": 0.0,
      "text": " So this is a longer, more interesting edit.",
      "tokens": [
        51486,
        407,
        341,
        307,
        257,
        2854,
        11,
        544,
        1880,
        8129,
        13,
        51586
      ]
    },
    {
      "id": 9,
      "avg_logprob": -0.2537037134170532,
      "compression_ratio": 1.7186311483383179,
      "end": 27.15999984741211,
      "no_speech_prob": 0.0850805938243866,
      "seek": 0,
      "start": 24.440000534057617,
      "temperature": 0.0,
      "text": " Let's see how O3 mini has performed.",
      "tokens": [
        51586,
        961,
        311,
        536,
        577,
        422,
        18,
        8382,
        575,
        10332,
        13,
        51722
      ]
    },
    {
      "id": 10,
      "avg_logprob": -0.2537037134170532,
      "compression_ratio": 1.7186311483383179,
      "end": 29.760000228881836,
      "no_speech_prob": 0.0850805938243866,
      "seek": 0,
      "start": 27.15999984741211,
      "temperature": 0.0,
      "text": " True, true, and then interestingly, false.",
      "tokens": [
        51722,
        13587,
        11,
        2074,
        11,
        293,
        550,
        25873,
        11,
        7908,
        13,
        51852
      ]
    },
    {
      "id": 11,
      "avg_logprob": -0.21431326866149902,
      "compression_ratio": 1.850649356842041,
      "end": 31.920000076293945,
      "no_speech_prob": 6.922166357981041e-05,
      "seek": 2976,
      "start": 29.760000228881836,
      "temperature": 0.0,
      "text": " So this is a case where more compute is not better.",
      "tokens": [
        50364,
        407,
        341,
        307,
        257,
        1389,
        689,
        544,
        14722,
        307,
        406,
        1101,
        13,
        50472
      ]
    },
    {
      "id": 12,
      "avg_logprob": -0.21431326866149902,
      "compression_ratio": 1.850649356842041,
      "end": 32.900001525878906,
      "no_speech_prob": 6.922166357981041e-05,
      "seek": 2976,
      "start": 31.920000076293945,
      "temperature": 0.0,
      "text": " Interesting to see that.",
      "tokens": [
        50472,
        14711,
        281,
        536,
        300,
        13,
        50521
      ]
    },
    {
      "id": 13,
      "avg_logprob": -0.21431326866149902,
      "compression_ratio": 1.850649356842041,
      "end": 35.47999954223633,
      "no_speech_prob": 6.922166357981041e-05,
      "seek": 2976,
      "start": 32.900001525878906,
      "temperature": 0.0,
      "text": " And we can go on down the line here, right?",
      "tokens": [
        50521,
        400,
        321,
        393,
        352,
        322,
        760,
        264,
        1622,
        510,
        11,
        558,
        30,
        50650
      ]
    },
    {
      "id": 14,
      "avg_logprob": -0.21431326866149902,
      "compression_ratio": 1.850649356842041,
      "end": 37.720001220703125,
      "no_speech_prob": 6.922166357981041e-05,
      "seek": 2976,
      "start": 35.47999954223633,
      "temperature": 0.0,
      "text": " I'm not gonna bore you with every single thing here.",
      "tokens": [
        50650,
        286,
        478,
        406,
        799,
        26002,
        291,
        365,
        633,
        2167,
        551,
        510,
        13,
        50762
      ]
    },
    {
      "id": 15,
      "avg_logprob": -0.21431326866149902,
      "compression_ratio": 1.850649356842041,
      "end": 41.599998474121094,
      "no_speech_prob": 6.922166357981041e-05,
      "seek": 2976,
      "start": 37.720001220703125,
      "temperature": 0.0,
      "text": " You can see here that this prompt chain over edited.",
      "tokens": [
        50762,
        509,
        393,
        536,
        510,
        300,
        341,
        12391,
        5021,
        670,
        23016,
        13,
        50956
      ]
    },
    {
      "id": 16,
      "avg_logprob": -0.21431326866149902,
      "compression_ratio": 1.850649356842041,
      "end": 45.08000183105469,
      "no_speech_prob": 6.922166357981041e-05,
      "seek": 2976,
      "start": 41.599998474121094,
      "temperature": 0.0,
      "text": " So it ended up cutting out way too much text, right?",
      "tokens": [
        50956,
        407,
        309,
        4590,
        493,
        6492,
        484,
        636,
        886,
        709,
        2487,
        11,
        558,
        30,
        51130
      ]
    },
    {
      "id": 17,
      "avg_logprob": -0.21431326866149902,
      "compression_ratio": 1.850649356842041,
      "end": 46.380001068115234,
      "no_speech_prob": 6.922166357981041e-05,
      "seek": 2976,
      "start": 45.08000183105469,
      "temperature": 0.0,
      "text": " It just kept going down.",
      "tokens": [
        51130,
        467,
        445,
        4305,
        516,
        760,
        13,
        51195
      ]
    },
    {
      "id": 18,
      "avg_logprob": -0.21431326866149902,
      "compression_ratio": 1.850649356842041,
      "end": 48.36000061035156,
      "no_speech_prob": 6.922166357981041e-05,
      "seek": 2976,
      "start": 46.380001068115234,
      "temperature": 0.0,
      "text": " This is the, you know, one of the problems",
      "tokens": [
        51195,
        639,
        307,
        264,
        11,
        291,
        458,
        11,
        472,
        295,
        264,
        2740,
        51294
      ]
    },
    {
      "id": 19,
      "avg_logprob": -0.21431326866149902,
      "compression_ratio": 1.850649356842041,
      "end": 50.68000030517578,
      "no_speech_prob": 6.922166357981041e-05,
      "seek": 2976,
      "start": 48.36000061035156,
      "temperature": 0.0,
      "text": " with just a prompt chain that just runs rampant.",
      "tokens": [
        51294,
        365,
        445,
        257,
        12391,
        5021,
        300,
        445,
        6676,
        12428,
        394,
        13,
        51410
      ]
    },
    {
      "id": 20,
      "avg_logprob": -0.21431326866149902,
      "compression_ratio": 1.850649356842041,
      "end": 52.70000076293945,
      "no_speech_prob": 6.922166357981041e-05,
      "seek": 2976,
      "start": 50.68000030517578,
      "temperature": 0.0,
      "text": " It just edited everything down, okay?",
      "tokens": [
        51410,
        467,
        445,
        23016,
        1203,
        760,
        11,
        1392,
        30,
        51511
      ]
    },
    {
      "id": 21,
      "avg_logprob": -0.21431326866149902,
      "compression_ratio": 1.850649356842041,
      "end": 54.47999954223633,
      "no_speech_prob": 6.922166357981041e-05,
      "seek": 2976,
      "start": 52.70000076293945,
      "temperature": 0.0,
      "text": " And it didn't have the tools, right?",
      "tokens": [
        51511,
        400,
        309,
        994,
        380,
        362,
        264,
        3873,
        11,
        558,
        30,
        51600
      ]
    },
    {
      "id": 22,
      "avg_logprob": -0.21431326866149902,
      "compression_ratio": 1.850649356842041,
      "end": 57.400001525878906,
      "no_speech_prob": 6.922166357981041e-05,
      "seek": 2976,
      "start": 54.47999954223633,
      "temperature": 0.0,
      "text": " It didn't have the tools available to reset or fix the edit.",
      "tokens": [
        51600,
        467,
        994,
        380,
        362,
        264,
        3873,
        2435,
        281,
        14322,
        420,
        3191,
        264,
        8129,
        13,
        51746
      ]
    },
    {
      "id": 23,
      "avg_logprob": -0.21431326866149902,
      "compression_ratio": 1.850649356842041,
      "end": 59.279998779296875,
      "no_speech_prob": 6.922166357981041e-05,
      "seek": 2976,
      "start": 57.400001525878906,
      "temperature": 0.0,
      "text": " So if we continue down the line here,",
      "tokens": [
        51746,
        407,
        498,
        321,
        2354,
        760,
        264,
        1622,
        510,
        11,
        51840
      ]
    },
    {
      "id": 24,
      "avg_logprob": -0.20788419246673584,
      "compression_ratio": 1.7827585935592651,
      "end": 61.20000076293945,
      "no_speech_prob": 0.0020829332061111927,
      "seek": 5928,
      "start": 59.7599983215332,
      "temperature": 0.0,
      "text": " it's that same problem with O3 mini.",
      "tokens": [
        50388,
        309,
        311,
        300,
        912,
        1154,
        365,
        422,
        18,
        8382,
        13,
        50460
      ]
    },
    {
      "id": 25,
      "avg_logprob": -0.20788419246673584,
      "compression_ratio": 1.7827585935592651,
      "end": 63.15999984741211,
      "no_speech_prob": 0.0020829332061111927,
      "seek": 5928,
      "start": 61.20000076293945,
      "temperature": 0.0,
      "text": " You can see here, prompt was wrong.",
      "tokens": [
        50460,
        509,
        393,
        536,
        510,
        11,
        12391,
        390,
        2085,
        13,
        50558
      ]
    },
    {
      "id": 26,
      "avg_logprob": -0.20788419246673584,
      "compression_ratio": 1.7827585935592651,
      "end": 67.80000305175781,
      "no_speech_prob": 0.0020829332061111927,
      "seek": 5928,
      "start": 63.15999984741211,
      "temperature": 0.0,
      "text": " The chain was correct, but the AI agent was not correct.",
      "tokens": [
        50558,
        440,
        5021,
        390,
        3006,
        11,
        457,
        264,
        7318,
        9461,
        390,
        406,
        3006,
        13,
        50790
      ]
    },
    {
      "id": 27,
      "avg_logprob": -0.20788419246673584,
      "compression_ratio": 1.7827585935592651,
      "end": 70.36000061035156,
      "no_speech_prob": 0.0020829332061111927,
      "seek": 5928,
      "start": 67.80000305175781,
      "temperature": 0.0,
      "text": " And so you can see here, it looks close, right?",
      "tokens": [
        50790,
        400,
        370,
        291,
        393,
        536,
        510,
        11,
        309,
        1542,
        1998,
        11,
        558,
        30,
        50918
      ]
    },
    {
      "id": 28,
      "avg_logprob": -0.20788419246673584,
      "compression_ratio": 1.7827585935592651,
      "end": 72.19999694824219,
      "no_speech_prob": 0.0020829332061111927,
      "seek": 5928,
      "start": 70.36000061035156,
      "temperature": 0.0,
      "text": " If we pull this text out, right?",
      "tokens": [
        50918,
        759,
        321,
        2235,
        341,
        2487,
        484,
        11,
        558,
        30,
        51010
      ]
    },
    {
      "id": 29,
      "avg_logprob": -0.20788419246673584,
      "compression_ratio": 1.7827585935592651,
      "end": 73.4800033569336,
      "no_speech_prob": 0.0020829332061111927,
      "seek": 5928,
      "start": 72.19999694824219,
      "temperature": 0.0,
      "text": " And we can just highlight this to see.",
      "tokens": [
        51010,
        400,
        321,
        393,
        445,
        5078,
        341,
        281,
        536,
        13,
        51074
      ]
    },
    {
      "id": 30,
      "avg_logprob": -0.20788419246673584,
      "compression_ratio": 1.7827585935592651,
      "end": 76.23999786376953,
      "no_speech_prob": 0.0020829332061111927,
      "seek": 5928,
      "start": 73.4800033569336,
      "temperature": 0.0,
      "text": " So let's go ahead and open up everyone's favorite",
      "tokens": [
        51074,
        407,
        718,
        311,
        352,
        2286,
        293,
        1269,
        493,
        1518,
        311,
        2954,
        51212
      ]
    },
    {
      "id": 31,
      "avg_logprob": -0.20788419246673584,
      "compression_ratio": 1.7827585935592651,
      "end": 77.08000183105469,
      "no_speech_prob": 0.0020829332061111927,
      "seek": 5928,
      "start": 76.23999786376953,
      "temperature": 0.0,
      "text": " local model tool.",
      "tokens": [
        51212,
        2654,
        2316,
        2290,
        13,
        51254
      ]
    },
    {
      "id": 32,
      "avg_logprob": -0.20788419246673584,
      "compression_ratio": 1.7827585935592651,
      "end": 81.13999938964844,
      "no_speech_prob": 0.0020829332061111927,
      "seek": 5928,
      "start": 77.08000183105469,
      "temperature": 0.0,
      "text": " We're going to be using Olama and here's where it branched.",
      "tokens": [
        51254,
        492,
        434,
        516,
        281,
        312,
        1228,
        6141,
        2404,
        293,
        510,
        311,
        689,
        309,
        9819,
        292,
        13,
        51457
      ]
    },
    {
      "id": 33,
      "avg_logprob": -0.20788419246673584,
      "compression_ratio": 1.7827585935592651,
      "end": 82.44000244140625,
      "no_speech_prob": 0.0020829332061111927,
      "seek": 5928,
      "start": 81.13999938964844,
      "temperature": 0.0,
      "text": " And we're going to be using a tool",
      "tokens": [
        51457,
        400,
        321,
        434,
        516,
        281,
        312,
        1228,
        257,
        2290,
        51522
      ]
    },
    {
      "id": 34,
      "avg_logprob": -0.20788419246673584,
      "compression_ratio": 1.7827585935592651,
      "end": 84.08000183105469,
      "no_speech_prob": 0.0020829332061111927,
      "seek": 5928,
      "start": 82.44000244140625,
      "temperature": 0.0,
      "text": " I'm building out called Benchy.",
      "tokens": [
        51522,
        286,
        478,
        2390,
        484,
        1219,
        3964,
        28629,
        13,
        51604
      ]
    },
    {
      "id": 35,
      "avg_logprob": -0.20788419246673584,
      "compression_ratio": 1.7827585935592651,
      "end": 85.19999694824219,
      "no_speech_prob": 0.0020829332061111927,
      "seek": 5928,
      "start": 84.08000183105469,
      "temperature": 0.0,
      "text": " And what we wanted was,",
      "tokens": [
        51604,
        400,
        437,
        321,
        1415,
        390,
        11,
        51660
      ]
    },
    {
      "id": 36,
      "avg_logprob": -0.20788419246673584,
      "compression_ratio": 1.7827585935592651,
      "end": 88.27999877929688,
      "no_speech_prob": 0.0020829332061111927,
      "seek": 5928,
      "start": 85.19999694824219,
      "temperature": 0.0,
      "text": " and a tool I'm building out called Benchy, okay?",
      "tokens": [
        51660,
        293,
        257,
        2290,
        286,
        478,
        2390,
        484,
        1219,
        3964,
        28629,
        11,
        1392,
        30,
        51814
      ]
    },
    {
      "id": 37,
      "avg_logprob": -0.20649729669094086,
      "compression_ratio": 1.791946291923523,
      "end": 92.08000183105469,
      "no_speech_prob": 0.00013135100016370416,
      "seek": 8828,
      "start": 88.80000305175781,
      "temperature": 0.0,
      "text": " So this is a better failure in the AI agent.",
      "tokens": [
        50390,
        407,
        341,
        307,
        257,
        1101,
        7763,
        294,
        264,
        7318,
        9461,
        13,
        50554
      ]
    },
    {
      "id": 38,
      "avg_logprob": -0.20649729669094086,
      "compression_ratio": 1.791946291923523,
      "end": 93.4800033569336,
      "no_speech_prob": 0.00013135100016370416,
      "seek": 8828,
      "start": 92.08000183105469,
      "temperature": 0.0,
      "text": " You know, it made a clean edit,",
      "tokens": [
        50554,
        509,
        458,
        11,
        309,
        1027,
        257,
        2541,
        8129,
        11,
        50624
      ]
    },
    {
      "id": 39,
      "avg_logprob": -0.20649729669094086,
      "compression_ratio": 1.791946291923523,
      "end": 95.44000244140625,
      "no_speech_prob": 0.00013135100016370416,
      "seek": 8828,
      "start": 93.4800033569336,
      "temperature": 0.0,
      "text": " but it wasn't the exact edit we're looking for.",
      "tokens": [
        50624,
        457,
        309,
        2067,
        380,
        264,
        1900,
        8129,
        321,
        434,
        1237,
        337,
        13,
        50722
      ]
    },
    {
      "id": 40,
      "avg_logprob": -0.20649729669094086,
      "compression_ratio": 1.791946291923523,
      "end": 98.72000122070312,
      "no_speech_prob": 0.00013135100016370416,
      "seek": 8828,
      "start": 95.44000244140625,
      "temperature": 0.0,
      "text": " And this is where making the prompt more specific",
      "tokens": [
        50722,
        400,
        341,
        307,
        689,
        1455,
        264,
        12391,
        544,
        2685,
        50886
      ]
    },
    {
      "id": 41,
      "avg_logprob": -0.20649729669094086,
      "compression_ratio": 1.791946291923523,
      "end": 102.94000244140625,
      "no_speech_prob": 0.00013135100016370416,
      "seek": 8828,
      "start": 98.72000122070312,
      "temperature": 0.0,
      "text": " to the exact editing decisions and subjective editing tastes",
      "tokens": [
        50886,
        281,
        264,
        1900,
        10000,
        5327,
        293,
        25972,
        10000,
        8666,
        51097
      ]
    },
    {
      "id": 42,
      "avg_logprob": -0.20649729669094086,
      "compression_ratio": 1.791946291923523,
      "end": 104.76000213623047,
      "no_speech_prob": 0.00013135100016370416,
      "seek": 8828,
      "start": 102.94000244140625,
      "temperature": 0.0,
      "text": " is going to be more and more important, okay?",
      "tokens": [
        51097,
        307,
        516,
        281,
        312,
        544,
        293,
        544,
        1021,
        11,
        1392,
        30,
        51188
      ]
    },
    {
      "id": 43,
      "avg_logprob": -0.20649729669094086,
      "compression_ratio": 1.791946291923523,
      "end": 106.36000061035156,
      "no_speech_prob": 0.00013135100016370416,
      "seek": 8828,
      "start": 104.76000213623047,
      "temperature": 0.0,
      "text": " There are many of these in here",
      "tokens": [
        51188,
        821,
        366,
        867,
        295,
        613,
        294,
        510,
        51268
      ]
    },
    {
      "id": 44,
      "avg_logprob": -0.20649729669094086,
      "compression_ratio": 1.791946291923523,
      "end": 108.4800033569336,
      "no_speech_prob": 0.00013135100016370416,
      "seek": 8828,
      "start": 106.36000061035156,
      "temperature": 0.0,
      "text": " that could have been marked right,",
      "tokens": [
        51268,
        300,
        727,
        362,
        668,
        12658,
        558,
        11,
        51374
      ]
    },
    {
      "id": 45,
      "avg_logprob": -0.20649729669094086,
      "compression_ratio": 1.791946291923523,
      "end": 110.4000015258789,
      "no_speech_prob": 0.00013135100016370416,
      "seek": 8828,
      "start": 108.4800033569336,
      "temperature": 0.0,
      "text": " specifically with the O3 mini model",
      "tokens": [
        51374,
        4682,
        365,
        264,
        422,
        18,
        8382,
        2316,
        51470
      ]
    },
    {
      "id": 46,
      "avg_logprob": -0.20649729669094086,
      "compression_ratio": 1.791946291923523,
      "end": 113.12000274658203,
      "no_speech_prob": 0.00013135100016370416,
      "seek": 8828,
      "start": 110.4000015258789,
      "temperature": 0.0,
      "text": " that could be marked true, could be marked correct,",
      "tokens": [
        51470,
        300,
        727,
        312,
        12658,
        2074,
        11,
        727,
        312,
        12658,
        3006,
        11,
        51606
      ]
    },
    {
      "id": 47,
      "avg_logprob": -0.20649729669094086,
      "compression_ratio": 1.791946291923523,
      "end": 116.0,
      "no_speech_prob": 0.00013135100016370416,
      "seek": 8828,
      "start": 113.12000274658203,
      "temperature": 0.0,
      "text": " but it doesn't exactly hit the target text, right?",
      "tokens": [
        51606,
        457,
        309,
        1177,
        380,
        2293,
        2045,
        264,
        3779,
        2487,
        11,
        558,
        30,
        51750
      ]
    },
    {
      "id": 48,
      "avg_logprob": -0.20649729669094086,
      "compression_ratio": 1.791946291923523,
      "end": 118.0,
      "no_speech_prob": 0.00013135100016370416,
      "seek": 8828,
      "start": 116.0,
      "temperature": 0.0,
      "text": " This is where something like 11 scene distance",
      "tokens": [
        51750,
        639,
        307,
        689,
        746,
        411,
        2975,
        4145,
        4560,
        51850
      ]
    },
    {
      "id": 49,
      "avg_logprob": -0.2548559308052063,
      "compression_ratio": 1.4855490922927856,
      "end": 121.5999984741211,
      "no_speech_prob": 0.020963406190276146,
      "seek": 11800,
      "start": 118.72000122070312,
      "temperature": 0.0,
      "text": " or some rough string comparison framework can come in, okay?",
      "tokens": [
        50400,
        420,
        512,
        5903,
        6798,
        9660,
        8388,
        393,
        808,
        294,
        11,
        1392,
        30,
        50544
      ]
    },
    {
      "id": 50,
      "avg_logprob": -0.2548559308052063,
      "compression_ratio": 1.4855490922927856,
      "end": 123.19999694824219,
      "no_speech_prob": 0.020963406190276146,
      "seek": 11800,
      "start": 121.5999984741211,
      "temperature": 0.0,
      "text": " So we're going to skip through these, right?",
      "tokens": [
        50544,
        407,
        321,
        434,
        516,
        281,
        10023,
        807,
        613,
        11,
        558,
        30,
        50624
      ]
    },
    {
      "id": 51,
      "avg_logprob": -0.2548559308052063,
      "compression_ratio": 1.4855490922927856,
      "end": 124.04000091552734,
      "no_speech_prob": 0.020963406190276146,
      "seek": 11800,
      "start": 123.19999694824219,
      "temperature": 0.0,
      "text": " There's a lot going on.",
      "tokens": [
        50624,
        821,
        311,
        257,
        688,
        516,
        322,
        13,
        50666
      ]
    },
    {
      "id": 52,
      "avg_logprob": -0.2548559308052063,
      "compression_ratio": 1.4855490922927856,
      "end": 125.87999725341797,
      "no_speech_prob": 0.020963406190276146,
      "seek": 11800,
      "start": 124.04000091552734,
      "temperature": 0.0,
      "text": " If we just scroll all the way to the bottom,",
      "tokens": [
        50666,
        759,
        321,
        445,
        11369,
        439,
        264,
        636,
        281,
        264,
        2767,
        11,
        50758
      ]
    },
    {
      "id": 53,
      "avg_logprob": -0.2548559308052063,
      "compression_ratio": 1.4855490922927856,
      "end": 129.36000061035156,
      "no_speech_prob": 0.020963406190276146,
      "seek": 11800,
      "start": 125.87999725341797,
      "temperature": 0.0,
      "text": " we can get some high level overarching benchmark results,",
      "tokens": [
        50758,
        321,
        393,
        483,
        512,
        1090,
        1496,
        45501,
        18927,
        3542,
        11,
        50932
      ]
    },
    {
      "id": 54,
      "avg_logprob": -0.2548559308052063,
      "compression_ratio": 1.4855490922927856,
      "end": 130.1999969482422,
      "no_speech_prob": 0.020963406190276146,
      "seek": 11800,
      "start": 129.36000061035156,
      "temperature": 0.0,
      "text": " okay?",
      "tokens": [
        50932,
        1392,
        30,
        50974
      ]
    },
    {
      "id": 55,
      "avg_logprob": -0.2548559308052063,
      "compression_ratio": 1.4855490922927856,
      "end": 131.0399932861328,
      "no_speech_prob": 0.020963406190276146,
      "seek": 11800,
      "start": 130.1999969482422,
      "temperature": 0.0,
      "text": " Here's everything.",
      "tokens": [
        50974,
        1692,
        311,
        1203,
        13,
        51016
      ]
    }
  ],
  "language": "english",
  "duration": 131.05999755859375,
  "timestamp": 1742835466.137728
}