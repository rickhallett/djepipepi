{
  "audio_path": "data/chunks/age_of_agents_how_to_evolve_your_o3_mini_prompts_20250324_151153_chunk_015.mp3",
  "text": "some problems when you're really in the mindset of value creation, you find problems first and then you apply tools to them from your tool belt. You don't find tools first and then look for problems to solve. It's just the wrong way. It's the backward way to do things. I was really happy to see that Anthropic explicitly mentions this in their documentation. It's about building the right systems for your needs. Start with prompts, then optimize them with evals and then add multi-step agentic systems, AKA prompt chains, workflows, graphs, whatever you want to call it, it doesn't matter. You only do that when the simpler solutions fail. It looks like for my problem here, for the problem space of editing down words in a transcript, it looks like what I'm actually looking for here is for my step four, my AI agents, they actually don't need to be AI agents. They can just be prompt chains. These are concrete steps, predefined steps of executing a prompt and then updating some state and then running another prompt. It looks like for my intelligent editing system, I really only need prompt chains. And that means that inside of this, for my intelligence, I really only need to let my assistant run over and over on a series of steps. Now, of course, there are tons of caveats to that. I don't really have enough data, right? This is not significant data to make a complete decision to completely rule out AI agents. Obviously, that's not enough, but I think it's just really important to stress that point. If a prompt chain can do the job for you, use a prompt chain. I think to push a little bit further, you can experiment with an AI agent and see what that might look like to give your AI agent full autonomy over the system. But I think that what we're going to see moving forward is that a lot of the AI agents that are going to be built underneath the hood, they're not actually going to be AI agents. They're going to be agentic workflows. And more specifically, they're going to be just prompt chains. So just to recap, there are three levels to this. You have just the raw prompt, you have what are called workflows or prompt chains or graphs, and then you have much more autonomous.",
  "segments": [
    {
      "id": 0,
      "avg_logprob": -0.2244783490896225,
      "compression_ratio": 1.7406015396118164,
      "end": 1.7599999904632568,
      "no_speech_prob": 0.003824355313554406,
      "seek": 0,
      "start": 0.0,
      "temperature": 0.0,
      "text": " some problems when you're really in the mindset",
      "tokens": [
        50364,
        512,
        2740,
        562,
        291,
        434,
        534,
        294,
        264,
        12543,
        50452
      ]
    },
    {
      "id": 1,
      "avg_logprob": -0.2244783490896225,
      "compression_ratio": 1.7406015396118164,
      "end": 4.320000171661377,
      "no_speech_prob": 0.003824355313554406,
      "seek": 0,
      "start": 1.7599999904632568,
      "temperature": 0.0,
      "text": " of value creation, you find problems first",
      "tokens": [
        50452,
        295,
        2158,
        8016,
        11,
        291,
        915,
        2740,
        700,
        50580
      ]
    },
    {
      "id": 2,
      "avg_logprob": -0.2244783490896225,
      "compression_ratio": 1.7406015396118164,
      "end": 7.519999980926514,
      "no_speech_prob": 0.003824355313554406,
      "seek": 0,
      "start": 4.320000171661377,
      "temperature": 0.0,
      "text": " and then you apply tools to them from your tool belt.",
      "tokens": [
        50580,
        293,
        550,
        291,
        3079,
        3873,
        281,
        552,
        490,
        428,
        2290,
        10750,
        13,
        50740
      ]
    },
    {
      "id": 3,
      "avg_logprob": -0.2244783490896225,
      "compression_ratio": 1.7406015396118164,
      "end": 9.279999732971191,
      "no_speech_prob": 0.003824355313554406,
      "seek": 0,
      "start": 7.519999980926514,
      "temperature": 0.0,
      "text": " You don't find tools first",
      "tokens": [
        50740,
        509,
        500,
        380,
        915,
        3873,
        700,
        50828
      ]
    },
    {
      "id": 4,
      "avg_logprob": -0.2244783490896225,
      "compression_ratio": 1.7406015396118164,
      "end": 11.199999809265137,
      "no_speech_prob": 0.003824355313554406,
      "seek": 0,
      "start": 9.279999732971191,
      "temperature": 0.0,
      "text": " and then look for problems to solve.",
      "tokens": [
        50828,
        293,
        550,
        574,
        337,
        2740,
        281,
        5039,
        13,
        50924
      ]
    },
    {
      "id": 5,
      "avg_logprob": -0.2244783490896225,
      "compression_ratio": 1.7406015396118164,
      "end": 12.119999885559082,
      "no_speech_prob": 0.003824355313554406,
      "seek": 0,
      "start": 11.199999809265137,
      "temperature": 0.0,
      "text": " It's just the wrong way.",
      "tokens": [
        50924,
        467,
        311,
        445,
        264,
        2085,
        636,
        13,
        50970
      ]
    },
    {
      "id": 6,
      "avg_logprob": -0.2244783490896225,
      "compression_ratio": 1.7406015396118164,
      "end": 13.640000343322754,
      "no_speech_prob": 0.003824355313554406,
      "seek": 0,
      "start": 12.119999885559082,
      "temperature": 0.0,
      "text": " It's the backward way to do things.",
      "tokens": [
        50970,
        467,
        311,
        264,
        23897,
        636,
        281,
        360,
        721,
        13,
        51046
      ]
    },
    {
      "id": 7,
      "avg_logprob": -0.2244783490896225,
      "compression_ratio": 1.7406015396118164,
      "end": 16.079999923706055,
      "no_speech_prob": 0.003824355313554406,
      "seek": 0,
      "start": 13.640000343322754,
      "temperature": 0.0,
      "text": " I was really happy to see that Anthropic",
      "tokens": [
        51046,
        286,
        390,
        534,
        2055,
        281,
        536,
        300,
        12727,
        39173,
        51168
      ]
    },
    {
      "id": 8,
      "avg_logprob": -0.2244783490896225,
      "compression_ratio": 1.7406015396118164,
      "end": 19.260000228881836,
      "no_speech_prob": 0.003824355313554406,
      "seek": 0,
      "start": 16.079999923706055,
      "temperature": 0.0,
      "text": " explicitly mentions this in their documentation.",
      "tokens": [
        51168,
        20803,
        23844,
        341,
        294,
        641,
        14333,
        13,
        51327
      ]
    },
    {
      "id": 9,
      "avg_logprob": -0.2244783490896225,
      "compression_ratio": 1.7406015396118164,
      "end": 22.479999542236328,
      "no_speech_prob": 0.003824355313554406,
      "seek": 0,
      "start": 19.260000228881836,
      "temperature": 0.0,
      "text": " It's about building the right systems for your needs.",
      "tokens": [
        51327,
        467,
        311,
        466,
        2390,
        264,
        558,
        3652,
        337,
        428,
        2203,
        13,
        51488
      ]
    },
    {
      "id": 10,
      "avg_logprob": -0.2244783490896225,
      "compression_ratio": 1.7406015396118164,
      "end": 26.18000030517578,
      "no_speech_prob": 0.003824355313554406,
      "seek": 0,
      "start": 22.479999542236328,
      "temperature": 0.0,
      "text": " Start with prompts, then optimize them with evals",
      "tokens": [
        51488,
        6481,
        365,
        41095,
        11,
        550,
        19719,
        552,
        365,
        1073,
        1124,
        51673
      ]
    },
    {
      "id": 11,
      "avg_logprob": -0.23027345538139343,
      "compression_ratio": 1.615384578704834,
      "end": 28.979999542236328,
      "no_speech_prob": 0.01150780450552702,
      "seek": 2618,
      "start": 26.18000030517578,
      "temperature": 0.0,
      "text": " and then add multi-step agentic systems,",
      "tokens": [
        50364,
        293,
        550,
        909,
        4825,
        12,
        16792,
        9461,
        299,
        3652,
        11,
        50504
      ]
    },
    {
      "id": 12,
      "avg_logprob": -0.23027345538139343,
      "compression_ratio": 1.615384578704834,
      "end": 33.099998474121094,
      "no_speech_prob": 0.01150780450552702,
      "seek": 2618,
      "start": 28.979999542236328,
      "temperature": 0.0,
      "text": " AKA prompt chains, workflows, graphs,",
      "tokens": [
        50504,
        45933,
        12391,
        12626,
        11,
        43461,
        11,
        24877,
        11,
        50710
      ]
    },
    {
      "id": 13,
      "avg_logprob": -0.23027345538139343,
      "compression_ratio": 1.615384578704834,
      "end": 34.97999954223633,
      "no_speech_prob": 0.01150780450552702,
      "seek": 2618,
      "start": 33.099998474121094,
      "temperature": 0.0,
      "text": " whatever you want to call it, it doesn't matter.",
      "tokens": [
        50710,
        2035,
        291,
        528,
        281,
        818,
        309,
        11,
        309,
        1177,
        380,
        1871,
        13,
        50804
      ]
    },
    {
      "id": 14,
      "avg_logprob": -0.23027345538139343,
      "compression_ratio": 1.615384578704834,
      "end": 37.70000076293945,
      "no_speech_prob": 0.01150780450552702,
      "seek": 2618,
      "start": 34.97999954223633,
      "temperature": 0.0,
      "text": " You only do that when the simpler solutions fail.",
      "tokens": [
        50804,
        509,
        787,
        360,
        300,
        562,
        264,
        18587,
        6547,
        3061,
        13,
        50940
      ]
    },
    {
      "id": 15,
      "avg_logprob": -0.23027345538139343,
      "compression_ratio": 1.615384578704834,
      "end": 41.18000030517578,
      "no_speech_prob": 0.01150780450552702,
      "seek": 2618,
      "start": 37.70000076293945,
      "temperature": 0.0,
      "text": " It looks like for my problem here,",
      "tokens": [
        50940,
        467,
        1542,
        411,
        337,
        452,
        1154,
        510,
        11,
        51114
      ]
    },
    {
      "id": 16,
      "avg_logprob": -0.23027345538139343,
      "compression_ratio": 1.615384578704834,
      "end": 46.18000030517578,
      "no_speech_prob": 0.01150780450552702,
      "seek": 2618,
      "start": 41.18000030517578,
      "temperature": 0.0,
      "text": " for the problem space of editing down words in a transcript,",
      "tokens": [
        51114,
        337,
        264,
        1154,
        1901,
        295,
        10000,
        760,
        2283,
        294,
        257,
        24444,
        11,
        51364
      ]
    },
    {
      "id": 17,
      "avg_logprob": -0.23027345538139343,
      "compression_ratio": 1.615384578704834,
      "end": 49.619998931884766,
      "no_speech_prob": 0.01150780450552702,
      "seek": 2618,
      "start": 46.939998626708984,
      "temperature": 0.0,
      "text": " it looks like what I'm actually looking for here",
      "tokens": [
        51402,
        309,
        1542,
        411,
        437,
        286,
        478,
        767,
        1237,
        337,
        510,
        51536
      ]
    },
    {
      "id": 18,
      "avg_logprob": -0.23027345538139343,
      "compression_ratio": 1.615384578704834,
      "end": 52.779998779296875,
      "no_speech_prob": 0.01150780450552702,
      "seek": 2618,
      "start": 49.619998931884766,
      "temperature": 0.0,
      "text": " is for my step four, my AI agents,",
      "tokens": [
        51536,
        307,
        337,
        452,
        1823,
        1451,
        11,
        452,
        7318,
        12554,
        11,
        51694
      ]
    },
    {
      "id": 19,
      "avg_logprob": -0.23027345538139343,
      "compression_ratio": 1.615384578704834,
      "end": 54.5,
      "no_speech_prob": 0.01150780450552702,
      "seek": 2618,
      "start": 52.779998779296875,
      "temperature": 0.0,
      "text": " they actually don't need to be AI agents.",
      "tokens": [
        51694,
        436,
        767,
        500,
        380,
        643,
        281,
        312,
        7318,
        12554,
        13,
        51780
      ]
    },
    {
      "id": 20,
      "avg_logprob": -0.19119501113891602,
      "compression_ratio": 1.7833333015441895,
      "end": 56.81999969482422,
      "no_speech_prob": 0.0002694784780032933,
      "seek": 5450,
      "start": 54.5,
      "temperature": 0.0,
      "text": " They can just be prompt chains.",
      "tokens": [
        50364,
        814,
        393,
        445,
        312,
        12391,
        12626,
        13,
        50480
      ]
    },
    {
      "id": 21,
      "avg_logprob": -0.19119501113891602,
      "compression_ratio": 1.7833333015441895,
      "end": 60.02000045776367,
      "no_speech_prob": 0.0002694784780032933,
      "seek": 5450,
      "start": 56.81999969482422,
      "temperature": 0.0,
      "text": " These are concrete steps, predefined steps",
      "tokens": [
        50480,
        1981,
        366,
        9859,
        4439,
        11,
        659,
        37716,
        4439,
        50640
      ]
    },
    {
      "id": 22,
      "avg_logprob": -0.19119501113891602,
      "compression_ratio": 1.7833333015441895,
      "end": 63.18000030517578,
      "no_speech_prob": 0.0002694784780032933,
      "seek": 5450,
      "start": 60.02000045776367,
      "temperature": 0.0,
      "text": " of executing a prompt and then updating some state",
      "tokens": [
        50640,
        295,
        32368,
        257,
        12391,
        293,
        550,
        25113,
        512,
        1785,
        50798
      ]
    },
    {
      "id": 23,
      "avg_logprob": -0.19119501113891602,
      "compression_ratio": 1.7833333015441895,
      "end": 64.9000015258789,
      "no_speech_prob": 0.0002694784780032933,
      "seek": 5450,
      "start": 63.18000030517578,
      "temperature": 0.0,
      "text": " and then running another prompt.",
      "tokens": [
        50798,
        293,
        550,
        2614,
        1071,
        12391,
        13,
        50884
      ]
    },
    {
      "id": 24,
      "avg_logprob": -0.19119501113891602,
      "compression_ratio": 1.7833333015441895,
      "end": 68.0199966430664,
      "no_speech_prob": 0.0002694784780032933,
      "seek": 5450,
      "start": 64.9000015258789,
      "temperature": 0.0,
      "text": " It looks like for my intelligent editing system,",
      "tokens": [
        50884,
        467,
        1542,
        411,
        337,
        452,
        13232,
        10000,
        1185,
        11,
        51040
      ]
    },
    {
      "id": 25,
      "avg_logprob": -0.19119501113891602,
      "compression_ratio": 1.7833333015441895,
      "end": 71.0199966430664,
      "no_speech_prob": 0.0002694784780032933,
      "seek": 5450,
      "start": 68.0199966430664,
      "temperature": 0.0,
      "text": " I really only need prompt chains.",
      "tokens": [
        51040,
        286,
        534,
        787,
        643,
        12391,
        12626,
        13,
        51190
      ]
    },
    {
      "id": 26,
      "avg_logprob": -0.19119501113891602,
      "compression_ratio": 1.7833333015441895,
      "end": 74.54000091552734,
      "no_speech_prob": 0.0002694784780032933,
      "seek": 5450,
      "start": 71.0199966430664,
      "temperature": 0.0,
      "text": " And that means that inside of this, for my intelligence,",
      "tokens": [
        51190,
        400,
        300,
        1355,
        300,
        1854,
        295,
        341,
        11,
        337,
        452,
        7599,
        11,
        51366
      ]
    },
    {
      "id": 27,
      "avg_logprob": -0.19119501113891602,
      "compression_ratio": 1.7833333015441895,
      "end": 79.05999755859375,
      "no_speech_prob": 0.0002694784780032933,
      "seek": 5450,
      "start": 74.54000091552734,
      "temperature": 0.0,
      "text": " I really only need to let my assistant run over and over",
      "tokens": [
        51366,
        286,
        534,
        787,
        643,
        281,
        718,
        452,
        10994,
        1190,
        670,
        293,
        670,
        51592
      ]
    },
    {
      "id": 28,
      "avg_logprob": -0.19119501113891602,
      "compression_ratio": 1.7833333015441895,
      "end": 80.45999908447266,
      "no_speech_prob": 0.0002694784780032933,
      "seek": 5450,
      "start": 79.05999755859375,
      "temperature": 0.0,
      "text": " on a series of steps.",
      "tokens": [
        51592,
        322,
        257,
        2638,
        295,
        4439,
        13,
        51662
      ]
    },
    {
      "id": 29,
      "avg_logprob": -0.19119501113891602,
      "compression_ratio": 1.7833333015441895,
      "end": 82.86000061035156,
      "no_speech_prob": 0.0002694784780032933,
      "seek": 5450,
      "start": 80.45999908447266,
      "temperature": 0.0,
      "text": " Now, of course, there are tons of caveats to that.",
      "tokens": [
        51662,
        823,
        11,
        295,
        1164,
        11,
        456,
        366,
        9131,
        295,
        11730,
        1720,
        281,
        300,
        13,
        51782
      ]
    },
    {
      "id": 30,
      "avg_logprob": -0.1687486320734024,
      "compression_ratio": 1.7006802558898926,
      "end": 84.69999694824219,
      "no_speech_prob": 0.0009849963244050741,
      "seek": 8286,
      "start": 82.86000061035156,
      "temperature": 0.0,
      "text": " I don't really have enough data, right?",
      "tokens": [
        50364,
        286,
        500,
        380,
        534,
        362,
        1547,
        1412,
        11,
        558,
        30,
        50456
      ]
    },
    {
      "id": 31,
      "avg_logprob": -0.1687486320734024,
      "compression_ratio": 1.7006802558898926,
      "end": 88.41999816894531,
      "no_speech_prob": 0.0009849963244050741,
      "seek": 8286,
      "start": 84.69999694824219,
      "temperature": 0.0,
      "text": " This is not significant data to make a complete decision",
      "tokens": [
        50456,
        639,
        307,
        406,
        4776,
        1412,
        281,
        652,
        257,
        3566,
        3537,
        50642
      ]
    },
    {
      "id": 32,
      "avg_logprob": -0.1687486320734024,
      "compression_ratio": 1.7006802558898926,
      "end": 89.86000061035156,
      "no_speech_prob": 0.0009849963244050741,
      "seek": 8286,
      "start": 88.41999816894531,
      "temperature": 0.0,
      "text": " to completely rule out AI agents.",
      "tokens": [
        50642,
        281,
        2584,
        4978,
        484,
        7318,
        12554,
        13,
        50714
      ]
    },
    {
      "id": 33,
      "avg_logprob": -0.1687486320734024,
      "compression_ratio": 1.7006802558898926,
      "end": 91.37999725341797,
      "no_speech_prob": 0.0009849963244050741,
      "seek": 8286,
      "start": 89.86000061035156,
      "temperature": 0.0,
      "text": " Obviously, that's not enough,",
      "tokens": [
        50714,
        7580,
        11,
        300,
        311,
        406,
        1547,
        11,
        50790
      ]
    },
    {
      "id": 34,
      "avg_logprob": -0.1687486320734024,
      "compression_ratio": 1.7006802558898926,
      "end": 94.18000030517578,
      "no_speech_prob": 0.0009849963244050741,
      "seek": 8286,
      "start": 91.37999725341797,
      "temperature": 0.0,
      "text": " but I think it's just really important to stress that point.",
      "tokens": [
        50790,
        457,
        286,
        519,
        309,
        311,
        445,
        534,
        1021,
        281,
        4244,
        300,
        935,
        13,
        50930
      ]
    },
    {
      "id": 35,
      "avg_logprob": -0.1687486320734024,
      "compression_ratio": 1.7006802558898926,
      "end": 96.81999969482422,
      "no_speech_prob": 0.0009849963244050741,
      "seek": 8286,
      "start": 94.18000030517578,
      "temperature": 0.0,
      "text": " If a prompt chain can do the job for you,",
      "tokens": [
        50930,
        759,
        257,
        12391,
        5021,
        393,
        360,
        264,
        1691,
        337,
        291,
        11,
        51062
      ]
    },
    {
      "id": 36,
      "avg_logprob": -0.1687486320734024,
      "compression_ratio": 1.7006802558898926,
      "end": 98.05999755859375,
      "no_speech_prob": 0.0009849963244050741,
      "seek": 8286,
      "start": 96.81999969482422,
      "temperature": 0.0,
      "text": " use a prompt chain.",
      "tokens": [
        51062,
        764,
        257,
        12391,
        5021,
        13,
        51124
      ]
    },
    {
      "id": 37,
      "avg_logprob": -0.1687486320734024,
      "compression_ratio": 1.7006802558898926,
      "end": 100.22000122070312,
      "no_speech_prob": 0.0009849963244050741,
      "seek": 8286,
      "start": 98.05999755859375,
      "temperature": 0.0,
      "text": " I think to push a little bit further,",
      "tokens": [
        51124,
        286,
        519,
        281,
        2944,
        257,
        707,
        857,
        3052,
        11,
        51232
      ]
    },
    {
      "id": 38,
      "avg_logprob": -0.1687486320734024,
      "compression_ratio": 1.7006802558898926,
      "end": 102.37999725341797,
      "no_speech_prob": 0.0009849963244050741,
      "seek": 8286,
      "start": 100.22000122070312,
      "temperature": 0.0,
      "text": " you can experiment with an AI agent",
      "tokens": [
        51232,
        291,
        393,
        5120,
        365,
        364,
        7318,
        9461,
        51340
      ]
    },
    {
      "id": 39,
      "avg_logprob": -0.1687486320734024,
      "compression_ratio": 1.7006802558898926,
      "end": 104.13999938964844,
      "no_speech_prob": 0.0009849963244050741,
      "seek": 8286,
      "start": 102.37999725341797,
      "temperature": 0.0,
      "text": " and see what that might look like",
      "tokens": [
        51340,
        293,
        536,
        437,
        300,
        1062,
        574,
        411,
        51428
      ]
    },
    {
      "id": 40,
      "avg_logprob": -0.1687486320734024,
      "compression_ratio": 1.7006802558898926,
      "end": 107.0199966430664,
      "no_speech_prob": 0.0009849963244050741,
      "seek": 8286,
      "start": 104.13999938964844,
      "temperature": 0.0,
      "text": " to give your AI agent full autonomy over the system.",
      "tokens": [
        51428,
        281,
        976,
        428,
        7318,
        9461,
        1577,
        27278,
        670,
        264,
        1185,
        13,
        51572
      ]
    },
    {
      "id": 41,
      "avg_logprob": -0.1687486320734024,
      "compression_ratio": 1.7006802558898926,
      "end": 109.58000183105469,
      "no_speech_prob": 0.0009849963244050741,
      "seek": 8286,
      "start": 107.0199966430664,
      "temperature": 0.0,
      "text": " But I think that what we're going to see moving forward",
      "tokens": [
        51572,
        583,
        286,
        519,
        300,
        437,
        321,
        434,
        516,
        281,
        536,
        2684,
        2128,
        51700
      ]
    },
    {
      "id": 42,
      "avg_logprob": -0.22231127321720123,
      "compression_ratio": 1.8584474325180054,
      "end": 112.9800033569336,
      "no_speech_prob": 0.26272350549697876,
      "seek": 10958,
      "start": 109.62000274658203,
      "temperature": 0.0,
      "text": " is that a lot of the AI agents that are going to be built",
      "tokens": [
        50366,
        307,
        300,
        257,
        688,
        295,
        264,
        7318,
        12554,
        300,
        366,
        516,
        281,
        312,
        3094,
        50534
      ]
    },
    {
      "id": 43,
      "avg_logprob": -0.22231127321720123,
      "compression_ratio": 1.8584474325180054,
      "end": 113.81999969482422,
      "no_speech_prob": 0.26272350549697876,
      "seek": 10958,
      "start": 112.9800033569336,
      "temperature": 0.0,
      "text": " underneath the hood,",
      "tokens": [
        50534,
        7223,
        264,
        13376,
        11,
        50576
      ]
    },
    {
      "id": 44,
      "avg_logprob": -0.22231127321720123,
      "compression_ratio": 1.8584474325180054,
      "end": 115.73999786376953,
      "no_speech_prob": 0.26272350549697876,
      "seek": 10958,
      "start": 113.81999969482422,
      "temperature": 0.0,
      "text": " they're not actually going to be AI agents.",
      "tokens": [
        50576,
        436,
        434,
        406,
        767,
        516,
        281,
        312,
        7318,
        12554,
        13,
        50672
      ]
    },
    {
      "id": 45,
      "avg_logprob": -0.22231127321720123,
      "compression_ratio": 1.8584474325180054,
      "end": 117.69999694824219,
      "no_speech_prob": 0.26272350549697876,
      "seek": 10958,
      "start": 115.73999786376953,
      "temperature": 0.0,
      "text": " They're going to be agentic workflows.",
      "tokens": [
        50672,
        814,
        434,
        516,
        281,
        312,
        9461,
        299,
        43461,
        13,
        50770
      ]
    },
    {
      "id": 46,
      "avg_logprob": -0.22231127321720123,
      "compression_ratio": 1.8584474325180054,
      "end": 118.81999969482422,
      "no_speech_prob": 0.26272350549697876,
      "seek": 10958,
      "start": 117.69999694824219,
      "temperature": 0.0,
      "text": " And more specifically,",
      "tokens": [
        50770,
        400,
        544,
        4682,
        11,
        50826
      ]
    },
    {
      "id": 47,
      "avg_logprob": -0.22231127321720123,
      "compression_ratio": 1.8584474325180054,
      "end": 121.30000305175781,
      "no_speech_prob": 0.26272350549697876,
      "seek": 10958,
      "start": 118.81999969482422,
      "temperature": 0.0,
      "text": " they're going to be just prompt chains.",
      "tokens": [
        50826,
        436,
        434,
        516,
        281,
        312,
        445,
        12391,
        12626,
        13,
        50950
      ]
    },
    {
      "id": 48,
      "avg_logprob": -0.22231127321720123,
      "compression_ratio": 1.8584474325180054,
      "end": 123.37999725341797,
      "no_speech_prob": 0.26272350549697876,
      "seek": 10958,
      "start": 121.30000305175781,
      "temperature": 0.0,
      "text": " So just to recap, there are three levels to this.",
      "tokens": [
        50950,
        407,
        445,
        281,
        20928,
        11,
        456,
        366,
        1045,
        4358,
        281,
        341,
        13,
        51054
      ]
    },
    {
      "id": 49,
      "avg_logprob": -0.22231127321720123,
      "compression_ratio": 1.8584474325180054,
      "end": 124.77999877929688,
      "no_speech_prob": 0.26272350549697876,
      "seek": 10958,
      "start": 123.37999725341797,
      "temperature": 0.0,
      "text": " You have just the raw prompt,",
      "tokens": [
        51054,
        509,
        362,
        445,
        264,
        8936,
        12391,
        11,
        51124
      ]
    },
    {
      "id": 50,
      "avg_logprob": -0.22231127321720123,
      "compression_ratio": 1.8584474325180054,
      "end": 129.10000610351562,
      "no_speech_prob": 0.26272350549697876,
      "seek": 10958,
      "start": 124.77999877929688,
      "temperature": 0.0,
      "text": " you have what are called workflows or prompt chains or graphs,",
      "tokens": [
        51124,
        291,
        362,
        437,
        366,
        1219,
        43461,
        420,
        12391,
        12626,
        420,
        24877,
        11,
        51340
      ]
    },
    {
      "id": 51,
      "avg_logprob": -0.22231127321720123,
      "compression_ratio": 1.8584474325180054,
      "end": 131.25999450683594,
      "no_speech_prob": 0.26272350549697876,
      "seek": 10958,
      "start": 129.10000610351562,
      "temperature": 0.0,
      "text": " and then you have much more autonomous.",
      "tokens": [
        51340,
        293,
        550,
        291,
        362,
        709,
        544,
        23797,
        13,
        51448
      ]
    }
  ],
  "language": "english",
  "duration": 131.05999755859375,
  "timestamp": 1742835490.432947
}