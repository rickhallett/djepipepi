{
  "audio_path": "data/chunks/age_of_agents_how_to_evolve_your_o3_mini_prompts_20250324_151153_chunk_003.mp3",
  "text": "and then word level breakdowns, just as you saw before. And this is a real transcript from the DeepSeek personal AI assistant video. There's just too many tokens. Even if you hand this off to Gemini, it's not going to be able to solve this problem in any cohesive way at scale. How can we handle this? We can create slices. So a slice is a chunk of the transcript containing a few sentences each. From the transcription above, we could create two slices. In these two slices, there are a couple of key things that need to be edited out. Next, we're going to allocate our AI agents, right? So in our case, this is going to be our prompt, prompt chain, or our full-on AI agent, as we saw here, right? And we're going to walk through this in just a moment. I'm really excited to share with you how the prompt versus the prompt chain versus the AI agent performs for this use case. The results are not exactly as you would expect. This is the really cool part about creating slices, and it's a great part about breaking your large problem into smaller, more manageable chunks, right? If ever you're feeling overwhelmed with a specific problem, it's probably just too big, and you haven't broken it down into small enough chunks. Engineering 101. Slices give us this incredible ability to, for each slice, we'll create and allocate an AI agent and basically hand off this problem to compute at scale. So you can imagine we'll have hundreds of slices getting edited all at the same time in parallel. Now, step four is where all the magic happens. Slice one, you have this before text, and then you end up with this after text, right? So this is happening in parallel. Slice one and slice two and slice N are getting edited all at the same time with compute. And then finally, we combine the edits into the sequence of timeline edits. Every video editor has a different format. I use Final Cut Pro. So I'm gonna be converting these into FCP XML files. We're not gonna get into this too much, but there's what the format looks like. We're gonna go from prompt to prompt chain to AI agent, and we're gonna see across these three levels of abstraction, which one performs the best. Let's go ahead and run a prompt and really understand what this problem looks like. On run a cut agent, and then we can kick this off. Let's let this prompt run. So this is just running a single prompt. You can see it got the problem wrong. Let's go ahead and figure out why.",
  "segments": [
    {
      "id": 0,
      "avg_logprob": -0.22358085215091705,
      "compression_ratio": 1.7015873193740845,
      "end": 3.440000057220459,
      "no_speech_prob": 0.05339203402400017,
      "seek": 0,
      "start": 0.0,
      "temperature": 0.0,
      "text": " and then word level breakdowns, just as you saw before.",
      "tokens": [
        50364,
        293,
        550,
        1349,
        1496,
        18188,
        82,
        11,
        445,
        382,
        291,
        1866,
        949,
        13,
        50536
      ]
    },
    {
      "id": 1,
      "avg_logprob": -0.22358085215091705,
      "compression_ratio": 1.7015873193740845,
      "end": 5.199999809265137,
      "no_speech_prob": 0.05339203402400017,
      "seek": 0,
      "start": 3.440000057220459,
      "temperature": 0.0,
      "text": " And this is a real transcript",
      "tokens": [
        50536,
        400,
        341,
        307,
        257,
        957,
        24444,
        50624
      ]
    },
    {
      "id": 2,
      "avg_logprob": -0.22358085215091705,
      "compression_ratio": 1.7015873193740845,
      "end": 7.960000038146973,
      "no_speech_prob": 0.05339203402400017,
      "seek": 0,
      "start": 5.199999809265137,
      "temperature": 0.0,
      "text": " from the DeepSeek personal AI assistant video.",
      "tokens": [
        50624,
        490,
        264,
        14895,
        10637,
        916,
        2973,
        7318,
        10994,
        960,
        13,
        50762
      ]
    },
    {
      "id": 3,
      "avg_logprob": -0.22358085215091705,
      "compression_ratio": 1.7015873193740845,
      "end": 9.239999771118164,
      "no_speech_prob": 0.05339203402400017,
      "seek": 0,
      "start": 7.960000038146973,
      "temperature": 0.0,
      "text": " There's just too many tokens.",
      "tokens": [
        50762,
        821,
        311,
        445,
        886,
        867,
        22667,
        13,
        50826
      ]
    },
    {
      "id": 4,
      "avg_logprob": -0.22358085215091705,
      "compression_ratio": 1.7015873193740845,
      "end": 11.0,
      "no_speech_prob": 0.05339203402400017,
      "seek": 0,
      "start": 9.239999771118164,
      "temperature": 0.0,
      "text": " Even if you hand this off to Gemini,",
      "tokens": [
        50826,
        2754,
        498,
        291,
        1011,
        341,
        766,
        281,
        22894,
        3812,
        11,
        50914
      ]
    },
    {
      "id": 5,
      "avg_logprob": -0.22358085215091705,
      "compression_ratio": 1.7015873193740845,
      "end": 13.0,
      "no_speech_prob": 0.05339203402400017,
      "seek": 0,
      "start": 11.0,
      "temperature": 0.0,
      "text": " it's not going to be able to solve this problem",
      "tokens": [
        50914,
        309,
        311,
        406,
        516,
        281,
        312,
        1075,
        281,
        5039,
        341,
        1154,
        51014
      ]
    },
    {
      "id": 6,
      "avg_logprob": -0.22358085215091705,
      "compression_ratio": 1.7015873193740845,
      "end": 15.15999984741211,
      "no_speech_prob": 0.05339203402400017,
      "seek": 0,
      "start": 13.0,
      "temperature": 0.0,
      "text": " in any cohesive way at scale.",
      "tokens": [
        51014,
        294,
        604,
        43025,
        636,
        412,
        4373,
        13,
        51122
      ]
    },
    {
      "id": 7,
      "avg_logprob": -0.22358085215091705,
      "compression_ratio": 1.7015873193740845,
      "end": 16.0,
      "no_speech_prob": 0.05339203402400017,
      "seek": 0,
      "start": 15.15999984741211,
      "temperature": 0.0,
      "text": " How can we handle this?",
      "tokens": [
        51122,
        1012,
        393,
        321,
        4813,
        341,
        30,
        51164
      ]
    },
    {
      "id": 8,
      "avg_logprob": -0.22358085215091705,
      "compression_ratio": 1.7015873193740845,
      "end": 17.440000534057617,
      "no_speech_prob": 0.05339203402400017,
      "seek": 0,
      "start": 16.0,
      "temperature": 0.0,
      "text": " We can create slices.",
      "tokens": [
        51164,
        492,
        393,
        1884,
        19793,
        13,
        51236
      ]
    },
    {
      "id": 9,
      "avg_logprob": -0.22358085215091705,
      "compression_ratio": 1.7015873193740845,
      "end": 20.040000915527344,
      "no_speech_prob": 0.05339203402400017,
      "seek": 0,
      "start": 17.440000534057617,
      "temperature": 0.0,
      "text": " So a slice is a chunk of the transcript",
      "tokens": [
        51236,
        407,
        257,
        13153,
        307,
        257,
        16635,
        295,
        264,
        24444,
        51366
      ]
    },
    {
      "id": 10,
      "avg_logprob": -0.22358085215091705,
      "compression_ratio": 1.7015873193740845,
      "end": 22.079999923706055,
      "no_speech_prob": 0.05339203402400017,
      "seek": 0,
      "start": 20.040000915527344,
      "temperature": 0.0,
      "text": " containing a few sentences each.",
      "tokens": [
        51366,
        19273,
        257,
        1326,
        16579,
        1184,
        13,
        51468
      ]
    },
    {
      "id": 11,
      "avg_logprob": -0.22358085215091705,
      "compression_ratio": 1.7015873193740845,
      "end": 25.040000915527344,
      "no_speech_prob": 0.05339203402400017,
      "seek": 0,
      "start": 22.079999923706055,
      "temperature": 0.0,
      "text": " From the transcription above, we could create two slices.",
      "tokens": [
        51468,
        3358,
        264,
        35288,
        3673,
        11,
        321,
        727,
        1884,
        732,
        19793,
        13,
        51616
      ]
    },
    {
      "id": 12,
      "avg_logprob": -0.22358085215091705,
      "compression_ratio": 1.7015873193740845,
      "end": 27.34000015258789,
      "no_speech_prob": 0.05339203402400017,
      "seek": 0,
      "start": 25.040000915527344,
      "temperature": 0.0,
      "text": " In these two slices, there are a couple of key things",
      "tokens": [
        51616,
        682,
        613,
        732,
        19793,
        11,
        456,
        366,
        257,
        1916,
        295,
        2141,
        721,
        51731
      ]
    },
    {
      "id": 13,
      "avg_logprob": -0.22358085215091705,
      "compression_ratio": 1.7015873193740845,
      "end": 28.65999984741211,
      "no_speech_prob": 0.05339203402400017,
      "seek": 0,
      "start": 27.34000015258789,
      "temperature": 0.0,
      "text": " that need to be edited out.",
      "tokens": [
        51731,
        300,
        643,
        281,
        312,
        23016,
        484,
        13,
        51797
      ]
    },
    {
      "id": 14,
      "avg_logprob": -0.18582940101623535,
      "compression_ratio": 1.8187310695648193,
      "end": 31.31999969482422,
      "no_speech_prob": 1.6442403648397885e-05,
      "seek": 2866,
      "start": 28.65999984741211,
      "temperature": 0.0,
      "text": " Next, we're going to allocate our AI agents, right?",
      "tokens": [
        50364,
        3087,
        11,
        321,
        434,
        516,
        281,
        35713,
        527,
        7318,
        12554,
        11,
        558,
        30,
        50497
      ]
    },
    {
      "id": 15,
      "avg_logprob": -0.18582940101623535,
      "compression_ratio": 1.8187310695648193,
      "end": 33.15999984741211,
      "no_speech_prob": 1.6442403648397885e-05,
      "seek": 2866,
      "start": 31.31999969482422,
      "temperature": 0.0,
      "text": " So in our case, this is going to be our prompt,",
      "tokens": [
        50497,
        407,
        294,
        527,
        1389,
        11,
        341,
        307,
        516,
        281,
        312,
        527,
        12391,
        11,
        50589
      ]
    },
    {
      "id": 16,
      "avg_logprob": -0.18582940101623535,
      "compression_ratio": 1.8187310695648193,
      "end": 37.040000915527344,
      "no_speech_prob": 1.6442403648397885e-05,
      "seek": 2866,
      "start": 33.15999984741211,
      "temperature": 0.0,
      "text": " prompt chain, or our full-on AI agent, as we saw here, right?",
      "tokens": [
        50589,
        12391,
        5021,
        11,
        420,
        527,
        1577,
        12,
        266,
        7318,
        9461,
        11,
        382,
        321,
        1866,
        510,
        11,
        558,
        30,
        50783
      ]
    },
    {
      "id": 17,
      "avg_logprob": -0.18582940101623535,
      "compression_ratio": 1.8187310695648193,
      "end": 38.84000015258789,
      "no_speech_prob": 1.6442403648397885e-05,
      "seek": 2866,
      "start": 37.040000915527344,
      "temperature": 0.0,
      "text": " And we're going to walk through this in just a moment.",
      "tokens": [
        50783,
        400,
        321,
        434,
        516,
        281,
        1792,
        807,
        341,
        294,
        445,
        257,
        1623,
        13,
        50873
      ]
    },
    {
      "id": 18,
      "avg_logprob": -0.18582940101623535,
      "compression_ratio": 1.8187310695648193,
      "end": 40.15999984741211,
      "no_speech_prob": 1.6442403648397885e-05,
      "seek": 2866,
      "start": 38.84000015258789,
      "temperature": 0.0,
      "text": " I'm really excited to share with you",
      "tokens": [
        50873,
        286,
        478,
        534,
        2919,
        281,
        2073,
        365,
        291,
        50939
      ]
    },
    {
      "id": 19,
      "avg_logprob": -0.18582940101623535,
      "compression_ratio": 1.8187310695648193,
      "end": 42.119998931884766,
      "no_speech_prob": 1.6442403648397885e-05,
      "seek": 2866,
      "start": 40.15999984741211,
      "temperature": 0.0,
      "text": " how the prompt versus the prompt chain",
      "tokens": [
        50939,
        577,
        264,
        12391,
        5717,
        264,
        12391,
        5021,
        51037
      ]
    },
    {
      "id": 20,
      "avg_logprob": -0.18582940101623535,
      "compression_ratio": 1.8187310695648193,
      "end": 45.439998626708984,
      "no_speech_prob": 1.6442403648397885e-05,
      "seek": 2866,
      "start": 42.119998931884766,
      "temperature": 0.0,
      "text": " versus the AI agent performs for this use case.",
      "tokens": [
        51037,
        5717,
        264,
        7318,
        9461,
        26213,
        337,
        341,
        764,
        1389,
        13,
        51203
      ]
    },
    {
      "id": 21,
      "avg_logprob": -0.18582940101623535,
      "compression_ratio": 1.8187310695648193,
      "end": 48.0,
      "no_speech_prob": 1.6442403648397885e-05,
      "seek": 2866,
      "start": 45.439998626708984,
      "temperature": 0.0,
      "text": " The results are not exactly as you would expect.",
      "tokens": [
        51203,
        440,
        3542,
        366,
        406,
        2293,
        382,
        291,
        576,
        2066,
        13,
        51331
      ]
    },
    {
      "id": 22,
      "avg_logprob": -0.18582940101623535,
      "compression_ratio": 1.8187310695648193,
      "end": 50.47999954223633,
      "no_speech_prob": 1.6442403648397885e-05,
      "seek": 2866,
      "start": 48.0,
      "temperature": 0.0,
      "text": " This is the really cool part about creating slices,",
      "tokens": [
        51331,
        639,
        307,
        264,
        534,
        1627,
        644,
        466,
        4084,
        19793,
        11,
        51455
      ]
    },
    {
      "id": 23,
      "avg_logprob": -0.18582940101623535,
      "compression_ratio": 1.8187310695648193,
      "end": 52.84000015258789,
      "no_speech_prob": 1.6442403648397885e-05,
      "seek": 2866,
      "start": 50.47999954223633,
      "temperature": 0.0,
      "text": " and it's a great part about breaking your large problem",
      "tokens": [
        51455,
        293,
        309,
        311,
        257,
        869,
        644,
        466,
        7697,
        428,
        2416,
        1154,
        51573
      ]
    },
    {
      "id": 24,
      "avg_logprob": -0.18582940101623535,
      "compression_ratio": 1.8187310695648193,
      "end": 55.560001373291016,
      "no_speech_prob": 1.6442403648397885e-05,
      "seek": 2866,
      "start": 52.84000015258789,
      "temperature": 0.0,
      "text": " into smaller, more manageable chunks, right?",
      "tokens": [
        51573,
        666,
        4356,
        11,
        544,
        38798,
        24004,
        11,
        558,
        30,
        51709
      ]
    },
    {
      "id": 25,
      "avg_logprob": -0.18582940101623535,
      "compression_ratio": 1.8187310695648193,
      "end": 58.08000183105469,
      "no_speech_prob": 1.6442403648397885e-05,
      "seek": 2866,
      "start": 55.560001373291016,
      "temperature": 0.0,
      "text": " If ever you're feeling overwhelmed with a specific problem,",
      "tokens": [
        51709,
        759,
        1562,
        291,
        434,
        2633,
        19042,
        365,
        257,
        2685,
        1154,
        11,
        51835
      ]
    },
    {
      "id": 26,
      "avg_logprob": -0.17465868592262268,
      "compression_ratio": 1.6806451082229614,
      "end": 59.41999816894531,
      "no_speech_prob": 0.00014883799303788692,
      "seek": 5808,
      "start": 58.099998474121094,
      "temperature": 0.0,
      "text": " it's probably just too big,",
      "tokens": [
        50365,
        309,
        311,
        1391,
        445,
        886,
        955,
        11,
        50431
      ]
    },
    {
      "id": 27,
      "avg_logprob": -0.17465868592262268,
      "compression_ratio": 1.6806451082229614,
      "end": 62.540000915527344,
      "no_speech_prob": 0.00014883799303788692,
      "seek": 5808,
      "start": 59.41999816894531,
      "temperature": 0.0,
      "text": " and you haven't broken it down into small enough chunks.",
      "tokens": [
        50431,
        293,
        291,
        2378,
        380,
        5463,
        309,
        760,
        666,
        1359,
        1547,
        24004,
        13,
        50587
      ]
    },
    {
      "id": 28,
      "avg_logprob": -0.17465868592262268,
      "compression_ratio": 1.6806451082229614,
      "end": 63.459999084472656,
      "no_speech_prob": 0.00014883799303788692,
      "seek": 5808,
      "start": 62.540000915527344,
      "temperature": 0.0,
      "text": " Engineering 101.",
      "tokens": [
        50587,
        16215,
        21055,
        13,
        50633
      ]
    },
    {
      "id": 29,
      "avg_logprob": -0.17465868592262268,
      "compression_ratio": 1.6806451082229614,
      "end": 65.69999694824219,
      "no_speech_prob": 0.00014883799303788692,
      "seek": 5808,
      "start": 63.459999084472656,
      "temperature": 0.0,
      "text": " Slices give us this incredible ability to,",
      "tokens": [
        50633,
        318,
        1050,
        279,
        976,
        505,
        341,
        4651,
        3485,
        281,
        11,
        50745
      ]
    },
    {
      "id": 30,
      "avg_logprob": -0.17465868592262268,
      "compression_ratio": 1.6806451082229614,
      "end": 68.81999969482422,
      "no_speech_prob": 0.00014883799303788692,
      "seek": 5808,
      "start": 65.69999694824219,
      "temperature": 0.0,
      "text": " for each slice, we'll create and allocate an AI agent",
      "tokens": [
        50745,
        337,
        1184,
        13153,
        11,
        321,
        603,
        1884,
        293,
        35713,
        364,
        7318,
        9461,
        50901
      ]
    },
    {
      "id": 31,
      "avg_logprob": -0.17465868592262268,
      "compression_ratio": 1.6806451082229614,
      "end": 72.05999755859375,
      "no_speech_prob": 0.00014883799303788692,
      "seek": 5808,
      "start": 68.81999969482422,
      "temperature": 0.0,
      "text": " and basically hand off this problem to compute at scale.",
      "tokens": [
        50901,
        293,
        1936,
        1011,
        766,
        341,
        1154,
        281,
        14722,
        412,
        4373,
        13,
        51063
      ]
    },
    {
      "id": 32,
      "avg_logprob": -0.17465868592262268,
      "compression_ratio": 1.6806451082229614,
      "end": 74.0199966430664,
      "no_speech_prob": 0.00014883799303788692,
      "seek": 5808,
      "start": 72.05999755859375,
      "temperature": 0.0,
      "text": " So you can imagine we'll have hundreds of slices",
      "tokens": [
        51063,
        407,
        291,
        393,
        3811,
        321,
        603,
        362,
        6779,
        295,
        19793,
        51161
      ]
    },
    {
      "id": 33,
      "avg_logprob": -0.17465868592262268,
      "compression_ratio": 1.6806451082229614,
      "end": 76.9800033569336,
      "no_speech_prob": 0.00014883799303788692,
      "seek": 5808,
      "start": 74.0199966430664,
      "temperature": 0.0,
      "text": " getting edited all at the same time in parallel.",
      "tokens": [
        51161,
        1242,
        23016,
        439,
        412,
        264,
        912,
        565,
        294,
        8952,
        13,
        51309
      ]
    },
    {
      "id": 34,
      "avg_logprob": -0.17465868592262268,
      "compression_ratio": 1.6806451082229614,
      "end": 79.81999969482422,
      "no_speech_prob": 0.00014883799303788692,
      "seek": 5808,
      "start": 76.9800033569336,
      "temperature": 0.0,
      "text": " Now, step four is where all the magic happens.",
      "tokens": [
        51309,
        823,
        11,
        1823,
        1451,
        307,
        689,
        439,
        264,
        5585,
        2314,
        13,
        51451
      ]
    },
    {
      "id": 35,
      "avg_logprob": -0.17465868592262268,
      "compression_ratio": 1.6806451082229614,
      "end": 81.81999969482422,
      "no_speech_prob": 0.00014883799303788692,
      "seek": 5808,
      "start": 79.81999969482422,
      "temperature": 0.0,
      "text": " Slice one, you have this before text,",
      "tokens": [
        51451,
        6187,
        573,
        472,
        11,
        291,
        362,
        341,
        949,
        2487,
        11,
        51551
      ]
    },
    {
      "id": 36,
      "avg_logprob": -0.17465868592262268,
      "compression_ratio": 1.6806451082229614,
      "end": 84.37999725341797,
      "no_speech_prob": 0.00014883799303788692,
      "seek": 5808,
      "start": 81.81999969482422,
      "temperature": 0.0,
      "text": " and then you end up with this after text, right?",
      "tokens": [
        51551,
        293,
        550,
        291,
        917,
        493,
        365,
        341,
        934,
        2487,
        11,
        558,
        30,
        51679
      ]
    },
    {
      "id": 37,
      "avg_logprob": -0.17465868592262268,
      "compression_ratio": 1.6806451082229614,
      "end": 85.77999877929688,
      "no_speech_prob": 0.00014883799303788692,
      "seek": 5808,
      "start": 84.37999725341797,
      "temperature": 0.0,
      "text": " So this is happening in parallel.",
      "tokens": [
        51679,
        407,
        341,
        307,
        2737,
        294,
        8952,
        13,
        51749
      ]
    },
    {
      "id": 38,
      "avg_logprob": -0.20915354788303375,
      "compression_ratio": 1.663082480430603,
      "end": 88.55999755859375,
      "no_speech_prob": 0.004538377746939659,
      "seek": 8578,
      "start": 85.77999877929688,
      "temperature": 0.0,
      "text": " Slice one and slice two and slice N",
      "tokens": [
        50364,
        6187,
        573,
        472,
        293,
        13153,
        732,
        293,
        13153,
        426,
        50503
      ]
    },
    {
      "id": 39,
      "avg_logprob": -0.20915354788303375,
      "compression_ratio": 1.663082480430603,
      "end": 92.08000183105469,
      "no_speech_prob": 0.004538377746939659,
      "seek": 8578,
      "start": 88.55999755859375,
      "temperature": 0.0,
      "text": " are getting edited all at the same time with compute.",
      "tokens": [
        50503,
        366,
        1242,
        23016,
        439,
        412,
        264,
        912,
        565,
        365,
        14722,
        13,
        50679
      ]
    },
    {
      "id": 40,
      "avg_logprob": -0.20915354788303375,
      "compression_ratio": 1.663082480430603,
      "end": 94.27999877929688,
      "no_speech_prob": 0.004538377746939659,
      "seek": 8578,
      "start": 92.08000183105469,
      "temperature": 0.0,
      "text": " And then finally, we combine the edits",
      "tokens": [
        50679,
        400,
        550,
        2721,
        11,
        321,
        10432,
        264,
        41752,
        50789
      ]
    },
    {
      "id": 41,
      "avg_logprob": -0.20915354788303375,
      "compression_ratio": 1.663082480430603,
      "end": 97.4800033569336,
      "no_speech_prob": 0.004538377746939659,
      "seek": 8578,
      "start": 94.27999877929688,
      "temperature": 0.0,
      "text": " into the sequence of timeline edits.",
      "tokens": [
        50789,
        666,
        264,
        8310,
        295,
        12933,
        41752,
        13,
        50949
      ]
    },
    {
      "id": 42,
      "avg_logprob": -0.20915354788303375,
      "compression_ratio": 1.663082480430603,
      "end": 99.16000366210938,
      "no_speech_prob": 0.004538377746939659,
      "seek": 8578,
      "start": 97.4800033569336,
      "temperature": 0.0,
      "text": " Every video editor has a different format.",
      "tokens": [
        50949,
        2048,
        960,
        9839,
        575,
        257,
        819,
        7877,
        13,
        51033
      ]
    },
    {
      "id": 43,
      "avg_logprob": -0.20915354788303375,
      "compression_ratio": 1.663082480430603,
      "end": 100.27999877929688,
      "no_speech_prob": 0.004538377746939659,
      "seek": 8578,
      "start": 99.16000366210938,
      "temperature": 0.0,
      "text": " I use Final Cut Pro.",
      "tokens": [
        51033,
        286,
        764,
        13443,
        9431,
        1705,
        13,
        51089
      ]
    },
    {
      "id": 44,
      "avg_logprob": -0.20915354788303375,
      "compression_ratio": 1.663082480430603,
      "end": 103.31999969482422,
      "no_speech_prob": 0.004538377746939659,
      "seek": 8578,
      "start": 100.27999877929688,
      "temperature": 0.0,
      "text": " So I'm gonna be converting these into FCP XML files.",
      "tokens": [
        51089,
        407,
        286,
        478,
        799,
        312,
        29942,
        613,
        666,
        479,
        20049,
        43484,
        7098,
        13,
        51241
      ]
    },
    {
      "id": 45,
      "avg_logprob": -0.20915354788303375,
      "compression_ratio": 1.663082480430603,
      "end": 104.58000183105469,
      "no_speech_prob": 0.004538377746939659,
      "seek": 8578,
      "start": 103.31999969482422,
      "temperature": 0.0,
      "text": " We're not gonna get into this too much,",
      "tokens": [
        51241,
        492,
        434,
        406,
        799,
        483,
        666,
        341,
        886,
        709,
        11,
        51304
      ]
    },
    {
      "id": 46,
      "avg_logprob": -0.20915354788303375,
      "compression_ratio": 1.663082480430603,
      "end": 106.4800033569336,
      "no_speech_prob": 0.004538377746939659,
      "seek": 8578,
      "start": 104.58000183105469,
      "temperature": 0.0,
      "text": " but there's what the format looks like.",
      "tokens": [
        51304,
        457,
        456,
        311,
        437,
        264,
        7877,
        1542,
        411,
        13,
        51399
      ]
    },
    {
      "id": 47,
      "avg_logprob": -0.20915354788303375,
      "compression_ratio": 1.663082480430603,
      "end": 111.68000030517578,
      "no_speech_prob": 0.004538377746939659,
      "seek": 8578,
      "start": 108.76000213623047,
      "temperature": 0.0,
      "text": " We're gonna go from prompt to prompt chain to AI agent,",
      "tokens": [
        51513,
        492,
        434,
        799,
        352,
        490,
        12391,
        281,
        12391,
        5021,
        281,
        7318,
        9461,
        11,
        51659
      ]
    },
    {
      "id": 48,
      "avg_logprob": -0.20915354788303375,
      "compression_ratio": 1.663082480430603,
      "end": 114.55999755859375,
      "no_speech_prob": 0.004538377746939659,
      "seek": 8578,
      "start": 111.68000030517578,
      "temperature": 0.0,
      "text": " and we're gonna see across these three levels",
      "tokens": [
        51659,
        293,
        321,
        434,
        799,
        536,
        2108,
        613,
        1045,
        4358,
        51803
      ]
    },
    {
      "id": 49,
      "avg_logprob": -0.24239203333854675,
      "compression_ratio": 1.6410256624221802,
      "end": 117.5,
      "no_speech_prob": 5.014671387471026e-06,
      "seek": 11456,
      "start": 114.55999755859375,
      "temperature": 0.0,
      "text": " of abstraction, which one performs the best.",
      "tokens": [
        50364,
        295,
        37765,
        11,
        597,
        472,
        26213,
        264,
        1151,
        13,
        50511
      ]
    },
    {
      "id": 50,
      "avg_logprob": -0.24239203333854675,
      "compression_ratio": 1.6410256624221802,
      "end": 119.69999694824219,
      "no_speech_prob": 5.014671387471026e-06,
      "seek": 11456,
      "start": 117.5,
      "temperature": 0.0,
      "text": " Let's go ahead and run a prompt",
      "tokens": [
        50511,
        961,
        311,
        352,
        2286,
        293,
        1190,
        257,
        12391,
        50621
      ]
    },
    {
      "id": 51,
      "avg_logprob": -0.24239203333854675,
      "compression_ratio": 1.6410256624221802,
      "end": 121.5,
      "no_speech_prob": 5.014671387471026e-06,
      "seek": 11456,
      "start": 119.69999694824219,
      "temperature": 0.0,
      "text": " and really understand what this problem looks like.",
      "tokens": [
        50621,
        293,
        534,
        1223,
        437,
        341,
        1154,
        1542,
        411,
        13,
        50711
      ]
    },
    {
      "id": 52,
      "avg_logprob": -0.24239203333854675,
      "compression_ratio": 1.6410256624221802,
      "end": 124.58000183105469,
      "no_speech_prob": 5.014671387471026e-06,
      "seek": 11456,
      "start": 121.5,
      "temperature": 0.0,
      "text": " On run a cut agent, and then we can kick this off.",
      "tokens": [
        50711,
        1282,
        1190,
        257,
        1723,
        9461,
        11,
        293,
        550,
        321,
        393,
        4437,
        341,
        766,
        13,
        50865
      ]
    },
    {
      "id": 53,
      "avg_logprob": -0.24239203333854675,
      "compression_ratio": 1.6410256624221802,
      "end": 126.69999694824219,
      "no_speech_prob": 5.014671387471026e-06,
      "seek": 11456,
      "start": 124.58000183105469,
      "temperature": 0.0,
      "text": " Let's let this prompt run.",
      "tokens": [
        50865,
        961,
        311,
        718,
        341,
        12391,
        1190,
        13,
        50971
      ]
    },
    {
      "id": 54,
      "avg_logprob": -0.24239203333854675,
      "compression_ratio": 1.6410256624221802,
      "end": 127.9000015258789,
      "no_speech_prob": 5.014671387471026e-06,
      "seek": 11456,
      "start": 126.69999694824219,
      "temperature": 0.0,
      "text": " So this is just running a single prompt.",
      "tokens": [
        50971,
        407,
        341,
        307,
        445,
        2614,
        257,
        2167,
        12391,
        13,
        51031
      ]
    },
    {
      "id": 55,
      "avg_logprob": -0.24239203333854675,
      "compression_ratio": 1.6410256624221802,
      "end": 129.4600067138672,
      "no_speech_prob": 5.014671387471026e-06,
      "seek": 11456,
      "start": 127.9000015258789,
      "temperature": 0.0,
      "text": " You can see it got the problem wrong.",
      "tokens": [
        51031,
        509,
        393,
        536,
        309,
        658,
        264,
        1154,
        2085,
        13,
        51109
      ]
    },
    {
      "id": 56,
      "avg_logprob": -0.24239203333854675,
      "compression_ratio": 1.6410256624221802,
      "end": 131.25999450683594,
      "no_speech_prob": 5.014671387471026e-06,
      "seek": 11456,
      "start": 129.4600067138672,
      "temperature": 0.0,
      "text": " Let's go ahead and figure out why.",
      "tokens": [
        51109,
        961,
        311,
        352,
        2286,
        293,
        2573,
        484,
        983,
        13,
        51199
      ]
    }
  ],
  "language": "english",
  "duration": 131.05999755859375,
  "timestamp": 1742835329.86548
}