{
  "audio_path": "data/chunks/age_of_agents_how_to_evolve_your_o3_mini_prompts_20250324_151153_chunk_008.mp3",
  "text": "and separate this just to make it a little bit more clear, right? So you can see those two removed words right there, okay? So this is our prompt chain and we can go ahead and dial into this code for a moment here, prompt chain. We can see here. We're just setting up logging, pulling out our model. We're pulling out the problem slice, starting text, and then we're actually running the code here. In the end, every one of these versions, right, every one of our prompt, our prompt chain, and our agent, it's just going to return deletions. It's really important to set up your workflows so that they're always returning consistent types, consistent formats. This allows you to operate on them at scale and create versions of them. So all we're doing here is outputting deletions and then some auxiliary information, right? In this case, we just have a cost. So let's go into generate a cut deletions v2. This is our prompt chain. So v1 is just a prompt, v2 is a prompt chain. So let's take a look at what this looks like and you can imagine it is, you know, effectively going to be a loop and you can see here we're passing in max compute and we're updating this to I think eight, but you can see here what's happening, right? We basically have the same thing and we're just looping this time, right? We're keeping track of some more state. This is our iteration slice. This is our original slice and here's our, you know, list of deletions that we're keeping track of and then of course, here's our loop, right? So just as you saw before, there's our instruction-rich prompt, purpose, and then our dynamic variables that get updated inside of our application, right? If we collapse the prompt, you can see we're always outputting our prompt file. We have our actual execution loop. This is the prompt chain. Let's go ahead and run the most exciting version, right? Let's run the AI agent. Let's kick this off. This agent has 16 compute loops. So I'm giving it 16 compute uses and unfortunately, GPT-40 got the problem wrong. So let's figure out why. Why did it edit this problem incorrectly, okay? Let's look at V3. You can see here it made four prompts in total and if we hop into this file, we can see a lot of the same things. The big difference here is that our AI agent now has access to a series of",
  "segments": [
    {
      "id": 0,
      "avg_logprob": -0.2871677875518799,
      "compression_ratio": 1.8591065406799316,
      "end": 2.5999999046325684,
      "no_speech_prob": 0.004007132723927498,
      "seek": 0,
      "start": 0.0,
      "temperature": 0.0,
      "text": " and separate this just to make it a little bit more clear, right?",
      "tokens": [
        50364,
        293,
        4994,
        341,
        445,
        281,
        652,
        309,
        257,
        707,
        857,
        544,
        1850,
        11,
        558,
        30,
        50494
      ]
    },
    {
      "id": 1,
      "avg_logprob": -0.2871677875518799,
      "compression_ratio": 1.8591065406799316,
      "end": 7.099999904632568,
      "no_speech_prob": 0.004007132723927498,
      "seek": 0,
      "start": 2.5999999046325684,
      "temperature": 0.0,
      "text": " So you can see those two removed words right there, okay?",
      "tokens": [
        50494,
        407,
        291,
        393,
        536,
        729,
        732,
        7261,
        2283,
        558,
        456,
        11,
        1392,
        30,
        50719
      ]
    },
    {
      "id": 2,
      "avg_logprob": -0.2871677875518799,
      "compression_ratio": 1.8591065406799316,
      "end": 8.899999618530273,
      "no_speech_prob": 0.004007132723927498,
      "seek": 0,
      "start": 7.099999904632568,
      "temperature": 0.0,
      "text": " So this is our prompt chain",
      "tokens": [
        50719,
        407,
        341,
        307,
        527,
        12391,
        5021,
        50809
      ]
    },
    {
      "id": 3,
      "avg_logprob": -0.2871677875518799,
      "compression_ratio": 1.8591065406799316,
      "end": 13.100000381469727,
      "no_speech_prob": 0.004007132723927498,
      "seek": 0,
      "start": 8.899999618530273,
      "temperature": 0.0,
      "text": " and we can go ahead and dial into this code for a moment here, prompt chain.",
      "tokens": [
        50809,
        293,
        321,
        393,
        352,
        2286,
        293,
        5502,
        666,
        341,
        3089,
        337,
        257,
        1623,
        510,
        11,
        12391,
        5021,
        13,
        51019
      ]
    },
    {
      "id": 4,
      "avg_logprob": -0.2871677875518799,
      "compression_ratio": 1.8591065406799316,
      "end": 14.0,
      "no_speech_prob": 0.004007132723927498,
      "seek": 0,
      "start": 13.100000381469727,
      "temperature": 0.0,
      "text": " We can see here.",
      "tokens": [
        51019,
        492,
        393,
        536,
        510,
        13,
        51064
      ]
    },
    {
      "id": 5,
      "avg_logprob": -0.2871677875518799,
      "compression_ratio": 1.8591065406799316,
      "end": 16.5,
      "no_speech_prob": 0.004007132723927498,
      "seek": 0,
      "start": 14.0,
      "temperature": 0.0,
      "text": " We're just setting up logging, pulling out our model.",
      "tokens": [
        51064,
        492,
        434,
        445,
        3287,
        493,
        27991,
        11,
        8407,
        484,
        527,
        2316,
        13,
        51189
      ]
    },
    {
      "id": 6,
      "avg_logprob": -0.2871677875518799,
      "compression_ratio": 1.8591065406799316,
      "end": 18.899999618530273,
      "no_speech_prob": 0.004007132723927498,
      "seek": 0,
      "start": 16.5,
      "temperature": 0.0,
      "text": " We're pulling out the problem slice, starting text,",
      "tokens": [
        51189,
        492,
        434,
        8407,
        484,
        264,
        1154,
        13153,
        11,
        2891,
        2487,
        11,
        51309
      ]
    },
    {
      "id": 7,
      "avg_logprob": -0.2871677875518799,
      "compression_ratio": 1.8591065406799316,
      "end": 20.899999618530273,
      "no_speech_prob": 0.004007132723927498,
      "seek": 0,
      "start": 18.899999618530273,
      "temperature": 0.0,
      "text": " and then we're actually running the code here.",
      "tokens": [
        51309,
        293,
        550,
        321,
        434,
        767,
        2614,
        264,
        3089,
        510,
        13,
        51409
      ]
    },
    {
      "id": 8,
      "avg_logprob": -0.2871677875518799,
      "compression_ratio": 1.8591065406799316,
      "end": 23.700000762939453,
      "no_speech_prob": 0.004007132723927498,
      "seek": 0,
      "start": 20.899999618530273,
      "temperature": 0.0,
      "text": " In the end, every one of these versions, right,",
      "tokens": [
        51409,
        682,
        264,
        917,
        11,
        633,
        472,
        295,
        613,
        9606,
        11,
        558,
        11,
        51549
      ]
    },
    {
      "id": 9,
      "avg_logprob": -0.2871677875518799,
      "compression_ratio": 1.8591065406799316,
      "end": 26.5,
      "no_speech_prob": 0.004007132723927498,
      "seek": 0,
      "start": 23.700000762939453,
      "temperature": 0.0,
      "text": " every one of our prompt, our prompt chain, and our agent,",
      "tokens": [
        51549,
        633,
        472,
        295,
        527,
        12391,
        11,
        527,
        12391,
        5021,
        11,
        293,
        527,
        9461,
        11,
        51689
      ]
    },
    {
      "id": 10,
      "avg_logprob": -0.2871677875518799,
      "compression_ratio": 1.8591065406799316,
      "end": 28.5,
      "no_speech_prob": 0.004007132723927498,
      "seek": 0,
      "start": 26.5,
      "temperature": 0.0,
      "text": " it's just going to return deletions.",
      "tokens": [
        51689,
        309,
        311,
        445,
        516,
        281,
        2736,
        1103,
        302,
        626,
        13,
        51789
      ]
    },
    {
      "id": 11,
      "avg_logprob": -0.17063140869140625,
      "compression_ratio": 1.7266186475753784,
      "end": 31.299999237060547,
      "no_speech_prob": 9.761468390934169e-05,
      "seek": 2850,
      "start": 28.5,
      "temperature": 0.0,
      "text": " It's really important to set up your workflows",
      "tokens": [
        50364,
        467,
        311,
        534,
        1021,
        281,
        992,
        493,
        428,
        43461,
        50504
      ]
    },
    {
      "id": 12,
      "avg_logprob": -0.17063140869140625,
      "compression_ratio": 1.7266186475753784,
      "end": 35.20000076293945,
      "no_speech_prob": 9.761468390934169e-05,
      "seek": 2850,
      "start": 31.299999237060547,
      "temperature": 0.0,
      "text": " so that they're always returning consistent types, consistent formats.",
      "tokens": [
        50504,
        370,
        300,
        436,
        434,
        1009,
        12678,
        8398,
        3467,
        11,
        8398,
        25879,
        13,
        50699
      ]
    },
    {
      "id": 13,
      "avg_logprob": -0.17063140869140625,
      "compression_ratio": 1.7266186475753784,
      "end": 38.599998474121094,
      "no_speech_prob": 9.761468390934169e-05,
      "seek": 2850,
      "start": 35.20000076293945,
      "temperature": 0.0,
      "text": " This allows you to operate on them at scale and create versions of them.",
      "tokens": [
        50699,
        639,
        4045,
        291,
        281,
        9651,
        322,
        552,
        412,
        4373,
        293,
        1884,
        9606,
        295,
        552,
        13,
        50869
      ]
    },
    {
      "id": 14,
      "avg_logprob": -0.17063140869140625,
      "compression_ratio": 1.7266186475753784,
      "end": 41.5,
      "no_speech_prob": 9.761468390934169e-05,
      "seek": 2850,
      "start": 38.599998474121094,
      "temperature": 0.0,
      "text": " So all we're doing here is outputting deletions",
      "tokens": [
        50869,
        407,
        439,
        321,
        434,
        884,
        510,
        307,
        5598,
        783,
        1103,
        302,
        626,
        51014
      ]
    },
    {
      "id": 15,
      "avg_logprob": -0.17063140869140625,
      "compression_ratio": 1.7266186475753784,
      "end": 43.20000076293945,
      "no_speech_prob": 9.761468390934169e-05,
      "seek": 2850,
      "start": 41.5,
      "temperature": 0.0,
      "text": " and then some auxiliary information, right?",
      "tokens": [
        51014,
        293,
        550,
        512,
        43741,
        1589,
        11,
        558,
        30,
        51099
      ]
    },
    {
      "id": 16,
      "avg_logprob": -0.17063140869140625,
      "compression_ratio": 1.7266186475753784,
      "end": 44.70000076293945,
      "no_speech_prob": 9.761468390934169e-05,
      "seek": 2850,
      "start": 43.20000076293945,
      "temperature": 0.0,
      "text": " In this case, we just have a cost.",
      "tokens": [
        51099,
        682,
        341,
        1389,
        11,
        321,
        445,
        362,
        257,
        2063,
        13,
        51174
      ]
    },
    {
      "id": 17,
      "avg_logprob": -0.17063140869140625,
      "compression_ratio": 1.7266186475753784,
      "end": 47.900001525878906,
      "no_speech_prob": 9.761468390934169e-05,
      "seek": 2850,
      "start": 44.70000076293945,
      "temperature": 0.0,
      "text": " So let's go into generate a cut deletions v2.",
      "tokens": [
        51174,
        407,
        718,
        311,
        352,
        666,
        8460,
        257,
        1723,
        1103,
        302,
        626,
        371,
        17,
        13,
        51334
      ]
    },
    {
      "id": 18,
      "avg_logprob": -0.17063140869140625,
      "compression_ratio": 1.7266186475753784,
      "end": 49.099998474121094,
      "no_speech_prob": 9.761468390934169e-05,
      "seek": 2850,
      "start": 47.900001525878906,
      "temperature": 0.0,
      "text": " This is our prompt chain.",
      "tokens": [
        51334,
        639,
        307,
        527,
        12391,
        5021,
        13,
        51394
      ]
    },
    {
      "id": 19,
      "avg_logprob": -0.17063140869140625,
      "compression_ratio": 1.7266186475753784,
      "end": 53.79999923706055,
      "no_speech_prob": 9.761468390934169e-05,
      "seek": 2850,
      "start": 49.099998474121094,
      "temperature": 0.0,
      "text": " So v1 is just a prompt, v2 is a prompt chain.",
      "tokens": [
        51394,
        407,
        371,
        16,
        307,
        445,
        257,
        12391,
        11,
        371,
        17,
        307,
        257,
        12391,
        5021,
        13,
        51629
      ]
    },
    {
      "id": 20,
      "avg_logprob": -0.17063140869140625,
      "compression_ratio": 1.7266186475753784,
      "end": 55.599998474121094,
      "no_speech_prob": 9.761468390934169e-05,
      "seek": 2850,
      "start": 53.79999923706055,
      "temperature": 0.0,
      "text": " So let's take a look at what this looks like",
      "tokens": [
        51629,
        407,
        718,
        311,
        747,
        257,
        574,
        412,
        437,
        341,
        1542,
        411,
        51719
      ]
    },
    {
      "id": 21,
      "avg_logprob": -0.19727951288223267,
      "compression_ratio": 1.9635258913040161,
      "end": 59.20000076293945,
      "no_speech_prob": 0.005301700439304113,
      "seek": 5560,
      "start": 55.70000076293945,
      "temperature": 0.0,
      "text": " and you can imagine it is, you know, effectively going to be a loop",
      "tokens": [
        50369,
        293,
        291,
        393,
        3811,
        309,
        307,
        11,
        291,
        458,
        11,
        8659,
        516,
        281,
        312,
        257,
        6367,
        50544
      ]
    },
    {
      "id": 22,
      "avg_logprob": -0.19727951288223267,
      "compression_ratio": 1.9635258913040161,
      "end": 61.400001525878906,
      "no_speech_prob": 0.005301700439304113,
      "seek": 5560,
      "start": 59.20000076293945,
      "temperature": 0.0,
      "text": " and you can see here we're passing in max compute",
      "tokens": [
        50544,
        293,
        291,
        393,
        536,
        510,
        321,
        434,
        8437,
        294,
        11469,
        14722,
        50654
      ]
    },
    {
      "id": 23,
      "avg_logprob": -0.19727951288223267,
      "compression_ratio": 1.9635258913040161,
      "end": 63.099998474121094,
      "no_speech_prob": 0.005301700439304113,
      "seek": 5560,
      "start": 61.400001525878906,
      "temperature": 0.0,
      "text": " and we're updating this to I think eight,",
      "tokens": [
        50654,
        293,
        321,
        434,
        25113,
        341,
        281,
        286,
        519,
        3180,
        11,
        50739
      ]
    },
    {
      "id": 24,
      "avg_logprob": -0.19727951288223267,
      "compression_ratio": 1.9635258913040161,
      "end": 64.4000015258789,
      "no_speech_prob": 0.005301700439304113,
      "seek": 5560,
      "start": 63.099998474121094,
      "temperature": 0.0,
      "text": " but you can see here what's happening, right?",
      "tokens": [
        50739,
        457,
        291,
        393,
        536,
        510,
        437,
        311,
        2737,
        11,
        558,
        30,
        50804
      ]
    },
    {
      "id": 25,
      "avg_logprob": -0.19727951288223267,
      "compression_ratio": 1.9635258913040161,
      "end": 67.4000015258789,
      "no_speech_prob": 0.005301700439304113,
      "seek": 5560,
      "start": 64.4000015258789,
      "temperature": 0.0,
      "text": " We basically have the same thing and we're just looping this time, right?",
      "tokens": [
        50804,
        492,
        1936,
        362,
        264,
        912,
        551,
        293,
        321,
        434,
        445,
        6367,
        278,
        341,
        565,
        11,
        558,
        30,
        50954
      ]
    },
    {
      "id": 26,
      "avg_logprob": -0.19727951288223267,
      "compression_ratio": 1.9635258913040161,
      "end": 68.69999694824219,
      "no_speech_prob": 0.005301700439304113,
      "seek": 5560,
      "start": 67.4000015258789,
      "temperature": 0.0,
      "text": " We're keeping track of some more state.",
      "tokens": [
        50954,
        492,
        434,
        5145,
        2837,
        295,
        512,
        544,
        1785,
        13,
        51019
      ]
    },
    {
      "id": 27,
      "avg_logprob": -0.19727951288223267,
      "compression_ratio": 1.9635258913040161,
      "end": 69.80000305175781,
      "no_speech_prob": 0.005301700439304113,
      "seek": 5560,
      "start": 68.69999694824219,
      "temperature": 0.0,
      "text": " This is our iteration slice.",
      "tokens": [
        51019,
        639,
        307,
        527,
        24784,
        13153,
        13,
        51074
      ]
    },
    {
      "id": 28,
      "avg_logprob": -0.19727951288223267,
      "compression_ratio": 1.9635258913040161,
      "end": 71.19999694824219,
      "no_speech_prob": 0.005301700439304113,
      "seek": 5560,
      "start": 69.80000305175781,
      "temperature": 0.0,
      "text": " This is our original slice",
      "tokens": [
        51074,
        639,
        307,
        527,
        3380,
        13153,
        51144
      ]
    },
    {
      "id": 29,
      "avg_logprob": -0.19727951288223267,
      "compression_ratio": 1.9635258913040161,
      "end": 74.5999984741211,
      "no_speech_prob": 0.005301700439304113,
      "seek": 5560,
      "start": 71.19999694824219,
      "temperature": 0.0,
      "text": " and here's our, you know, list of deletions that we're keeping track of",
      "tokens": [
        51144,
        293,
        510,
        311,
        527,
        11,
        291,
        458,
        11,
        1329,
        295,
        1103,
        302,
        626,
        300,
        321,
        434,
        5145,
        2837,
        295,
        51314
      ]
    },
    {
      "id": 30,
      "avg_logprob": -0.19727951288223267,
      "compression_ratio": 1.9635258913040161,
      "end": 76.5999984741211,
      "no_speech_prob": 0.005301700439304113,
      "seek": 5560,
      "start": 74.5999984741211,
      "temperature": 0.0,
      "text": " and then of course, here's our loop, right?",
      "tokens": [
        51314,
        293,
        550,
        295,
        1164,
        11,
        510,
        311,
        527,
        6367,
        11,
        558,
        30,
        51414
      ]
    },
    {
      "id": 31,
      "avg_logprob": -0.19727951288223267,
      "compression_ratio": 1.9635258913040161,
      "end": 79.5999984741211,
      "no_speech_prob": 0.005301700439304113,
      "seek": 5560,
      "start": 76.5999984741211,
      "temperature": 0.0,
      "text": " So just as you saw before, there's our instruction-rich prompt,",
      "tokens": [
        51414,
        407,
        445,
        382,
        291,
        1866,
        949,
        11,
        456,
        311,
        527,
        10951,
        12,
        10794,
        12391,
        11,
        51564
      ]
    },
    {
      "id": 32,
      "avg_logprob": -0.19727951288223267,
      "compression_ratio": 1.9635258913040161,
      "end": 82.5999984741211,
      "no_speech_prob": 0.005301700439304113,
      "seek": 5560,
      "start": 79.5999984741211,
      "temperature": 0.0,
      "text": " purpose, and then our dynamic variables",
      "tokens": [
        51564,
        4334,
        11,
        293,
        550,
        527,
        8546,
        9102,
        51714
      ]
    },
    {
      "id": 33,
      "avg_logprob": -0.19727951288223267,
      "compression_ratio": 1.9635258913040161,
      "end": 84.69999694824219,
      "no_speech_prob": 0.005301700439304113,
      "seek": 5560,
      "start": 82.5999984741211,
      "temperature": 0.0,
      "text": " that get updated inside of our application, right?",
      "tokens": [
        51714,
        300,
        483,
        10588,
        1854,
        295,
        527,
        3861,
        11,
        558,
        30,
        51819
      ]
    },
    {
      "id": 34,
      "avg_logprob": -0.19621127843856812,
      "compression_ratio": 1.565573811531067,
      "end": 88.5,
      "no_speech_prob": 0.00042388090514577925,
      "seek": 8470,
      "start": 84.80000305175781,
      "temperature": 0.0,
      "text": " If we collapse the prompt, you can see we're always outputting our prompt file.",
      "tokens": [
        50369,
        759,
        321,
        15584,
        264,
        12391,
        11,
        291,
        393,
        536,
        321,
        434,
        1009,
        5598,
        783,
        527,
        12391,
        3991,
        13,
        50554
      ]
    },
    {
      "id": 35,
      "avg_logprob": -0.19621127843856812,
      "compression_ratio": 1.565573811531067,
      "end": 91.0,
      "no_speech_prob": 0.00042388090514577925,
      "seek": 8470,
      "start": 88.5,
      "temperature": 0.0,
      "text": " We have our actual execution loop.",
      "tokens": [
        50554,
        492,
        362,
        527,
        3539,
        15058,
        6367,
        13,
        50679
      ]
    },
    {
      "id": 36,
      "avg_logprob": -0.19621127843856812,
      "compression_ratio": 1.565573811531067,
      "end": 92.9000015258789,
      "no_speech_prob": 0.00042388090514577925,
      "seek": 8470,
      "start": 91.0,
      "temperature": 0.0,
      "text": " This is the prompt chain.",
      "tokens": [
        50679,
        639,
        307,
        264,
        12391,
        5021,
        13,
        50774
      ]
    },
    {
      "id": 37,
      "avg_logprob": -0.19621127843856812,
      "compression_ratio": 1.565573811531067,
      "end": 97.0,
      "no_speech_prob": 0.00042388090514577925,
      "seek": 8470,
      "start": 92.9000015258789,
      "temperature": 0.0,
      "text": " Let's go ahead and run the most exciting version, right?",
      "tokens": [
        50774,
        961,
        311,
        352,
        2286,
        293,
        1190,
        264,
        881,
        4670,
        3037,
        11,
        558,
        30,
        50979
      ]
    },
    {
      "id": 38,
      "avg_logprob": -0.19621127843856812,
      "compression_ratio": 1.565573811531067,
      "end": 99.5,
      "no_speech_prob": 0.00042388090514577925,
      "seek": 8470,
      "start": 97.0,
      "temperature": 0.0,
      "text": " Let's run the AI agent.",
      "tokens": [
        50979,
        961,
        311,
        1190,
        264,
        7318,
        9461,
        13,
        51104
      ]
    },
    {
      "id": 39,
      "avg_logprob": -0.19621127843856812,
      "compression_ratio": 1.565573811531067,
      "end": 103.0,
      "no_speech_prob": 0.00042388090514577925,
      "seek": 8470,
      "start": 102.0,
      "temperature": 0.0,
      "text": " Let's kick this off.",
      "tokens": [
        51229,
        961,
        311,
        4437,
        341,
        766,
        13,
        51279
      ]
    },
    {
      "id": 40,
      "avg_logprob": -0.19621127843856812,
      "compression_ratio": 1.565573811531067,
      "end": 107.0,
      "no_speech_prob": 0.00042388090514577925,
      "seek": 8470,
      "start": 104.0,
      "temperature": 0.0,
      "text": " This agent has 16 compute loops.",
      "tokens": [
        51329,
        639,
        9461,
        575,
        3165,
        14722,
        16121,
        13,
        51479
      ]
    },
    {
      "id": 41,
      "avg_logprob": -0.19621127843856812,
      "compression_ratio": 1.565573811531067,
      "end": 109.5,
      "no_speech_prob": 0.00042388090514577925,
      "seek": 8470,
      "start": 107.0,
      "temperature": 0.0,
      "text": " So I'm giving it 16 compute uses",
      "tokens": [
        51479,
        407,
        286,
        478,
        2902,
        309,
        3165,
        14722,
        4960,
        51604
      ]
    },
    {
      "id": 42,
      "avg_logprob": -0.19621127843856812,
      "compression_ratio": 1.565573811531067,
      "end": 113.5999984741211,
      "no_speech_prob": 0.00042388090514577925,
      "seek": 8470,
      "start": 109.5,
      "temperature": 0.0,
      "text": " and unfortunately, GPT-40 got the problem wrong.",
      "tokens": [
        51604,
        293,
        7015,
        11,
        26039,
        51,
        12,
        5254,
        658,
        264,
        1154,
        2085,
        13,
        51809
      ]
    },
    {
      "id": 43,
      "avg_logprob": -0.19621127843856812,
      "compression_ratio": 1.565573811531067,
      "end": 114.5999984741211,
      "no_speech_prob": 0.00042388090514577925,
      "seek": 8470,
      "start": 113.5999984741211,
      "temperature": 0.0,
      "text": " So let's figure out why.",
      "tokens": [
        51809,
        407,
        718,
        311,
        2573,
        484,
        983,
        13,
        51859
      ]
    },
    {
      "id": 44,
      "avg_logprob": -0.25334426760673523,
      "compression_ratio": 1.4269663095474243,
      "end": 117.19999694824219,
      "no_speech_prob": 0.0012065675109624863,
      "seek": 11460,
      "start": 114.5999984741211,
      "temperature": 0.0,
      "text": " Why did it edit this problem incorrectly, okay?",
      "tokens": [
        50364,
        1545,
        630,
        309,
        8129,
        341,
        1154,
        42892,
        11,
        1392,
        30,
        50494
      ]
    },
    {
      "id": 45,
      "avg_logprob": -0.25334426760673523,
      "compression_ratio": 1.4269663095474243,
      "end": 118.80000305175781,
      "no_speech_prob": 0.0012065675109624863,
      "seek": 11460,
      "start": 117.19999694824219,
      "temperature": 0.0,
      "text": " Let's look at V3.",
      "tokens": [
        50494,
        961,
        311,
        574,
        412,
        691,
        18,
        13,
        50574
      ]
    },
    {
      "id": 46,
      "avg_logprob": -0.25334426760673523,
      "compression_ratio": 1.4269663095474243,
      "end": 121.5999984741211,
      "no_speech_prob": 0.0012065675109624863,
      "seek": 11460,
      "start": 118.80000305175781,
      "temperature": 0.0,
      "text": " You can see here it made four prompts in total",
      "tokens": [
        50574,
        509,
        393,
        536,
        510,
        309,
        1027,
        1451,
        41095,
        294,
        3217,
        50714
      ]
    },
    {
      "id": 47,
      "avg_logprob": -0.25334426760673523,
      "compression_ratio": 1.4269663095474243,
      "end": 125.30000305175781,
      "no_speech_prob": 0.0012065675109624863,
      "seek": 11460,
      "start": 121.5999984741211,
      "temperature": 0.0,
      "text": " and if we hop into this file, we can see a lot of the same things.",
      "tokens": [
        50714,
        293,
        498,
        321,
        3818,
        666,
        341,
        3991,
        11,
        321,
        393,
        536,
        257,
        688,
        295,
        264,
        912,
        721,
        13,
        50899
      ]
    },
    {
      "id": 48,
      "avg_logprob": -0.25334426760673523,
      "compression_ratio": 1.4269663095474243,
      "end": 130.89999389648438,
      "no_speech_prob": 0.0012065675109624863,
      "seek": 11460,
      "start": 125.30000305175781,
      "temperature": 0.0,
      "text": " The big difference here is that our AI agent now has access to a series of",
      "tokens": [
        50899,
        440,
        955,
        2649,
        510,
        307,
        300,
        527,
        7318,
        9461,
        586,
        575,
        2105,
        281,
        257,
        2638,
        295,
        51179
      ]
    }
  ],
  "language": "english",
  "duration": 131.05999755859375,
  "timestamp": 1742835398.728637
}