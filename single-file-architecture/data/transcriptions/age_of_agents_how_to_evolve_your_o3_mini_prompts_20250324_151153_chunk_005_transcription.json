{
  "audio_path": "data/chunks/age_of_agents_how_to_evolve_your_o3_mini_prompts_20250324_151153_chunk_005.mp3",
  "text": "purpose, we have our instructions, and then we have our variables at the bottom. This was a dynamic variable and our application replaced it. So this, you know, looks like this in our code, right? So this is a, you know, variable that will be replaced. And what it ended up doing here was placing the iteration slice that we're operating on, right? And remember the slice is just a small piece of the entire transcript. So that's what the iteration slice is. You can see we have some instructions here, classic prompting, nothing new here that we haven't discussed on the channel before. Let's scale this up, right? This is the prompt. And if we open up our LLMs file, collapse everything, we have the intelligence comment here, and this is what that looks like in code, okay? So nothing too fancy, but it is important that I share it here because this tool is proprietary. Prompt up there, we have our XML slice that we're replacing with the incoming slice, and then we're doing some logging, reasoning effort check, and then we're just pulling out a bunch of auxiliary information from the response, right? That's the prompt. If we look at Anthropx building effective agents doc, it all starts with the prompt. What we've done here is we've given our language model access to an output structure so that we can pass in a prompt. Our LLM then makes a judgment call on the output. In the prompt version, we don't have anything fancy here. All we have is a structured outputs call that generates our deletions. And so that's all our prompt is doing. You can see here we have our response format that has our uncut deletion, and we are using Zodd response format for structured outputs. Let's go ahead and kick up our compute. There's two levers we can pull here right away. We can just do one of them for fun, right? We can quickly just take this, right? This is running GPT-40. We can go config, we can go LLM model, we can go O3 mini, and then we can just quickly, you know, kick this off again. And now O3 mini is going to take a hit at solving this problem. Let's take a look at the output that O3 mini gave us. Same deal. We have the original text, we have the target, there's the prompt for O3 mini, here's the deletion it created. And so if we scroll down here, you can see O3 mini got this problem right with just a prompt. Very cool to see",
  "segments": [
    {
      "id": 0,
      "avg_logprob": -0.2232224941253662,
      "compression_ratio": 1.864661693572998,
      "end": 6.800000190734863,
      "no_speech_prob": 0.008984766900539398,
      "seek": 0,
      "start": 0.0,
      "temperature": 0.0,
      "text": " purpose, we have our instructions, and then we have our variables at the bottom. This was a dynamic",
      "tokens": [
        50364,
        4334,
        11,
        321,
        362,
        527,
        9415,
        11,
        293,
        550,
        321,
        362,
        527,
        9102,
        412,
        264,
        2767,
        13,
        639,
        390,
        257,
        8546,
        50704
      ]
    },
    {
      "id": 1,
      "avg_logprob": -0.2232224941253662,
      "compression_ratio": 1.864661693572998,
      "end": 12.15999984741211,
      "no_speech_prob": 0.008984766900539398,
      "seek": 0,
      "start": 6.800000190734863,
      "temperature": 0.0,
      "text": " variable and our application replaced it. So this, you know, looks like this in our code, right? So",
      "tokens": [
        50704,
        7006,
        293,
        527,
        3861,
        10772,
        309,
        13,
        407,
        341,
        11,
        291,
        458,
        11,
        1542,
        411,
        341,
        294,
        527,
        3089,
        11,
        558,
        30,
        407,
        50972
      ]
    },
    {
      "id": 2,
      "avg_logprob": -0.2232224941253662,
      "compression_ratio": 1.864661693572998,
      "end": 18.0,
      "no_speech_prob": 0.008984766900539398,
      "seek": 0,
      "start": 12.15999984741211,
      "temperature": 0.0,
      "text": " this is a, you know, variable that will be replaced. And what it ended up doing here was placing the",
      "tokens": [
        50972,
        341,
        307,
        257,
        11,
        291,
        458,
        11,
        7006,
        300,
        486,
        312,
        10772,
        13,
        400,
        437,
        309,
        4590,
        493,
        884,
        510,
        390,
        17221,
        264,
        51264
      ]
    },
    {
      "id": 3,
      "avg_logprob": -0.2232224941253662,
      "compression_ratio": 1.864661693572998,
      "end": 23.920000076293945,
      "no_speech_prob": 0.008984766900539398,
      "seek": 0,
      "start": 18.0,
      "temperature": 0.0,
      "text": " iteration slice that we're operating on, right? And remember the slice is just a small piece of",
      "tokens": [
        51264,
        24784,
        13153,
        300,
        321,
        434,
        7447,
        322,
        11,
        558,
        30,
        400,
        1604,
        264,
        13153,
        307,
        445,
        257,
        1359,
        2522,
        295,
        51560
      ]
    },
    {
      "id": 4,
      "avg_logprob": -0.2232224941253662,
      "compression_ratio": 1.864661693572998,
      "end": 27.520000457763672,
      "no_speech_prob": 0.008984766900539398,
      "seek": 0,
      "start": 23.920000076293945,
      "temperature": 0.0,
      "text": " the entire transcript. So that's what the iteration slice is. You can see we have some instructions",
      "tokens": [
        51560,
        264,
        2302,
        24444,
        13,
        407,
        300,
        311,
        437,
        264,
        24784,
        13153,
        307,
        13,
        509,
        393,
        536,
        321,
        362,
        512,
        9415,
        51740
      ]
    },
    {
      "id": 5,
      "avg_logprob": -0.17429576814174652,
      "compression_ratio": 1.7155425548553467,
      "end": 31.920000076293945,
      "no_speech_prob": 0.011331046000123024,
      "seek": 2752,
      "start": 27.520000457763672,
      "temperature": 0.0,
      "text": " here, classic prompting, nothing new here that we haven't discussed on the channel before. Let's",
      "tokens": [
        50364,
        510,
        11,
        7230,
        12391,
        278,
        11,
        1825,
        777,
        510,
        300,
        321,
        2378,
        380,
        7152,
        322,
        264,
        2269,
        949,
        13,
        961,
        311,
        50584
      ]
    },
    {
      "id": 6,
      "avg_logprob": -0.17429576814174652,
      "compression_ratio": 1.7155425548553467,
      "end": 37.20000076293945,
      "no_speech_prob": 0.011331046000123024,
      "seek": 2752,
      "start": 31.920000076293945,
      "temperature": 0.0,
      "text": " scale this up, right? This is the prompt. And if we open up our LLMs file, collapse everything,",
      "tokens": [
        50584,
        4373,
        341,
        493,
        11,
        558,
        30,
        639,
        307,
        264,
        12391,
        13,
        400,
        498,
        321,
        1269,
        493,
        527,
        441,
        43,
        26386,
        3991,
        11,
        15584,
        1203,
        11,
        50848
      ]
    },
    {
      "id": 7,
      "avg_logprob": -0.17429576814174652,
      "compression_ratio": 1.7155425548553467,
      "end": 42.560001373291016,
      "no_speech_prob": 0.011331046000123024,
      "seek": 2752,
      "start": 37.20000076293945,
      "temperature": 0.0,
      "text": " we have the intelligence comment here, and this is what that looks like in code, okay? So nothing",
      "tokens": [
        50848,
        321,
        362,
        264,
        7599,
        2871,
        510,
        11,
        293,
        341,
        307,
        437,
        300,
        1542,
        411,
        294,
        3089,
        11,
        1392,
        30,
        407,
        1825,
        51116
      ]
    },
    {
      "id": 8,
      "avg_logprob": -0.17429576814174652,
      "compression_ratio": 1.7155425548553467,
      "end": 47.040000915527344,
      "no_speech_prob": 0.011331046000123024,
      "seek": 2752,
      "start": 42.560001373291016,
      "temperature": 0.0,
      "text": " too fancy, but it is important that I share it here because this tool is proprietary. Prompt up",
      "tokens": [
        51116,
        886,
        10247,
        11,
        457,
        309,
        307,
        1021,
        300,
        286,
        2073,
        309,
        510,
        570,
        341,
        2290,
        307,
        38992,
        13,
        15833,
        662,
        493,
        51340
      ]
    },
    {
      "id": 9,
      "avg_logprob": -0.17429576814174652,
      "compression_ratio": 1.7155425548553467,
      "end": 52.0,
      "no_speech_prob": 0.011331046000123024,
      "seek": 2752,
      "start": 47.040000915527344,
      "temperature": 0.0,
      "text": " there, we have our XML slice that we're replacing with the incoming slice, and then we're doing some",
      "tokens": [
        51340,
        456,
        11,
        321,
        362,
        527,
        43484,
        13153,
        300,
        321,
        434,
        19139,
        365,
        264,
        22341,
        13153,
        11,
        293,
        550,
        321,
        434,
        884,
        512,
        51588
      ]
    },
    {
      "id": 10,
      "avg_logprob": -0.17429576814174652,
      "compression_ratio": 1.7155425548553467,
      "end": 56.63999938964844,
      "no_speech_prob": 0.011331046000123024,
      "seek": 2752,
      "start": 52.0,
      "temperature": 0.0,
      "text": " logging, reasoning effort check, and then we're just pulling out a bunch of auxiliary information",
      "tokens": [
        51588,
        27991,
        11,
        21577,
        4630,
        1520,
        11,
        293,
        550,
        321,
        434,
        445,
        8407,
        484,
        257,
        3840,
        295,
        43741,
        1589,
        51820
      ]
    },
    {
      "id": 11,
      "avg_logprob": -0.18597561120986938,
      "compression_ratio": 1.6971831321716309,
      "end": 62.400001525878906,
      "no_speech_prob": 0.0037653562612831593,
      "seek": 5664,
      "start": 56.720001220703125,
      "temperature": 0.0,
      "text": " from the response, right? That's the prompt. If we look at Anthropx building effective agents doc,",
      "tokens": [
        50368,
        490,
        264,
        4134,
        11,
        558,
        30,
        663,
        311,
        264,
        12391,
        13,
        759,
        321,
        574,
        412,
        12727,
        1513,
        87,
        2390,
        4942,
        12554,
        3211,
        11,
        50652
      ]
    },
    {
      "id": 12,
      "avg_logprob": -0.18597561120986938,
      "compression_ratio": 1.6971831321716309,
      "end": 69.12000274658203,
      "no_speech_prob": 0.0037653562612831593,
      "seek": 5664,
      "start": 62.400001525878906,
      "temperature": 0.0,
      "text": " it all starts with the prompt. What we've done here is we've given our language model access to",
      "tokens": [
        50652,
        309,
        439,
        3719,
        365,
        264,
        12391,
        13,
        708,
        321,
        600,
        1096,
        510,
        307,
        321,
        600,
        2212,
        527,
        2856,
        2316,
        2105,
        281,
        50988
      ]
    },
    {
      "id": 13,
      "avg_logprob": -0.18597561120986938,
      "compression_ratio": 1.6971831321716309,
      "end": 75.27999877929688,
      "no_speech_prob": 0.0037653562612831593,
      "seek": 5664,
      "start": 69.12000274658203,
      "temperature": 0.0,
      "text": " an output structure so that we can pass in a prompt. Our LLM then makes a judgment call on",
      "tokens": [
        50988,
        364,
        5598,
        3877,
        370,
        300,
        321,
        393,
        1320,
        294,
        257,
        12391,
        13,
        2621,
        441,
        43,
        44,
        550,
        1669,
        257,
        12216,
        818,
        322,
        51296
      ]
    },
    {
      "id": 14,
      "avg_logprob": -0.18597561120986938,
      "compression_ratio": 1.6971831321716309,
      "end": 80.0,
      "no_speech_prob": 0.0037653562612831593,
      "seek": 5664,
      "start": 75.27999877929688,
      "temperature": 0.0,
      "text": " the output. In the prompt version, we don't have anything fancy here. All we have is a structured",
      "tokens": [
        51296,
        264,
        5598,
        13,
        682,
        264,
        12391,
        3037,
        11,
        321,
        500,
        380,
        362,
        1340,
        10247,
        510,
        13,
        1057,
        321,
        362,
        307,
        257,
        18519,
        51532
      ]
    },
    {
      "id": 15,
      "avg_logprob": -0.18597561120986938,
      "compression_ratio": 1.6971831321716309,
      "end": 86.0,
      "no_speech_prob": 0.0037653562612831593,
      "seek": 5664,
      "start": 80.0,
      "temperature": 0.0,
      "text": " outputs call that generates our deletions. And so that's all our prompt is doing. You can see here",
      "tokens": [
        51532,
        23930,
        818,
        300,
        23815,
        527,
        1103,
        302,
        626,
        13,
        400,
        370,
        300,
        311,
        439,
        527,
        12391,
        307,
        884,
        13,
        509,
        393,
        536,
        510,
        51832
      ]
    },
    {
      "id": 16,
      "avg_logprob": -0.18104620277881622,
      "compression_ratio": 1.695804238319397,
      "end": 92.16000366210938,
      "no_speech_prob": 0.00026530097238719463,
      "seek": 8600,
      "start": 86.0,
      "temperature": 0.0,
      "text": " we have our response format that has our uncut deletion, and we are using Zodd response format",
      "tokens": [
        50364,
        321,
        362,
        527,
        4134,
        7877,
        300,
        575,
        527,
        6219,
        325,
        1103,
        302,
        313,
        11,
        293,
        321,
        366,
        1228,
        1176,
        378,
        67,
        4134,
        7877,
        50672
      ]
    },
    {
      "id": 17,
      "avg_logprob": -0.18104620277881622,
      "compression_ratio": 1.695804238319397,
      "end": 98.55999755859375,
      "no_speech_prob": 0.00026530097238719463,
      "seek": 8600,
      "start": 92.16000366210938,
      "temperature": 0.0,
      "text": " for structured outputs. Let's go ahead and kick up our compute. There's two levers we can pull here",
      "tokens": [
        50672,
        337,
        18519,
        23930,
        13,
        961,
        311,
        352,
        2286,
        293,
        4437,
        493,
        527,
        14722,
        13,
        821,
        311,
        732,
        45571,
        321,
        393,
        2235,
        510,
        50992
      ]
    },
    {
      "id": 18,
      "avg_logprob": -0.18104620277881622,
      "compression_ratio": 1.695804238319397,
      "end": 103.5199966430664,
      "no_speech_prob": 0.00026530097238719463,
      "seek": 8600,
      "start": 98.55999755859375,
      "temperature": 0.0,
      "text": " right away. We can just do one of them for fun, right? We can quickly just take this, right? This",
      "tokens": [
        50992,
        558,
        1314,
        13,
        492,
        393,
        445,
        360,
        472,
        295,
        552,
        337,
        1019,
        11,
        558,
        30,
        492,
        393,
        2661,
        445,
        747,
        341,
        11,
        558,
        30,
        639,
        51240
      ]
    },
    {
      "id": 19,
      "avg_logprob": -0.18104620277881622,
      "compression_ratio": 1.695804238319397,
      "end": 108.72000122070312,
      "no_speech_prob": 0.00026530097238719463,
      "seek": 8600,
      "start": 103.5199966430664,
      "temperature": 0.0,
      "text": " is running GPT-40. We can go config, we can go LLM model, we can go O3 mini, and then we can just",
      "tokens": [
        51240,
        307,
        2614,
        26039,
        51,
        12,
        5254,
        13,
        492,
        393,
        352,
        6662,
        11,
        321,
        393,
        352,
        441,
        43,
        44,
        2316,
        11,
        321,
        393,
        352,
        422,
        18,
        8382,
        11,
        293,
        550,
        321,
        393,
        445,
        51500
      ]
    },
    {
      "id": 20,
      "avg_logprob": -0.18104620277881622,
      "compression_ratio": 1.695804238319397,
      "end": 114.31999969482422,
      "no_speech_prob": 0.00026530097238719463,
      "seek": 8600,
      "start": 108.72000122070312,
      "temperature": 0.0,
      "text": " quickly, you know, kick this off again. And now O3 mini is going to take a hit at solving this",
      "tokens": [
        51500,
        2661,
        11,
        291,
        458,
        11,
        4437,
        341,
        766,
        797,
        13,
        400,
        586,
        422,
        18,
        8382,
        307,
        516,
        281,
        747,
        257,
        2045,
        412,
        12606,
        341,
        51780
      ]
    },
    {
      "id": 21,
      "avg_logprob": -0.16662892699241638,
      "compression_ratio": 1.54450261592865,
      "end": 119.91999816894531,
      "no_speech_prob": 0.012052685022354126,
      "seek": 11432,
      "start": 114.31999969482422,
      "temperature": 0.0,
      "text": " problem. Let's take a look at the output that O3 mini gave us. Same deal. We have the original text,",
      "tokens": [
        50364,
        1154,
        13,
        961,
        311,
        747,
        257,
        574,
        412,
        264,
        5598,
        300,
        422,
        18,
        8382,
        2729,
        505,
        13,
        10635,
        2028,
        13,
        492,
        362,
        264,
        3380,
        2487,
        11,
        50644
      ]
    },
    {
      "id": 22,
      "avg_logprob": -0.16662892699241638,
      "compression_ratio": 1.54450261592865,
      "end": 125.12000274658203,
      "no_speech_prob": 0.012052685022354126,
      "seek": 11432,
      "start": 119.91999816894531,
      "temperature": 0.0,
      "text": " we have the target, there's the prompt for O3 mini, here's the deletion it created. And so if",
      "tokens": [
        50644,
        321,
        362,
        264,
        3779,
        11,
        456,
        311,
        264,
        12391,
        337,
        422,
        18,
        8382,
        11,
        510,
        311,
        264,
        1103,
        302,
        313,
        309,
        2942,
        13,
        400,
        370,
        498,
        50904
      ]
    },
    {
      "id": 23,
      "avg_logprob": -0.16662892699241638,
      "compression_ratio": 1.54450261592865,
      "end": 130.9600067138672,
      "no_speech_prob": 0.012052685022354126,
      "seek": 11432,
      "start": 125.12000274658203,
      "temperature": 0.0,
      "text": " we scroll down here, you can see O3 mini got this problem right with just a prompt. Very cool to see",
      "tokens": [
        50904,
        321,
        11369,
        760,
        510,
        11,
        291,
        393,
        536,
        422,
        18,
        8382,
        658,
        341,
        1154,
        558,
        365,
        445,
        257,
        12391,
        13,
        4372,
        1627,
        281,
        536,
        51196
      ]
    }
  ],
  "language": "english",
  "duration": 131.05999755859375,
  "timestamp": 1742835357.435242
}