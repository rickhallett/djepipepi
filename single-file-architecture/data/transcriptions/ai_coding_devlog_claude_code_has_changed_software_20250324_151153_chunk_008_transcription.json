{
  "audio_path": "data/chunks/ai_coding_devlog_claude_code_has_changed_software_20250324_151153_chunk_008.mp3",
  "text": "of dollars on CloudCode already, that number is only going to go up. When we talk about trade-offs, when we talk about ADA versus CloudCode and CloudCode versus really anything, this tool is going to cost you. This thing is a token gobbler. I know some engineers have talked about Klein and other agentic tools, you know, chewing through tokens. CloudCode is no different. Everything must come through the context window. Okay, so this is fantastic. We just ran that help command and you can see everything looks like it passed properly. CloudCode is reporting success. If we run slash costs, you can see exactly what I'm talking about there. That session, right, those three and really one prompt cost two dollars to run. Okay, so this is not cheap technology, right? We are talking, when you're writing these big powerful spec prompts, we are talking dollars per prompt, right? And this is nowhere near the largest spec prompt I've created. Okay, so you can imagine as you scale this up, as you hand off more of your work, as you create these more powerful precise packages for your AI tooling, those costs, these numbers go up, right? And I do want to quickly mention, this is something we talk about in lesson four of principled AI coding. You want to be spending money to save time. You know, as long as the costs are worth it, I absolutely recommend using a tool like this, using other tools that let you scale up your compute. Right now, we're doing the review process. Okay, so we're reviewing, we're validating, we're making sure that, you know, no matter how great these tools get, it's always good to just take a pass through the code to make sure it looks good. So that's what I'm doing right now. We're walking through our brand new PocketPick MCP server functionality. So it all looks good. And what I'm going to do now is just understand how much impact I just had. So I'm going to remove the log file, stage these current files, and then run git diff head stat. So you can see there, 1600 lines, 1,600 lines of code created from a, I think we have a hundred line prompt.",
  "segments": [
    {
      "id": 0,
      "avg_logprob": -0.23930656909942627,
      "compression_ratio": 1.6832579374313354,
      "end": 6.480000019073486,
      "no_speech_prob": 0.2845074236392975,
      "seek": 0,
      "start": 0.0,
      "temperature": 0.0,
      "text": " of dollars on CloudCode already, that number is only going to go up. When we talk about trade-offs,",
      "tokens": [
        50364,
        295,
        3808,
        322,
        2033,
        1861,
        34,
        1429,
        1217,
        11,
        300,
        1230,
        307,
        787,
        516,
        281,
        352,
        493,
        13,
        1133,
        321,
        751,
        466,
        4923,
        12,
        19231,
        11,
        50688
      ]
    },
    {
      "id": 1,
      "avg_logprob": -0.23930656909942627,
      "compression_ratio": 1.6832579374313354,
      "end": 12.640000343322754,
      "no_speech_prob": 0.2845074236392975,
      "seek": 0,
      "start": 6.480000019073486,
      "temperature": 0.0,
      "text": " when we talk about ADA versus CloudCode and CloudCode versus really anything, this tool",
      "tokens": [
        50688,
        562,
        321,
        751,
        466,
        39354,
        5717,
        8061,
        34,
        1429,
        293,
        8061,
        34,
        1429,
        5717,
        534,
        1340,
        11,
        341,
        2290,
        50996
      ]
    },
    {
      "id": 2,
      "avg_logprob": -0.23930656909942627,
      "compression_ratio": 1.6832579374313354,
      "end": 19.360000610351562,
      "no_speech_prob": 0.2845074236392975,
      "seek": 0,
      "start": 12.640000343322754,
      "temperature": 0.0,
      "text": " is going to cost you. This thing is a token gobbler. I know some engineers have talked about",
      "tokens": [
        50996,
        307,
        516,
        281,
        2063,
        291,
        13,
        639,
        551,
        307,
        257,
        14862,
        352,
        6692,
        1918,
        13,
        286,
        458,
        512,
        11955,
        362,
        2825,
        466,
        51332
      ]
    },
    {
      "id": 3,
      "avg_logprob": -0.23930656909942627,
      "compression_ratio": 1.6832579374313354,
      "end": 25.360000610351562,
      "no_speech_prob": 0.2845074236392975,
      "seek": 0,
      "start": 19.360000610351562,
      "temperature": 0.0,
      "text": " Klein and other agentic tools, you know, chewing through tokens. CloudCode is no different.",
      "tokens": [
        51332,
        33327,
        293,
        661,
        9461,
        299,
        3873,
        11,
        291,
        458,
        11,
        31444,
        807,
        22667,
        13,
        8061,
        34,
        1429,
        307,
        572,
        819,
        13,
        51632
      ]
    },
    {
      "id": 4,
      "avg_logprob": -0.21875,
      "compression_ratio": 1.6468530893325806,
      "end": 30.479999542236328,
      "no_speech_prob": 0.0566377192735672,
      "seek": 2536,
      "start": 25.360000610351562,
      "temperature": 0.0,
      "text": " Everything must come through the context window. Okay, so this is fantastic. We just ran that help",
      "tokens": [
        50364,
        5471,
        1633,
        808,
        807,
        264,
        4319,
        4910,
        13,
        1033,
        11,
        370,
        341,
        307,
        5456,
        13,
        492,
        445,
        5872,
        300,
        854,
        50620
      ]
    },
    {
      "id": 5,
      "avg_logprob": -0.21875,
      "compression_ratio": 1.6468530893325806,
      "end": 37.119998931884766,
      "no_speech_prob": 0.0566377192735672,
      "seek": 2536,
      "start": 30.479999542236328,
      "temperature": 0.0,
      "text": " command and you can see everything looks like it passed properly. CloudCode is reporting success.",
      "tokens": [
        50620,
        5622,
        293,
        291,
        393,
        536,
        1203,
        1542,
        411,
        309,
        4678,
        6108,
        13,
        8061,
        34,
        1429,
        307,
        10031,
        2245,
        13,
        50952
      ]
    },
    {
      "id": 6,
      "avg_logprob": -0.21875,
      "compression_ratio": 1.6468530893325806,
      "end": 41.7599983215332,
      "no_speech_prob": 0.0566377192735672,
      "seek": 2536,
      "start": 37.119998931884766,
      "temperature": 0.0,
      "text": " If we run slash costs, you can see exactly what I'm talking about there. That session,",
      "tokens": [
        50952,
        759,
        321,
        1190,
        17330,
        5497,
        11,
        291,
        393,
        536,
        2293,
        437,
        286,
        478,
        1417,
        466,
        456,
        13,
        663,
        5481,
        11,
        51184
      ]
    },
    {
      "id": 7,
      "avg_logprob": -0.21875,
      "compression_ratio": 1.6468530893325806,
      "end": 49.20000076293945,
      "no_speech_prob": 0.0566377192735672,
      "seek": 2536,
      "start": 41.7599983215332,
      "temperature": 0.0,
      "text": " right, those three and really one prompt cost two dollars to run. Okay, so this is not cheap",
      "tokens": [
        51184,
        558,
        11,
        729,
        1045,
        293,
        534,
        472,
        12391,
        2063,
        732,
        3808,
        281,
        1190,
        13,
        1033,
        11,
        370,
        341,
        307,
        406,
        7084,
        51556
      ]
    },
    {
      "id": 8,
      "avg_logprob": -0.21875,
      "compression_ratio": 1.6468530893325806,
      "end": 53.36000061035156,
      "no_speech_prob": 0.0566377192735672,
      "seek": 2536,
      "start": 49.20000076293945,
      "temperature": 0.0,
      "text": " technology, right? We are talking, when you're writing these big powerful spec prompts, we are",
      "tokens": [
        51556,
        2899,
        11,
        558,
        30,
        492,
        366,
        1417,
        11,
        562,
        291,
        434,
        3579,
        613,
        955,
        4005,
        1608,
        41095,
        11,
        321,
        366,
        51764
      ]
    },
    {
      "id": 9,
      "avg_logprob": -0.2169797122478485,
      "compression_ratio": 1.6008583307266235,
      "end": 60.400001525878906,
      "no_speech_prob": 0.00036258227191865444,
      "seek": 5336,
      "start": 53.36000061035156,
      "temperature": 0.0,
      "text": " talking dollars per prompt, right? And this is nowhere near the largest spec prompt I've created.",
      "tokens": [
        50364,
        1417,
        3808,
        680,
        12391,
        11,
        558,
        30,
        400,
        341,
        307,
        11159,
        2651,
        264,
        6443,
        1608,
        12391,
        286,
        600,
        2942,
        13,
        50716
      ]
    },
    {
      "id": 10,
      "avg_logprob": -0.2169797122478485,
      "compression_ratio": 1.6008583307266235,
      "end": 64.95999908447266,
      "no_speech_prob": 0.00036258227191865444,
      "seek": 5336,
      "start": 60.400001525878906,
      "temperature": 0.0,
      "text": " Okay, so you can imagine as you scale this up, as you hand off more of your work, as you create",
      "tokens": [
        50716,
        1033,
        11,
        370,
        291,
        393,
        3811,
        382,
        291,
        4373,
        341,
        493,
        11,
        382,
        291,
        1011,
        766,
        544,
        295,
        428,
        589,
        11,
        382,
        291,
        1884,
        50944
      ]
    },
    {
      "id": 11,
      "avg_logprob": -0.2169797122478485,
      "compression_ratio": 1.6008583307266235,
      "end": 71.44000244140625,
      "no_speech_prob": 0.00036258227191865444,
      "seek": 5336,
      "start": 64.95999908447266,
      "temperature": 0.0,
      "text": " these more powerful precise packages for your AI tooling, those costs, these numbers go up, right?",
      "tokens": [
        50944,
        613,
        544,
        4005,
        13600,
        17401,
        337,
        428,
        7318,
        46593,
        11,
        729,
        5497,
        11,
        613,
        3547,
        352,
        493,
        11,
        558,
        30,
        51268
      ]
    },
    {
      "id": 12,
      "avg_logprob": -0.2169797122478485,
      "compression_ratio": 1.6008583307266235,
      "end": 76.23999786376953,
      "no_speech_prob": 0.00036258227191865444,
      "seek": 5336,
      "start": 72.55999755859375,
      "temperature": 0.0,
      "text": " And I do want to quickly mention, this is something we talk about in lesson four",
      "tokens": [
        51324,
        400,
        286,
        360,
        528,
        281,
        2661,
        2152,
        11,
        341,
        307,
        746,
        321,
        751,
        466,
        294,
        6898,
        1451,
        51508
      ]
    },
    {
      "id": 13,
      "avg_logprob": -0.1838838905096054,
      "compression_ratio": 1.675276756286621,
      "end": 81.27999877929688,
      "no_speech_prob": 0.04603176936507225,
      "seek": 7624,
      "start": 76.23999786376953,
      "temperature": 0.0,
      "text": " of principled AI coding. You want to be spending money to save time.",
      "tokens": [
        50364,
        295,
        3681,
        15551,
        7318,
        17720,
        13,
        509,
        528,
        281,
        312,
        6434,
        1460,
        281,
        3155,
        565,
        13,
        50616
      ]
    },
    {
      "id": 14,
      "avg_logprob": -0.1838838905096054,
      "compression_ratio": 1.675276756286621,
      "end": 88.23999786376953,
      "no_speech_prob": 0.04603176936507225,
      "seek": 7624,
      "start": 83.19999694824219,
      "temperature": 0.0,
      "text": " You know, as long as the costs are worth it, I absolutely recommend using a tool like this,",
      "tokens": [
        50712,
        509,
        458,
        11,
        382,
        938,
        382,
        264,
        5497,
        366,
        3163,
        309,
        11,
        286,
        3122,
        2748,
        1228,
        257,
        2290,
        411,
        341,
        11,
        50964
      ]
    },
    {
      "id": 15,
      "avg_logprob": -0.1838838905096054,
      "compression_ratio": 1.675276756286621,
      "end": 93.04000091552734,
      "no_speech_prob": 0.04603176936507225,
      "seek": 7624,
      "start": 88.23999786376953,
      "temperature": 0.0,
      "text": " using other tools that let you scale up your compute. Right now, we're doing the review process.",
      "tokens": [
        50964,
        1228,
        661,
        3873,
        300,
        718,
        291,
        4373,
        493,
        428,
        14722,
        13,
        1779,
        586,
        11,
        321,
        434,
        884,
        264,
        3131,
        1399,
        13,
        51204
      ]
    },
    {
      "id": 16,
      "avg_logprob": -0.1838838905096054,
      "compression_ratio": 1.675276756286621,
      "end": 98.87999725341797,
      "no_speech_prob": 0.04603176936507225,
      "seek": 7624,
      "start": 93.04000091552734,
      "temperature": 0.0,
      "text": " Okay, so we're reviewing, we're validating, we're making sure that, you know, no matter how great",
      "tokens": [
        51204,
        1033,
        11,
        370,
        321,
        434,
        19576,
        11,
        321,
        434,
        7363,
        990,
        11,
        321,
        434,
        1455,
        988,
        300,
        11,
        291,
        458,
        11,
        572,
        1871,
        577,
        869,
        51496
      ]
    },
    {
      "id": 17,
      "avg_logprob": -0.1838838905096054,
      "compression_ratio": 1.675276756286621,
      "end": 103.44000244140625,
      "no_speech_prob": 0.04603176936507225,
      "seek": 7624,
      "start": 98.87999725341797,
      "temperature": 0.0,
      "text": " these tools get, it's always good to just take a pass through the code to make sure it looks good.",
      "tokens": [
        51496,
        613,
        3873,
        483,
        11,
        309,
        311,
        1009,
        665,
        281,
        445,
        747,
        257,
        1320,
        807,
        264,
        3089,
        281,
        652,
        988,
        309,
        1542,
        665,
        13,
        51724
      ]
    },
    {
      "id": 18,
      "avg_logprob": -0.2405698150396347,
      "compression_ratio": 1.5606060028076172,
      "end": 106.72000122070312,
      "no_speech_prob": 0.0005792671581730247,
      "seek": 10344,
      "start": 103.44000244140625,
      "temperature": 0.0,
      "text": " So that's what I'm doing right now. We're walking through our brand new PocketPick",
      "tokens": [
        50364,
        407,
        300,
        311,
        437,
        286,
        478,
        884,
        558,
        586,
        13,
        492,
        434,
        4494,
        807,
        527,
        3360,
        777,
        44594,
        47,
        618,
        50528
      ]
    },
    {
      "id": 19,
      "avg_logprob": -0.2405698150396347,
      "compression_ratio": 1.5606060028076172,
      "end": 112.16000366210938,
      "no_speech_prob": 0.0005792671581730247,
      "seek": 10344,
      "start": 106.72000122070312,
      "temperature": 0.0,
      "text": " MCP server functionality. So it all looks good. And what I'm going to do now is just understand",
      "tokens": [
        50528,
        8797,
        47,
        7154,
        14980,
        13,
        407,
        309,
        439,
        1542,
        665,
        13,
        400,
        437,
        286,
        478,
        516,
        281,
        360,
        586,
        307,
        445,
        1223,
        50800
      ]
    },
    {
      "id": 20,
      "avg_logprob": -0.2405698150396347,
      "compression_ratio": 1.5606060028076172,
      "end": 118.72000122070312,
      "no_speech_prob": 0.0005792671581730247,
      "seek": 10344,
      "start": 112.16000366210938,
      "temperature": 0.0,
      "text": " how much impact I just had. So I'm going to remove the log file, stage these current files,",
      "tokens": [
        50800,
        577,
        709,
        2712,
        286,
        445,
        632,
        13,
        407,
        286,
        478,
        516,
        281,
        4159,
        264,
        3565,
        3991,
        11,
        3233,
        613,
        2190,
        7098,
        11,
        51128
      ]
    },
    {
      "id": 21,
      "avg_logprob": -0.2405698150396347,
      "compression_ratio": 1.5606060028076172,
      "end": 126.0,
      "no_speech_prob": 0.0005792671581730247,
      "seek": 10344,
      "start": 118.72000122070312,
      "temperature": 0.0,
      "text": " and then run git diff head stat. So you can see there, 1600 lines, 1,600 lines of code created",
      "tokens": [
        51128,
        293,
        550,
        1190,
        18331,
        7593,
        1378,
        2219,
        13,
        407,
        291,
        393,
        536,
        456,
        11,
        36885,
        3876,
        11,
        502,
        11,
        15707,
        3876,
        295,
        3089,
        2942,
        51492
      ]
    },
    {
      "id": 22,
      "avg_logprob": -0.2405698150396347,
      "compression_ratio": 1.5606060028076172,
      "end": 131.0399932861328,
      "no_speech_prob": 0.0005792671581730247,
      "seek": 10344,
      "start": 126.63999938964844,
      "temperature": 0.0,
      "text": " from a, I think we have a hundred line prompt.",
      "tokens": [
        51524,
        490,
        257,
        11,
        286,
        519,
        321,
        362,
        257,
        3262,
        1622,
        12391,
        13,
        51744
      ]
    }
  ],
  "language": "english",
  "duration": 131.05999755859375,
  "timestamp": 1742834792.985166
}