{
  "audio_path": "data/chunks/my_top_6_claude_code_pro_tips_for_ai_coding_mcp_20250324_151153_chunk_002.mp3",
  "text": "reads with this cool sub-agent call tool. All these items are now getting added to cloud codes, context window at application startup. You can see here, it's giving us a nice summary, but most importantly, it now has this information inside of its context window. If we type slash cost, you can see there, of course, this is not a cheap tool, right? You're going to have to pay to play with cloud code, but you can see here, you know, we've done some operations already. We have 40 cents worth of work that is now context, AKA active memory for cloud code. This lets you set up your AI coding assistant to win right away by downloading the essential elements of your code base. I highly recommend you add something like this to your read me so that you can get started quickly when you have to reset your cloud code instances. When you're nearing that 200 K context token window limit, as you'll see here in a moment, when we run an AI coding prompt, this small tweak speeds up your work quite a bit. If we run get Alice files, you know, we can now see a general structure of the code base. I also like to run a tree sometimes and pass that into cloud. And so as you can see here, we are operating in the single file agents code base. This is something we've worked on on the channel. All the work we're doing here is going to be available to you link in the description. We're using UV to build up these powerful single file agents. Again, we're going to look at these in just a moment here. That's tip number one. Let's move on to tip number two. Context is King. This is not new. If you've been working in the AI coding space, and if you've been working with language models, you always want to be feeding cloud code and your AI coding tools, the context it needs to get the job done. If we type slash MCP here, you can see I have five MCP servers in this current project. The winning engineers in the gen AI age will always be thinking from the perspective of their AI agents. What can they see if someone were to prompt me with this context and this tool, would I be able to accomplish?",
  "segments": [
    {
      "id": 0,
      "avg_logprob": -0.2708410918712616,
      "compression_ratio": 1.7090909481048584,
      "end": 3.359999895095825,
      "no_speech_prob": 0.0726279690861702,
      "seek": 0,
      "start": 0.0,
      "temperature": 0.0,
      "text": " reads with this cool sub-agent call tool.",
      "tokens": [
        50364,
        15700,
        365,
        341,
        1627,
        1422,
        12,
        559,
        317,
        818,
        2290,
        13,
        50532
      ]
    },
    {
      "id": 1,
      "avg_logprob": -0.2708410918712616,
      "compression_ratio": 1.7090909481048584,
      "end": 6.440000057220459,
      "no_speech_prob": 0.0726279690861702,
      "seek": 0,
      "start": 3.4000000953674316,
      "temperature": 0.0,
      "text": " All these items are now getting added to cloud codes,",
      "tokens": [
        50534,
        1057,
        613,
        4754,
        366,
        586,
        1242,
        3869,
        281,
        4588,
        14211,
        11,
        50686
      ]
    },
    {
      "id": 2,
      "avg_logprob": -0.2708410918712616,
      "compression_ratio": 1.7090909481048584,
      "end": 9.199999809265137,
      "no_speech_prob": 0.0726279690861702,
      "seek": 0,
      "start": 6.480000019073486,
      "temperature": 0.0,
      "text": " context window at application startup.",
      "tokens": [
        50688,
        4319,
        4910,
        412,
        3861,
        18578,
        13,
        50824
      ]
    },
    {
      "id": 3,
      "avg_logprob": -0.2708410918712616,
      "compression_ratio": 1.7090909481048584,
      "end": 13.760000228881836,
      "no_speech_prob": 0.0726279690861702,
      "seek": 0,
      "start": 9.199999809265137,
      "temperature": 0.0,
      "text": " You can see here, it's giving us a nice summary, but most importantly,",
      "tokens": [
        50824,
        509,
        393,
        536,
        510,
        11,
        309,
        311,
        2902,
        505,
        257,
        1481,
        12691,
        11,
        457,
        881,
        8906,
        11,
        51052
      ]
    },
    {
      "id": 4,
      "avg_logprob": -0.2708410918712616,
      "compression_ratio": 1.7090909481048584,
      "end": 18.239999771118164,
      "no_speech_prob": 0.0726279690861702,
      "seek": 0,
      "start": 13.899999618530273,
      "temperature": 0.0,
      "text": " it now has this information inside of its context window.",
      "tokens": [
        51059,
        309,
        586,
        575,
        341,
        1589,
        1854,
        295,
        1080,
        4319,
        4910,
        13,
        51276
      ]
    },
    {
      "id": 5,
      "avg_logprob": -0.2708410918712616,
      "compression_ratio": 1.7090909481048584,
      "end": 23.1200008392334,
      "no_speech_prob": 0.0726279690861702,
      "seek": 0,
      "start": 18.239999771118164,
      "temperature": 0.0,
      "text": " If we type slash cost, you can see there, of course, this is not a cheap tool,",
      "tokens": [
        51276,
        759,
        321,
        2010,
        17330,
        2063,
        11,
        291,
        393,
        536,
        456,
        11,
        295,
        1164,
        11,
        341,
        307,
        406,
        257,
        7084,
        2290,
        11,
        51520
      ]
    },
    {
      "id": 6,
      "avg_logprob": -0.2708410918712616,
      "compression_ratio": 1.7090909481048584,
      "end": 26.959999084472656,
      "no_speech_prob": 0.0726279690861702,
      "seek": 0,
      "start": 23.1200008392334,
      "temperature": 0.0,
      "text": " right? You're going to have to pay to play with cloud code, but you can see here,",
      "tokens": [
        51520,
        558,
        30,
        509,
        434,
        516,
        281,
        362,
        281,
        1689,
        281,
        862,
        365,
        4588,
        3089,
        11,
        457,
        291,
        393,
        536,
        510,
        11,
        51712
      ]
    },
    {
      "id": 7,
      "avg_logprob": -0.2708410918712616,
      "compression_ratio": 1.7090909481048584,
      "end": 29.200000762939453,
      "no_speech_prob": 0.0726279690861702,
      "seek": 0,
      "start": 27.0,
      "temperature": 0.0,
      "text": " you know, we've done some operations already.",
      "tokens": [
        51714,
        291,
        458,
        11,
        321,
        600,
        1096,
        512,
        7705,
        1217,
        13,
        51824
      ]
    },
    {
      "id": 8,
      "avg_logprob": -0.2682304084300995,
      "compression_ratio": 1.598455548286438,
      "end": 33.400001525878906,
      "no_speech_prob": 0.0006771882181055844,
      "seek": 2920,
      "start": 29.360000610351562,
      "temperature": 0.0,
      "text": " We have 40 cents worth of work that is now context,",
      "tokens": [
        50372,
        492,
        362,
        3356,
        14941,
        3163,
        295,
        589,
        300,
        307,
        586,
        4319,
        11,
        50574
      ]
    },
    {
      "id": 9,
      "avg_logprob": -0.2682304084300995,
      "compression_ratio": 1.598455548286438,
      "end": 36.84000015258789,
      "no_speech_prob": 0.0006771882181055844,
      "seek": 2920,
      "start": 33.47999954223633,
      "temperature": 0.0,
      "text": " AKA active memory for cloud code.",
      "tokens": [
        50578,
        45933,
        4967,
        4675,
        337,
        4588,
        3089,
        13,
        50746
      ]
    },
    {
      "id": 10,
      "avg_logprob": -0.2682304084300995,
      "compression_ratio": 1.598455548286438,
      "end": 41.560001373291016,
      "no_speech_prob": 0.0006771882181055844,
      "seek": 2920,
      "start": 36.91999816894531,
      "temperature": 0.0,
      "text": " This lets you set up your AI coding assistant to win right away by",
      "tokens": [
        50750,
        639,
        6653,
        291,
        992,
        493,
        428,
        7318,
        17720,
        10994,
        281,
        1942,
        558,
        1314,
        538,
        50982
      ]
    },
    {
      "id": 11,
      "avg_logprob": -0.2682304084300995,
      "compression_ratio": 1.598455548286438,
      "end": 44.91999816894531,
      "no_speech_prob": 0.0006771882181055844,
      "seek": 2920,
      "start": 41.560001373291016,
      "temperature": 0.0,
      "text": " downloading the essential elements of your code base.",
      "tokens": [
        50982,
        32529,
        264,
        7115,
        4959,
        295,
        428,
        3089,
        3096,
        13,
        51150
      ]
    },
    {
      "id": 12,
      "avg_logprob": -0.2682304084300995,
      "compression_ratio": 1.598455548286438,
      "end": 48.279998779296875,
      "no_speech_prob": 0.0006771882181055844,
      "seek": 2920,
      "start": 45.0,
      "temperature": 0.0,
      "text": " I highly recommend you add something like this to your read me so that you can",
      "tokens": [
        51154,
        286,
        5405,
        2748,
        291,
        909,
        746,
        411,
        341,
        281,
        428,
        1401,
        385,
        370,
        300,
        291,
        393,
        51318
      ]
    },
    {
      "id": 13,
      "avg_logprob": -0.2682304084300995,
      "compression_ratio": 1.598455548286438,
      "end": 52.36000061035156,
      "no_speech_prob": 0.0006771882181055844,
      "seek": 2920,
      "start": 48.279998779296875,
      "temperature": 0.0,
      "text": " get started quickly when you have to reset your cloud code instances.",
      "tokens": [
        51318,
        483,
        1409,
        2661,
        562,
        291,
        362,
        281,
        14322,
        428,
        4588,
        3089,
        14519,
        13,
        51522
      ]
    },
    {
      "id": 14,
      "avg_logprob": -0.2682304084300995,
      "compression_ratio": 1.598455548286438,
      "end": 57.119998931884766,
      "no_speech_prob": 0.0006771882181055844,
      "seek": 2920,
      "start": 52.560001373291016,
      "temperature": 0.0,
      "text": " When you're nearing that 200 K context token window limit,",
      "tokens": [
        51532,
        1133,
        291,
        434,
        408,
        1921,
        300,
        2331,
        591,
        4319,
        14862,
        4910,
        4948,
        11,
        51760
      ]
    },
    {
      "id": 15,
      "avg_logprob": -0.22534312307834625,
      "compression_ratio": 1.697841763496399,
      "end": 60.91999816894531,
      "no_speech_prob": 0.0004512050363700837,
      "seek": 5712,
      "start": 57.2400016784668,
      "temperature": 0.0,
      "text": " as you'll see here in a moment, when we run an AI coding prompt,",
      "tokens": [
        50370,
        382,
        291,
        603,
        536,
        510,
        294,
        257,
        1623,
        11,
        562,
        321,
        1190,
        364,
        7318,
        17720,
        12391,
        11,
        50554
      ]
    },
    {
      "id": 16,
      "avg_logprob": -0.22534312307834625,
      "compression_ratio": 1.697841763496399,
      "end": 65.23999786376953,
      "no_speech_prob": 0.0004512050363700837,
      "seek": 5712,
      "start": 61.0,
      "temperature": 0.0,
      "text": " this small tweak speeds up your work quite a bit. If we run get Alice files,",
      "tokens": [
        50558,
        341,
        1359,
        29879,
        16411,
        493,
        428,
        589,
        1596,
        257,
        857,
        13,
        759,
        321,
        1190,
        483,
        16004,
        7098,
        11,
        50770
      ]
    },
    {
      "id": 17,
      "avg_logprob": -0.22534312307834625,
      "compression_ratio": 1.697841763496399,
      "end": 68.91999816894531,
      "no_speech_prob": 0.0004512050363700837,
      "seek": 5712,
      "start": 65.44000244140625,
      "temperature": 0.0,
      "text": " you know, we can now see a general structure of the code base.",
      "tokens": [
        50780,
        291,
        458,
        11,
        321,
        393,
        586,
        536,
        257,
        2674,
        3877,
        295,
        264,
        3089,
        3096,
        13,
        50954
      ]
    },
    {
      "id": 18,
      "avg_logprob": -0.22534312307834625,
      "compression_ratio": 1.697841763496399,
      "end": 73.16000366210938,
      "no_speech_prob": 0.0004512050363700837,
      "seek": 5712,
      "start": 69.16000366210938,
      "temperature": 0.0,
      "text": " I also like to run a tree sometimes and pass that into cloud.",
      "tokens": [
        50966,
        286,
        611,
        411,
        281,
        1190,
        257,
        4230,
        2171,
        293,
        1320,
        300,
        666,
        4588,
        13,
        51166
      ]
    },
    {
      "id": 19,
      "avg_logprob": -0.22534312307834625,
      "compression_ratio": 1.697841763496399,
      "end": 78.12000274658203,
      "no_speech_prob": 0.0004512050363700837,
      "seek": 5712,
      "start": 73.19999694824219,
      "temperature": 0.0,
      "text": " And so as you can see here, we are operating in the single file agents code base.",
      "tokens": [
        51168,
        400,
        370,
        382,
        291,
        393,
        536,
        510,
        11,
        321,
        366,
        7447,
        294,
        264,
        2167,
        3991,
        12554,
        3089,
        3096,
        13,
        51414
      ]
    },
    {
      "id": 20,
      "avg_logprob": -0.22534312307834625,
      "compression_ratio": 1.697841763496399,
      "end": 80.5999984741211,
      "no_speech_prob": 0.0004512050363700837,
      "seek": 5712,
      "start": 78.12000274658203,
      "temperature": 0.0,
      "text": " This is something we've worked on on the channel.",
      "tokens": [
        51414,
        639,
        307,
        746,
        321,
        600,
        2732,
        322,
        322,
        264,
        2269,
        13,
        51538
      ]
    },
    {
      "id": 21,
      "avg_logprob": -0.22534312307834625,
      "compression_ratio": 1.697841763496399,
      "end": 84.27999877929688,
      "no_speech_prob": 0.0004512050363700837,
      "seek": 5712,
      "start": 80.63999938964844,
      "temperature": 0.0,
      "text": " All the work we're doing here is going to be available to you link in the",
      "tokens": [
        51540,
        1057,
        264,
        589,
        321,
        434,
        884,
        510,
        307,
        516,
        281,
        312,
        2435,
        281,
        291,
        2113,
        294,
        264,
        51722
      ]
    },
    {
      "id": 22,
      "avg_logprob": -0.22741647064685822,
      "compression_ratio": 1.6525096893310547,
      "end": 84.95999908447266,
      "no_speech_prob": 0.11436038464307785,
      "seek": 8428,
      "start": 84.31999969482422,
      "temperature": 0.0,
      "text": " description.",
      "tokens": [
        50366,
        3855,
        13,
        50398
      ]
    },
    {
      "id": 23,
      "avg_logprob": -0.22741647064685822,
      "compression_ratio": 1.6525096893310547,
      "end": 89.83999633789062,
      "no_speech_prob": 0.11436038464307785,
      "seek": 8428,
      "start": 84.95999908447266,
      "temperature": 0.0,
      "text": " We're using UV to build up these powerful single file agents. Again,",
      "tokens": [
        50398,
        492,
        434,
        1228,
        17887,
        281,
        1322,
        493,
        613,
        4005,
        2167,
        3991,
        12554,
        13,
        3764,
        11,
        50642
      ]
    },
    {
      "id": 24,
      "avg_logprob": -0.22741647064685822,
      "compression_ratio": 1.6525096893310547,
      "end": 93.44000244140625,
      "no_speech_prob": 0.11436038464307785,
      "seek": 8428,
      "start": 89.87999725341797,
      "temperature": 0.0,
      "text": " we're going to look at these in just a moment here. That's tip number one.",
      "tokens": [
        50644,
        321,
        434,
        516,
        281,
        574,
        412,
        613,
        294,
        445,
        257,
        1623,
        510,
        13,
        663,
        311,
        4125,
        1230,
        472,
        13,
        50822
      ]
    },
    {
      "id": 25,
      "avg_logprob": -0.22741647064685822,
      "compression_ratio": 1.6525096893310547,
      "end": 97.55999755859375,
      "no_speech_prob": 0.11436038464307785,
      "seek": 8428,
      "start": 93.4800033569336,
      "temperature": 0.0,
      "text": " Let's move on to tip number two. Context is King.",
      "tokens": [
        50824,
        961,
        311,
        1286,
        322,
        281,
        4125,
        1230,
        732,
        13,
        4839,
        3828,
        307,
        3819,
        13,
        51028
      ]
    },
    {
      "id": 26,
      "avg_logprob": -0.22741647064685822,
      "compression_ratio": 1.6525096893310547,
      "end": 101.72000122070312,
      "no_speech_prob": 0.11436038464307785,
      "seek": 8428,
      "start": 97.63999938964844,
      "temperature": 0.0,
      "text": " This is not new. If you've been working in the AI coding space,",
      "tokens": [
        51032,
        639,
        307,
        406,
        777,
        13,
        759,
        291,
        600,
        668,
        1364,
        294,
        264,
        7318,
        17720,
        1901,
        11,
        51236
      ]
    },
    {
      "id": 27,
      "avg_logprob": -0.22741647064685822,
      "compression_ratio": 1.6525096893310547,
      "end": 103.83999633789062,
      "no_speech_prob": 0.11436038464307785,
      "seek": 8428,
      "start": 101.87999725341797,
      "temperature": 0.0,
      "text": " and if you've been working with language models,",
      "tokens": [
        51244,
        293,
        498,
        291,
        600,
        668,
        1364,
        365,
        2856,
        5245,
        11,
        51342
      ]
    },
    {
      "id": 28,
      "avg_logprob": -0.22741647064685822,
      "compression_ratio": 1.6525096893310547,
      "end": 107.12000274658203,
      "no_speech_prob": 0.11436038464307785,
      "seek": 8428,
      "start": 103.87999725341797,
      "temperature": 0.0,
      "text": " you always want to be feeding cloud code and your AI coding tools,",
      "tokens": [
        51344,
        291,
        1009,
        528,
        281,
        312,
        12919,
        4588,
        3089,
        293,
        428,
        7318,
        17720,
        3873,
        11,
        51506
      ]
    },
    {
      "id": 29,
      "avg_logprob": -0.22741647064685822,
      "compression_ratio": 1.6525096893310547,
      "end": 110.87999725341797,
      "no_speech_prob": 0.11436038464307785,
      "seek": 8428,
      "start": 107.12000274658203,
      "temperature": 0.0,
      "text": " the context it needs to get the job done.",
      "tokens": [
        51506,
        264,
        4319,
        309,
        2203,
        281,
        483,
        264,
        1691,
        1096,
        13,
        51694
      ]
    },
    {
      "id": 30,
      "avg_logprob": -0.2512071132659912,
      "compression_ratio": 1.4852941036224365,
      "end": 113.16000366210938,
      "no_speech_prob": 0.19924476742744446,
      "seek": 11088,
      "start": 110.95999908447266,
      "temperature": 0.0,
      "text": " If we type slash MCP here,",
      "tokens": [
        50368,
        759,
        321,
        2010,
        17330,
        8797,
        47,
        510,
        11,
        50478
      ]
    },
    {
      "id": 31,
      "avg_logprob": -0.2512071132659912,
      "compression_ratio": 1.4852941036224365,
      "end": 117.87999725341797,
      "no_speech_prob": 0.19924476742744446,
      "seek": 11088,
      "start": 113.36000061035156,
      "temperature": 0.0,
      "text": " you can see I have five MCP servers in this current project.",
      "tokens": [
        50488,
        291,
        393,
        536,
        286,
        362,
        1732,
        8797,
        47,
        15909,
        294,
        341,
        2190,
        1716,
        13,
        50714
      ]
    },
    {
      "id": 32,
      "avg_logprob": -0.2512071132659912,
      "compression_ratio": 1.4852941036224365,
      "end": 122.55999755859375,
      "no_speech_prob": 0.19924476742744446,
      "seek": 11088,
      "start": 117.95999908447266,
      "temperature": 0.0,
      "text": " The winning engineers in the gen AI age will always be thinking from the",
      "tokens": [
        50718,
        440,
        8224,
        11955,
        294,
        264,
        1049,
        7318,
        3205,
        486,
        1009,
        312,
        1953,
        490,
        264,
        50948
      ]
    },
    {
      "id": 33,
      "avg_logprob": -0.2512071132659912,
      "compression_ratio": 1.4852941036224365,
      "end": 124.72000122070312,
      "no_speech_prob": 0.19924476742744446,
      "seek": 11088,
      "start": 122.55999755859375,
      "temperature": 0.0,
      "text": " perspective of their AI agents.",
      "tokens": [
        50948,
        4585,
        295,
        641,
        7318,
        12554,
        13,
        51056
      ]
    },
    {
      "id": 34,
      "avg_logprob": -0.2512071132659912,
      "compression_ratio": 1.4852941036224365,
      "end": 129.27999877929688,
      "no_speech_prob": 0.19924476742744446,
      "seek": 11088,
      "start": 124.72000122070312,
      "temperature": 0.0,
      "text": " What can they see if someone were to prompt me with this context and this",
      "tokens": [
        51056,
        708,
        393,
        436,
        536,
        498,
        1580,
        645,
        281,
        12391,
        385,
        365,
        341,
        4319,
        293,
        341,
        51284
      ]
    },
    {
      "id": 35,
      "avg_logprob": -0.2512071132659912,
      "compression_ratio": 1.4852941036224365,
      "end": 131.16000366210938,
      "no_speech_prob": 0.19924476742744446,
      "seek": 11088,
      "start": 129.27999877929688,
      "temperature": 0.0,
      "text": " tool, would I be able to accomplish?",
      "tokens": [
        51284,
        2290,
        11,
        576,
        286,
        312,
        1075,
        281,
        9021,
        30,
        51378
      ]
    }
  ],
  "language": "english",
  "duration": 131.05999755859375,
  "timestamp": 1742834577.864653
}