{
  "audio_path": "data/chunks/single_file_agents_python_scripts_with_astral_uv_20250324_151153_chunk_004.mp3",
  "text": "in all of our arguments. We have our dash d for database, dash p for prompt, dash c for compute. We have our open AI key check here. We're then making our database path global so that every single function can access it. We're building our prompt and then we're running the main agentic loop. So this is where all the magic happens. This is where we give our AI agent full control to run tools, run functions, gather feedback, build up its context, and solve the problem we want solved. Let's open this up. So we're doing our compute loop check here and then as you can imagine we're running our intelligence. So we're using o3mini. You can see here we're requiring a tool call. With every loop we want our LLM to execute a tool. We're then looking for tool calls. Once we find a tool which we should always have, we are running our process of determining which tool to select. You can build any type of structure, any type of map you want to handle this. I'm just doing this with the simplest pattern possible here. We just have a bunch of if statements that parse the arguments and then call the function. We then take the result and we pass it back in to the language model. We pass it back into our context window. Very importantly here we also have an exception. So if something goes wrong inside of the tool, we're also passing this back to the model. We want our agent to just solve the problem. We want to design a great prompt. We want to design great tools and then just hand off the problem to our agent. We go off. We go on about our day. We go, you know, solve other specific problems. And then in parallel it's solving this problem that we gave it really well thanks to our clean agent design. So this is one agent pattern that I'm working with. But I want to point out something that's really powerful here. You saw the list tables, describe, sample. But we also have this run test SQL query and run final SQL query. From the tech ecosphere you may have picked up on this idea of verifiable domains and closed loop systems. This is something that we talk about in principled AI coding specifically for the AI coding domain. But it really applies to all agentic technology. We can do something interesting here with this run test SQL query. Let's go ahead and open up the arguments. And you can see here it looks the",
  "segments": [
    {
      "id": 0,
      "avg_logprob": -0.20756246149539948,
      "compression_ratio": 1.790697693824768,
      "end": 5.519999980926514,
      "no_speech_prob": 0.04336019232869148,
      "seek": 0,
      "start": 0.0,
      "temperature": 0.0,
      "text": " in all of our arguments. We have our dash d for database, dash p for prompt, dash c for compute.",
      "tokens": [
        50364,
        294,
        439,
        295,
        527,
        12869,
        13,
        492,
        362,
        527,
        8240,
        274,
        337,
        8149,
        11,
        8240,
        280,
        337,
        12391,
        11,
        8240,
        269,
        337,
        14722,
        13,
        50640
      ]
    },
    {
      "id": 1,
      "avg_logprob": -0.20756246149539948,
      "compression_ratio": 1.790697693824768,
      "end": 11.039999961853027,
      "no_speech_prob": 0.04336019232869148,
      "seek": 0,
      "start": 5.519999980926514,
      "temperature": 0.0,
      "text": " We have our open AI key check here. We're then making our database path global so that every",
      "tokens": [
        50640,
        492,
        362,
        527,
        1269,
        7318,
        2141,
        1520,
        510,
        13,
        492,
        434,
        550,
        1455,
        527,
        8149,
        3100,
        4338,
        370,
        300,
        633,
        50916
      ]
    },
    {
      "id": 2,
      "avg_logprob": -0.20756246149539948,
      "compression_ratio": 1.790697693824768,
      "end": 15.680000305175781,
      "no_speech_prob": 0.04336019232869148,
      "seek": 0,
      "start": 11.039999961853027,
      "temperature": 0.0,
      "text": " single function can access it. We're building our prompt and then we're running the main",
      "tokens": [
        50916,
        2167,
        2445,
        393,
        2105,
        309,
        13,
        492,
        434,
        2390,
        527,
        12391,
        293,
        550,
        321,
        434,
        2614,
        264,
        2135,
        51148
      ]
    },
    {
      "id": 3,
      "avg_logprob": -0.20756246149539948,
      "compression_ratio": 1.790697693824768,
      "end": 19.360000610351562,
      "no_speech_prob": 0.04336019232869148,
      "seek": 0,
      "start": 15.680000305175781,
      "temperature": 0.0,
      "text": " agentic loop. So this is where all the magic happens. This is where we give our AI agent",
      "tokens": [
        51148,
        9461,
        299,
        6367,
        13,
        407,
        341,
        307,
        689,
        439,
        264,
        5585,
        2314,
        13,
        639,
        307,
        689,
        321,
        976,
        527,
        7318,
        9461,
        51332
      ]
    },
    {
      "id": 4,
      "avg_logprob": -0.20756246149539948,
      "compression_ratio": 1.790697693824768,
      "end": 26.079999923706055,
      "no_speech_prob": 0.04336019232869148,
      "seek": 0,
      "start": 19.360000610351562,
      "temperature": 0.0,
      "text": " full control to run tools, run functions, gather feedback, build up its context, and solve the",
      "tokens": [
        51332,
        1577,
        1969,
        281,
        1190,
        3873,
        11,
        1190,
        6828,
        11,
        5448,
        5824,
        11,
        1322,
        493,
        1080,
        4319,
        11,
        293,
        5039,
        264,
        51668
      ]
    },
    {
      "id": 5,
      "avg_logprob": -0.1890609711408615,
      "compression_ratio": 1.7413127422332764,
      "end": 30.079999923706055,
      "no_speech_prob": 0.2658589780330658,
      "seek": 2608,
      "start": 26.15999984741211,
      "temperature": 0.0,
      "text": " problem we want solved. Let's open this up. So we're doing our compute loop check here",
      "tokens": [
        50368,
        1154,
        321,
        528,
        13041,
        13,
        961,
        311,
        1269,
        341,
        493,
        13,
        407,
        321,
        434,
        884,
        527,
        14722,
        6367,
        1520,
        510,
        50564
      ]
    },
    {
      "id": 6,
      "avg_logprob": -0.1890609711408615,
      "compression_ratio": 1.7413127422332764,
      "end": 34.47999954223633,
      "no_speech_prob": 0.2658589780330658,
      "seek": 2608,
      "start": 30.639999389648438,
      "temperature": 0.0,
      "text": " and then as you can imagine we're running our intelligence. So we're using",
      "tokens": [
        50592,
        293,
        550,
        382,
        291,
        393,
        3811,
        321,
        434,
        2614,
        527,
        7599,
        13,
        407,
        321,
        434,
        1228,
        50784
      ]
    },
    {
      "id": 7,
      "avg_logprob": -0.1890609711408615,
      "compression_ratio": 1.7413127422332764,
      "end": 41.119998931884766,
      "no_speech_prob": 0.2658589780330658,
      "seek": 2608,
      "start": 34.47999954223633,
      "temperature": 0.0,
      "text": " o3mini. You can see here we're requiring a tool call. With every loop we want our LLM to execute",
      "tokens": [
        50784,
        277,
        18,
        2367,
        72,
        13,
        509,
        393,
        536,
        510,
        321,
        434,
        24165,
        257,
        2290,
        818,
        13,
        2022,
        633,
        6367,
        321,
        528,
        527,
        441,
        43,
        44,
        281,
        14483,
        51116
      ]
    },
    {
      "id": 8,
      "avg_logprob": -0.1890609711408615,
      "compression_ratio": 1.7413127422332764,
      "end": 46.880001068115234,
      "no_speech_prob": 0.2658589780330658,
      "seek": 2608,
      "start": 41.119998931884766,
      "temperature": 0.0,
      "text": " a tool. We're then looking for tool calls. Once we find a tool which we should always have,",
      "tokens": [
        51116,
        257,
        2290,
        13,
        492,
        434,
        550,
        1237,
        337,
        2290,
        5498,
        13,
        3443,
        321,
        915,
        257,
        2290,
        597,
        321,
        820,
        1009,
        362,
        11,
        51404
      ]
    },
    {
      "id": 9,
      "avg_logprob": -0.1890609711408615,
      "compression_ratio": 1.7413127422332764,
      "end": 52.720001220703125,
      "no_speech_prob": 0.2658589780330658,
      "seek": 2608,
      "start": 46.880001068115234,
      "temperature": 0.0,
      "text": " we are running our process of determining which tool to select. You can build any type of structure,",
      "tokens": [
        51404,
        321,
        366,
        2614,
        527,
        1399,
        295,
        23751,
        597,
        2290,
        281,
        3048,
        13,
        509,
        393,
        1322,
        604,
        2010,
        295,
        3877,
        11,
        51696
      ]
    },
    {
      "id": 10,
      "avg_logprob": -0.19474637508392334,
      "compression_ratio": 1.8827362060546875,
      "end": 56.31999969482422,
      "no_speech_prob": 0.024052763357758522,
      "seek": 5272,
      "start": 52.720001220703125,
      "temperature": 0.0,
      "text": " any type of map you want to handle this. I'm just doing this with the simplest pattern possible",
      "tokens": [
        50364,
        604,
        2010,
        295,
        4471,
        291,
        528,
        281,
        4813,
        341,
        13,
        286,
        478,
        445,
        884,
        341,
        365,
        264,
        22811,
        5102,
        1944,
        50544
      ]
    },
    {
      "id": 11,
      "avg_logprob": -0.19474637508392334,
      "compression_ratio": 1.8827362060546875,
      "end": 61.36000061035156,
      "no_speech_prob": 0.024052763357758522,
      "seek": 5272,
      "start": 56.31999969482422,
      "temperature": 0.0,
      "text": " here. We just have a bunch of if statements that parse the arguments and then call the function.",
      "tokens": [
        50544,
        510,
        13,
        492,
        445,
        362,
        257,
        3840,
        295,
        498,
        12363,
        300,
        48377,
        264,
        12869,
        293,
        550,
        818,
        264,
        2445,
        13,
        50796
      ]
    },
    {
      "id": 12,
      "avg_logprob": -0.19474637508392334,
      "compression_ratio": 1.8827362060546875,
      "end": 66.55999755859375,
      "no_speech_prob": 0.024052763357758522,
      "seek": 5272,
      "start": 61.36000061035156,
      "temperature": 0.0,
      "text": " We then take the result and we pass it back in to the language model. We pass it back into our",
      "tokens": [
        50796,
        492,
        550,
        747,
        264,
        1874,
        293,
        321,
        1320,
        309,
        646,
        294,
        281,
        264,
        2856,
        2316,
        13,
        492,
        1320,
        309,
        646,
        666,
        527,
        51056
      ]
    },
    {
      "id": 13,
      "avg_logprob": -0.19474637508392334,
      "compression_ratio": 1.8827362060546875,
      "end": 72.55999755859375,
      "no_speech_prob": 0.024052763357758522,
      "seek": 5272,
      "start": 66.55999755859375,
      "temperature": 0.0,
      "text": " context window. Very importantly here we also have an exception. So if something goes wrong inside of",
      "tokens": [
        51056,
        4319,
        4910,
        13,
        4372,
        8906,
        510,
        321,
        611,
        362,
        364,
        11183,
        13,
        407,
        498,
        746,
        1709,
        2085,
        1854,
        295,
        51356
      ]
    },
    {
      "id": 14,
      "avg_logprob": -0.19474637508392334,
      "compression_ratio": 1.8827362060546875,
      "end": 78.23999786376953,
      "no_speech_prob": 0.024052763357758522,
      "seek": 5272,
      "start": 72.55999755859375,
      "temperature": 0.0,
      "text": " the tool, we're also passing this back to the model. We want our agent to just solve the problem.",
      "tokens": [
        51356,
        264,
        2290,
        11,
        321,
        434,
        611,
        8437,
        341,
        646,
        281,
        264,
        2316,
        13,
        492,
        528,
        527,
        9461,
        281,
        445,
        5039,
        264,
        1154,
        13,
        51640
      ]
    },
    {
      "id": 15,
      "avg_logprob": -0.19474637508392334,
      "compression_ratio": 1.8827362060546875,
      "end": 81.91999816894531,
      "no_speech_prob": 0.024052763357758522,
      "seek": 5272,
      "start": 78.23999786376953,
      "temperature": 0.0,
      "text": " We want to design a great prompt. We want to design great tools and then just hand off the",
      "tokens": [
        51640,
        492,
        528,
        281,
        1715,
        257,
        869,
        12391,
        13,
        492,
        528,
        281,
        1715,
        869,
        3873,
        293,
        550,
        445,
        1011,
        766,
        264,
        51824
      ]
    },
    {
      "id": 16,
      "avg_logprob": -0.22590042650699615,
      "compression_ratio": 1.6514084339141846,
      "end": 86.63999938964844,
      "no_speech_prob": 0.00108170579187572,
      "seek": 8192,
      "start": 81.91999816894531,
      "temperature": 0.0,
      "text": " problem to our agent. We go off. We go on about our day. We go, you know, solve other specific",
      "tokens": [
        50364,
        1154,
        281,
        527,
        9461,
        13,
        492,
        352,
        766,
        13,
        492,
        352,
        322,
        466,
        527,
        786,
        13,
        492,
        352,
        11,
        291,
        458,
        11,
        5039,
        661,
        2685,
        50600
      ]
    },
    {
      "id": 17,
      "avg_logprob": -0.22590042650699615,
      "compression_ratio": 1.6514084339141846,
      "end": 91.27999877929688,
      "no_speech_prob": 0.00108170579187572,
      "seek": 8192,
      "start": 86.63999938964844,
      "temperature": 0.0,
      "text": " problems. And then in parallel it's solving this problem that we gave it really well",
      "tokens": [
        50600,
        2740,
        13,
        400,
        550,
        294,
        8952,
        309,
        311,
        12606,
        341,
        1154,
        300,
        321,
        2729,
        309,
        534,
        731,
        50832
      ]
    },
    {
      "id": 18,
      "avg_logprob": -0.22590042650699615,
      "compression_ratio": 1.6514084339141846,
      "end": 97.36000061035156,
      "no_speech_prob": 0.00108170579187572,
      "seek": 8192,
      "start": 91.27999877929688,
      "temperature": 0.0,
      "text": " thanks to our clean agent design. So this is one agent pattern that I'm working with. But I want",
      "tokens": [
        50832,
        3231,
        281,
        527,
        2541,
        9461,
        1715,
        13,
        407,
        341,
        307,
        472,
        9461,
        5102,
        300,
        286,
        478,
        1364,
        365,
        13,
        583,
        286,
        528,
        51136
      ]
    },
    {
      "id": 19,
      "avg_logprob": -0.22590042650699615,
      "compression_ratio": 1.6514084339141846,
      "end": 102.16000366210938,
      "no_speech_prob": 0.00108170579187572,
      "seek": 8192,
      "start": 97.36000061035156,
      "temperature": 0.0,
      "text": " to point out something that's really powerful here. You saw the list tables, describe, sample.",
      "tokens": [
        51136,
        281,
        935,
        484,
        746,
        300,
        311,
        534,
        4005,
        510,
        13,
        509,
        1866,
        264,
        1329,
        8020,
        11,
        6786,
        11,
        6889,
        13,
        51376
      ]
    },
    {
      "id": 20,
      "avg_logprob": -0.22590042650699615,
      "compression_ratio": 1.6514084339141846,
      "end": 108.95999908447266,
      "no_speech_prob": 0.00108170579187572,
      "seek": 8192,
      "start": 102.16000366210938,
      "temperature": 0.0,
      "text": " But we also have this run test SQL query and run final SQL query. From the tech ecosphere you may",
      "tokens": [
        51376,
        583,
        321,
        611,
        362,
        341,
        1190,
        1500,
        19200,
        14581,
        293,
        1190,
        2572,
        19200,
        14581,
        13,
        3358,
        264,
        7553,
        11007,
        6605,
        291,
        815,
        51716
      ]
    },
    {
      "id": 21,
      "avg_logprob": -0.19306829571723938,
      "compression_ratio": 1.5708154439926147,
      "end": 115.76000213623047,
      "no_speech_prob": 0.0008830161532387137,
      "seek": 10896,
      "start": 108.95999908447266,
      "temperature": 0.0,
      "text": " have picked up on this idea of verifiable domains and closed loop systems. This is something that",
      "tokens": [
        50364,
        362,
        6183,
        493,
        322,
        341,
        1558,
        295,
        1306,
        30876,
        25514,
        293,
        5395,
        6367,
        3652,
        13,
        639,
        307,
        746,
        300,
        50704
      ]
    },
    {
      "id": 22,
      "avg_logprob": -0.19306829571723938,
      "compression_ratio": 1.5708154439926147,
      "end": 121.27999877929688,
      "no_speech_prob": 0.0008830161532387137,
      "seek": 10896,
      "start": 115.76000213623047,
      "temperature": 0.0,
      "text": " we talk about in principled AI coding specifically for the AI coding domain. But it really applies",
      "tokens": [
        50704,
        321,
        751,
        466,
        294,
        3681,
        15551,
        7318,
        17720,
        4682,
        337,
        264,
        7318,
        17720,
        9274,
        13,
        583,
        309,
        534,
        13165,
        50980
      ]
    },
    {
      "id": 23,
      "avg_logprob": -0.19306829571723938,
      "compression_ratio": 1.5708154439926147,
      "end": 126.4800033569336,
      "no_speech_prob": 0.0008830161532387137,
      "seek": 10896,
      "start": 121.27999877929688,
      "temperature": 0.0,
      "text": " to all agentic technology. We can do something interesting here with this run test SQL query.",
      "tokens": [
        50980,
        281,
        439,
        9461,
        299,
        2899,
        13,
        492,
        393,
        360,
        746,
        1880,
        510,
        365,
        341,
        1190,
        1500,
        19200,
        14581,
        13,
        51240
      ]
    },
    {
      "id": 24,
      "avg_logprob": -0.19306829571723938,
      "compression_ratio": 1.5708154439926147,
      "end": 130.8000030517578,
      "no_speech_prob": 0.0008830161532387137,
      "seek": 10896,
      "start": 126.4800033569336,
      "temperature": 0.0,
      "text": " Let's go ahead and open up the arguments. And you can see here it looks the",
      "tokens": [
        51240,
        961,
        311,
        352,
        2286,
        293,
        1269,
        493,
        264,
        12869,
        13,
        400,
        291,
        393,
        536,
        510,
        309,
        1542,
        264,
        51456
      ]
    }
  ],
  "language": "english",
  "duration": 131.05999755859375,
  "timestamp": 1742834940.802575
}