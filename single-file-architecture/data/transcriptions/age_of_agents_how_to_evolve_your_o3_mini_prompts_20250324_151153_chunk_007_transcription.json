{
  "audio_path": "data/chunks/age_of_agents_how_to_evolve_your_o3_mini_prompts_20250324_151153_chunk_007.mp3",
  "text": "looks like with the edits made from the deletions. Okay. So there's that nice JSON object there. And then you can see here, here's the original, here's what our prompt chain output for us. And here's the target text. So you can see, we of course have the correct answer. This is really important because for this simple use case of effectively determining which words to remove that don't make sense, you can see here that having a little bit more compute, you know, quite literally, it just needed one additional run. Right. So we had one deletion and then we had another deletion. And then on our third compute loop, it said, there's nothing left for me to do here. I'm done by turning our prompt into a prompt chain, quite literally just saying, run it again. It was able to generate an improved answer for us. So, you know, that's the log and you can see here we have three prompts, right? So one prompt for our first run. So we have this first prompt here, and then we have our second prompt and then our third prompt. And it was in our third prompt where the prompt chain, you know, the language model returned no deletions. So that means we're done. Okay. Let's go ahead and just take a look at the prompt for a moment here. And you'll see a very similar structure, except for the primary difference that we have an original slice and current deletions. Okay. So in addition to the iteration slice, which is the small piece of the transcript that we're working on currently, we also have the current deletions. So when our model is looping, it needs to know what it's already edited and it needs to know the starting point. Okay. So this is where it started. This is the current deletions that it's made. And then the iteration slice is the result of those deletions on the original slice. So let me just show you the second prompt. You can see here, current deletions is empty. And in this version, our current deletions has some content, right? So in the second prompt, we have made edits, right? And you can see here, this corresponds to the edits we saw exactly in the log, the prompt chain, it allows us to keep track of the state, the original slice, the work that it's done and the work that it's doing that's iteration two. And then of course we have the third iteration. You can see here, same deal, except we have an additional current deletion, right? So we have two deletions in here. We can go ahead.",
  "segments": [
    {
      "id": 0,
      "avg_logprob": -0.2798928916454315,
      "compression_ratio": 1.6881028413772583,
      "end": 2.6600000858306885,
      "no_speech_prob": 0.017710955813527107,
      "seek": 0,
      "start": 0.0,
      "temperature": 0.0,
      "text": " looks like with the edits made from the deletions.",
      "tokens": [
        50364,
        1542,
        411,
        365,
        264,
        41752,
        1027,
        490,
        264,
        1103,
        302,
        626,
        13,
        50497
      ]
    },
    {
      "id": 1,
      "avg_logprob": -0.2798928916454315,
      "compression_ratio": 1.6881028413772583,
      "end": 2.859999895095825,
      "no_speech_prob": 0.017710955813527107,
      "seek": 0,
      "start": 2.6600000858306885,
      "temperature": 0.0,
      "text": " Okay.",
      "tokens": [
        50497,
        1033,
        13,
        50507
      ]
    },
    {
      "id": 2,
      "avg_logprob": -0.2798928916454315,
      "compression_ratio": 1.6881028413772583,
      "end": 4.920000076293945,
      "no_speech_prob": 0.017710955813527107,
      "seek": 0,
      "start": 2.859999895095825,
      "temperature": 0.0,
      "text": " So there's that nice JSON object there.",
      "tokens": [
        50507,
        407,
        456,
        311,
        300,
        1481,
        31828,
        2657,
        456,
        13,
        50610
      ]
    },
    {
      "id": 3,
      "avg_logprob": -0.2798928916454315,
      "compression_ratio": 1.6881028413772583,
      "end": 7.5,
      "no_speech_prob": 0.017710955813527107,
      "seek": 0,
      "start": 5.019999980926514,
      "temperature": 0.0,
      "text": " And then you can see here, here's the original, here's what our",
      "tokens": [
        50615,
        400,
        550,
        291,
        393,
        536,
        510,
        11,
        510,
        311,
        264,
        3380,
        11,
        510,
        311,
        437,
        527,
        50739
      ]
    },
    {
      "id": 4,
      "avg_logprob": -0.2798928916454315,
      "compression_ratio": 1.6881028413772583,
      "end": 9.260000228881836,
      "no_speech_prob": 0.017710955813527107,
      "seek": 0,
      "start": 7.5,
      "temperature": 0.0,
      "text": " prompt chain output for us.",
      "tokens": [
        50739,
        12391,
        5021,
        5598,
        337,
        505,
        13,
        50827
      ]
    },
    {
      "id": 5,
      "avg_logprob": -0.2798928916454315,
      "compression_ratio": 1.6881028413772583,
      "end": 10.319999694824219,
      "no_speech_prob": 0.017710955813527107,
      "seek": 0,
      "start": 9.359999656677246,
      "temperature": 0.0,
      "text": " And here's the target text.",
      "tokens": [
        50832,
        400,
        510,
        311,
        264,
        3779,
        2487,
        13,
        50880
      ]
    },
    {
      "id": 6,
      "avg_logprob": -0.2798928916454315,
      "compression_ratio": 1.6881028413772583,
      "end": 12.859999656677246,
      "no_speech_prob": 0.017710955813527107,
      "seek": 0,
      "start": 10.319999694824219,
      "temperature": 0.0,
      "text": " So you can see, we of course have the correct answer.",
      "tokens": [
        50880,
        407,
        291,
        393,
        536,
        11,
        321,
        295,
        1164,
        362,
        264,
        3006,
        1867,
        13,
        51007
      ]
    },
    {
      "id": 7,
      "avg_logprob": -0.2798928916454315,
      "compression_ratio": 1.6881028413772583,
      "end": 18.139999389648438,
      "no_speech_prob": 0.017710955813527107,
      "seek": 0,
      "start": 12.979999542236328,
      "temperature": 0.0,
      "text": " This is really important because for this simple use case of effectively",
      "tokens": [
        51013,
        639,
        307,
        534,
        1021,
        570,
        337,
        341,
        2199,
        764,
        1389,
        295,
        8659,
        51271
      ]
    },
    {
      "id": 8,
      "avg_logprob": -0.2798928916454315,
      "compression_ratio": 1.6881028413772583,
      "end": 22.440000534057617,
      "no_speech_prob": 0.017710955813527107,
      "seek": 0,
      "start": 18.239999771118164,
      "temperature": 0.0,
      "text": " determining which words to remove that don't make sense, you can see here",
      "tokens": [
        51276,
        23751,
        597,
        2283,
        281,
        4159,
        300,
        500,
        380,
        652,
        2020,
        11,
        291,
        393,
        536,
        510,
        51486
      ]
    },
    {
      "id": 9,
      "avg_logprob": -0.2798928916454315,
      "compression_ratio": 1.6881028413772583,
      "end": 25.18000030517578,
      "no_speech_prob": 0.017710955813527107,
      "seek": 0,
      "start": 22.440000534057617,
      "temperature": 0.0,
      "text": " that having a little bit more compute, you know, quite literally, it",
      "tokens": [
        51486,
        300,
        1419,
        257,
        707,
        857,
        544,
        14722,
        11,
        291,
        458,
        11,
        1596,
        3736,
        11,
        309,
        51623
      ]
    },
    {
      "id": 10,
      "avg_logprob": -0.2798928916454315,
      "compression_ratio": 1.6881028413772583,
      "end": 27.360000610351562,
      "no_speech_prob": 0.017710955813527107,
      "seek": 0,
      "start": 25.18000030517578,
      "temperature": 0.0,
      "text": " just needed one additional run.",
      "tokens": [
        51623,
        445,
        2978,
        472,
        4497,
        1190,
        13,
        51732
      ]
    },
    {
      "id": 11,
      "avg_logprob": -0.2798928916454315,
      "compression_ratio": 1.6881028413772583,
      "end": 27.65999984741211,
      "no_speech_prob": 0.017710955813527107,
      "seek": 0,
      "start": 27.420000076293945,
      "temperature": 0.0,
      "text": " Right.",
      "tokens": [
        51735,
        1779,
        13,
        51747
      ]
    },
    {
      "id": 12,
      "avg_logprob": -0.24428415298461914,
      "compression_ratio": 1.8272058963775635,
      "end": 30.559999465942383,
      "no_speech_prob": 0.00027802796103060246,
      "seek": 2766,
      "start": 27.65999984741211,
      "temperature": 0.0,
      "text": " So we had one deletion and then we had another deletion.",
      "tokens": [
        50364,
        407,
        321,
        632,
        472,
        1103,
        302,
        313,
        293,
        550,
        321,
        632,
        1071,
        1103,
        302,
        313,
        13,
        50509
      ]
    },
    {
      "id": 13,
      "avg_logprob": -0.24428415298461914,
      "compression_ratio": 1.8272058963775635,
      "end": 34.20000076293945,
      "no_speech_prob": 0.00027802796103060246,
      "seek": 2766,
      "start": 30.600000381469727,
      "temperature": 0.0,
      "text": " And then on our third compute loop, it said, there's nothing",
      "tokens": [
        50511,
        400,
        550,
        322,
        527,
        2636,
        14722,
        6367,
        11,
        309,
        848,
        11,
        456,
        311,
        1825,
        50691
      ]
    },
    {
      "id": 14,
      "avg_logprob": -0.24428415298461914,
      "compression_ratio": 1.8272058963775635,
      "end": 34.959999084472656,
      "no_speech_prob": 0.00027802796103060246,
      "seek": 2766,
      "start": 34.20000076293945,
      "temperature": 0.0,
      "text": " left for me to do here.",
      "tokens": [
        50691,
        1411,
        337,
        385,
        281,
        360,
        510,
        13,
        50729
      ]
    },
    {
      "id": 15,
      "avg_logprob": -0.24428415298461914,
      "compression_ratio": 1.8272058963775635,
      "end": 38.15999984741211,
      "no_speech_prob": 0.00027802796103060246,
      "seek": 2766,
      "start": 34.959999084472656,
      "temperature": 0.0,
      "text": " I'm done by turning our prompt into a prompt chain, quite literally",
      "tokens": [
        50729,
        286,
        478,
        1096,
        538,
        6246,
        527,
        12391,
        666,
        257,
        12391,
        5021,
        11,
        1596,
        3736,
        50889
      ]
    },
    {
      "id": 16,
      "avg_logprob": -0.24428415298461914,
      "compression_ratio": 1.8272058963775635,
      "end": 39.84000015258789,
      "no_speech_prob": 0.00027802796103060246,
      "seek": 2766,
      "start": 38.15999984741211,
      "temperature": 0.0,
      "text": " just saying, run it again.",
      "tokens": [
        50889,
        445,
        1566,
        11,
        1190,
        309,
        797,
        13,
        50973
      ]
    },
    {
      "id": 17,
      "avg_logprob": -0.24428415298461914,
      "compression_ratio": 1.8272058963775635,
      "end": 42.36000061035156,
      "no_speech_prob": 0.00027802796103060246,
      "seek": 2766,
      "start": 39.91999816894531,
      "temperature": 0.0,
      "text": " It was able to generate an improved answer for us.",
      "tokens": [
        50977,
        467,
        390,
        1075,
        281,
        8460,
        364,
        9689,
        1867,
        337,
        505,
        13,
        51099
      ]
    },
    {
      "id": 18,
      "avg_logprob": -0.24428415298461914,
      "compression_ratio": 1.8272058963775635,
      "end": 47.040000915527344,
      "no_speech_prob": 0.00027802796103060246,
      "seek": 2766,
      "start": 42.36000061035156,
      "temperature": 0.0,
      "text": " So, you know, that's the log and you can see here we have three prompts, right?",
      "tokens": [
        51099,
        407,
        11,
        291,
        458,
        11,
        300,
        311,
        264,
        3565,
        293,
        291,
        393,
        536,
        510,
        321,
        362,
        1045,
        41095,
        11,
        558,
        30,
        51333
      ]
    },
    {
      "id": 19,
      "avg_logprob": -0.24428415298461914,
      "compression_ratio": 1.8272058963775635,
      "end": 49.560001373291016,
      "no_speech_prob": 0.00027802796103060246,
      "seek": 2766,
      "start": 47.15999984741211,
      "temperature": 0.0,
      "text": " So one prompt for our first run.",
      "tokens": [
        51339,
        407,
        472,
        12391,
        337,
        527,
        700,
        1190,
        13,
        51459
      ]
    },
    {
      "id": 20,
      "avg_logprob": -0.24428415298461914,
      "compression_ratio": 1.8272058963775635,
      "end": 52.2400016784668,
      "no_speech_prob": 0.00027802796103060246,
      "seek": 2766,
      "start": 49.619998931884766,
      "temperature": 0.0,
      "text": " So we have this first prompt here, and then we have our second",
      "tokens": [
        51462,
        407,
        321,
        362,
        341,
        700,
        12391,
        510,
        11,
        293,
        550,
        321,
        362,
        527,
        1150,
        51593
      ]
    },
    {
      "id": 21,
      "avg_logprob": -0.24428415298461914,
      "compression_ratio": 1.8272058963775635,
      "end": 54.20000076293945,
      "no_speech_prob": 0.00027802796103060246,
      "seek": 2766,
      "start": 52.2400016784668,
      "temperature": 0.0,
      "text": " prompt and then our third prompt.",
      "tokens": [
        51593,
        12391,
        293,
        550,
        527,
        2636,
        12391,
        13,
        51691
      ]
    },
    {
      "id": 22,
      "avg_logprob": -0.25094693899154663,
      "compression_ratio": 1.7420494556427002,
      "end": 58.18000030517578,
      "no_speech_prob": 0.00034062567283399403,
      "seek": 5420,
      "start": 54.220001220703125,
      "temperature": 0.0,
      "text": " And it was in our third prompt where the prompt chain, you know, the",
      "tokens": [
        50365,
        400,
        309,
        390,
        294,
        527,
        2636,
        12391,
        689,
        264,
        12391,
        5021,
        11,
        291,
        458,
        11,
        264,
        50563
      ]
    },
    {
      "id": 23,
      "avg_logprob": -0.25094693899154663,
      "compression_ratio": 1.7420494556427002,
      "end": 60.459999084472656,
      "no_speech_prob": 0.00034062567283399403,
      "seek": 5420,
      "start": 58.18000030517578,
      "temperature": 0.0,
      "text": " language model returned no deletions.",
      "tokens": [
        50563,
        2856,
        2316,
        8752,
        572,
        1103,
        302,
        626,
        13,
        50677
      ]
    },
    {
      "id": 24,
      "avg_logprob": -0.25094693899154663,
      "compression_ratio": 1.7420494556427002,
      "end": 61.459999084472656,
      "no_speech_prob": 0.00034062567283399403,
      "seek": 5420,
      "start": 60.459999084472656,
      "temperature": 0.0,
      "text": " So that means we're done.",
      "tokens": [
        50677,
        407,
        300,
        1355,
        321,
        434,
        1096,
        13,
        50727
      ]
    },
    {
      "id": 25,
      "avg_logprob": -0.25094693899154663,
      "compression_ratio": 1.7420494556427002,
      "end": 61.81999969482422,
      "no_speech_prob": 0.00034062567283399403,
      "seek": 5420,
      "start": 61.5,
      "temperature": 0.0,
      "text": " Okay.",
      "tokens": [
        50729,
        1033,
        13,
        50745
      ]
    },
    {
      "id": 26,
      "avg_logprob": -0.25094693899154663,
      "compression_ratio": 1.7420494556427002,
      "end": 64.5,
      "no_speech_prob": 0.00034062567283399403,
      "seek": 5420,
      "start": 61.97999954223633,
      "temperature": 0.0,
      "text": " Let's go ahead and just take a look at the prompt for a moment here.",
      "tokens": [
        50753,
        961,
        311,
        352,
        2286,
        293,
        445,
        747,
        257,
        574,
        412,
        264,
        12391,
        337,
        257,
        1623,
        510,
        13,
        50879
      ]
    },
    {
      "id": 27,
      "avg_logprob": -0.25094693899154663,
      "compression_ratio": 1.7420494556427002,
      "end": 68.37999725341797,
      "no_speech_prob": 0.00034062567283399403,
      "seek": 5420,
      "start": 64.58000183105469,
      "temperature": 0.0,
      "text": " And you'll see a very similar structure, except for the primary difference that",
      "tokens": [
        50883,
        400,
        291,
        603,
        536,
        257,
        588,
        2531,
        3877,
        11,
        3993,
        337,
        264,
        6194,
        2649,
        300,
        51073
      ]
    },
    {
      "id": 28,
      "avg_logprob": -0.25094693899154663,
      "compression_ratio": 1.7420494556427002,
      "end": 71.9000015258789,
      "no_speech_prob": 0.00034062567283399403,
      "seek": 5420,
      "start": 68.45999908447266,
      "temperature": 0.0,
      "text": " we have an original slice and current deletions.",
      "tokens": [
        51077,
        321,
        362,
        364,
        3380,
        13153,
        293,
        2190,
        1103,
        302,
        626,
        13,
        51249
      ]
    },
    {
      "id": 29,
      "avg_logprob": -0.25094693899154663,
      "compression_ratio": 1.7420494556427002,
      "end": 72.37999725341797,
      "no_speech_prob": 0.00034062567283399403,
      "seek": 5420,
      "start": 71.95999908447266,
      "temperature": 0.0,
      "text": " Okay.",
      "tokens": [
        51252,
        1033,
        13,
        51273
      ]
    },
    {
      "id": 30,
      "avg_logprob": -0.25094693899154663,
      "compression_ratio": 1.7420494556427002,
      "end": 77.62000274658203,
      "no_speech_prob": 0.00034062567283399403,
      "seek": 5420,
      "start": 72.54000091552734,
      "temperature": 0.0,
      "text": " So in addition to the iteration slice, which is the small piece of the transcript",
      "tokens": [
        51281,
        407,
        294,
        4500,
        281,
        264,
        24784,
        13153,
        11,
        597,
        307,
        264,
        1359,
        2522,
        295,
        264,
        24444,
        51535
      ]
    },
    {
      "id": 31,
      "avg_logprob": -0.25094693899154663,
      "compression_ratio": 1.7420494556427002,
      "end": 81.13999938964844,
      "no_speech_prob": 0.00034062567283399403,
      "seek": 5420,
      "start": 77.62000274658203,
      "temperature": 0.0,
      "text": " that we're working on currently, we also have the current deletions.",
      "tokens": [
        51535,
        300,
        321,
        434,
        1364,
        322,
        4362,
        11,
        321,
        611,
        362,
        264,
        2190,
        1103,
        302,
        626,
        13,
        51711
      ]
    },
    {
      "id": 32,
      "avg_logprob": -0.24546630680561066,
      "compression_ratio": 1.8803088665008545,
      "end": 85.76000213623047,
      "no_speech_prob": 0.000319995975587517,
      "seek": 8114,
      "start": 81.19999694824219,
      "temperature": 0.0,
      "text": " So when our model is looping, it needs to know what it's already edited and",
      "tokens": [
        50367,
        407,
        562,
        527,
        2316,
        307,
        6367,
        278,
        11,
        309,
        2203,
        281,
        458,
        437,
        309,
        311,
        1217,
        23016,
        293,
        50595
      ]
    },
    {
      "id": 33,
      "avg_logprob": -0.24546630680561066,
      "compression_ratio": 1.8803088665008545,
      "end": 87.72000122070312,
      "no_speech_prob": 0.000319995975587517,
      "seek": 8114,
      "start": 85.76000213623047,
      "temperature": 0.0,
      "text": " it needs to know the starting point.",
      "tokens": [
        50595,
        309,
        2203,
        281,
        458,
        264,
        2891,
        935,
        13,
        50693
      ]
    },
    {
      "id": 34,
      "avg_logprob": -0.24546630680561066,
      "compression_ratio": 1.8803088665008545,
      "end": 88.04000091552734,
      "no_speech_prob": 0.000319995975587517,
      "seek": 8114,
      "start": 87.80000305175781,
      "temperature": 0.0,
      "text": " Okay.",
      "tokens": [
        50697,
        1033,
        13,
        50709
      ]
    },
    {
      "id": 35,
      "avg_logprob": -0.24546630680561066,
      "compression_ratio": 1.8803088665008545,
      "end": 89.16000366210938,
      "no_speech_prob": 0.000319995975587517,
      "seek": 8114,
      "start": 88.04000091552734,
      "temperature": 0.0,
      "text": " So this is where it started.",
      "tokens": [
        50709,
        407,
        341,
        307,
        689,
        309,
        1409,
        13,
        50765
      ]
    },
    {
      "id": 36,
      "avg_logprob": -0.24546630680561066,
      "compression_ratio": 1.8803088665008545,
      "end": 92.19999694824219,
      "no_speech_prob": 0.000319995975587517,
      "seek": 8114,
      "start": 89.19999694824219,
      "temperature": 0.0,
      "text": " This is the current deletions that it's made.",
      "tokens": [
        50767,
        639,
        307,
        264,
        2190,
        1103,
        302,
        626,
        300,
        309,
        311,
        1027,
        13,
        50917
      ]
    },
    {
      "id": 37,
      "avg_logprob": -0.24546630680561066,
      "compression_ratio": 1.8803088665008545,
      "end": 95.80000305175781,
      "no_speech_prob": 0.000319995975587517,
      "seek": 8114,
      "start": 92.36000061035156,
      "temperature": 0.0,
      "text": " And then the iteration slice is the result of those",
      "tokens": [
        50925,
        400,
        550,
        264,
        24784,
        13153,
        307,
        264,
        1874,
        295,
        729,
        51097
      ]
    },
    {
      "id": 38,
      "avg_logprob": -0.24546630680561066,
      "compression_ratio": 1.8803088665008545,
      "end": 98.0,
      "no_speech_prob": 0.000319995975587517,
      "seek": 8114,
      "start": 95.80000305175781,
      "temperature": 0.0,
      "text": " deletions on the original slice.",
      "tokens": [
        51097,
        1103,
        302,
        626,
        322,
        264,
        3380,
        13153,
        13,
        51207
      ]
    },
    {
      "id": 39,
      "avg_logprob": -0.24546630680561066,
      "compression_ratio": 1.8803088665008545,
      "end": 99.5199966430664,
      "no_speech_prob": 0.000319995975587517,
      "seek": 8114,
      "start": 98.0,
      "temperature": 0.0,
      "text": " So let me just show you the second prompt.",
      "tokens": [
        51207,
        407,
        718,
        385,
        445,
        855,
        291,
        264,
        1150,
        12391,
        13,
        51283
      ]
    },
    {
      "id": 40,
      "avg_logprob": -0.24546630680561066,
      "compression_ratio": 1.8803088665008545,
      "end": 101.27999877929688,
      "no_speech_prob": 0.000319995975587517,
      "seek": 8114,
      "start": 99.55999755859375,
      "temperature": 0.0,
      "text": " You can see here, current deletions is empty.",
      "tokens": [
        51285,
        509,
        393,
        536,
        510,
        11,
        2190,
        1103,
        302,
        626,
        307,
        6707,
        13,
        51371
      ]
    },
    {
      "id": 41,
      "avg_logprob": -0.24546630680561066,
      "compression_ratio": 1.8803088665008545,
      "end": 106.16000366210938,
      "no_speech_prob": 0.000319995975587517,
      "seek": 8114,
      "start": 101.5999984741211,
      "temperature": 0.0,
      "text": " And in this version, our current deletions has some content, right?",
      "tokens": [
        51387,
        400,
        294,
        341,
        3037,
        11,
        527,
        2190,
        1103,
        302,
        626,
        575,
        512,
        2701,
        11,
        558,
        30,
        51615
      ]
    },
    {
      "id": 42,
      "avg_logprob": -0.24546630680561066,
      "compression_ratio": 1.8803088665008545,
      "end": 109.36000061035156,
      "no_speech_prob": 0.000319995975587517,
      "seek": 8114,
      "start": 106.5199966430664,
      "temperature": 0.0,
      "text": " So in the second prompt, we have made edits, right?",
      "tokens": [
        51633,
        407,
        294,
        264,
        1150,
        12391,
        11,
        321,
        362,
        1027,
        41752,
        11,
        558,
        30,
        51775
      ]
    },
    {
      "id": 43,
      "avg_logprob": -0.2541390657424927,
      "compression_ratio": 1.7692307233810425,
      "end": 114.30000305175781,
      "no_speech_prob": 0.0009546919609420002,
      "seek": 10936,
      "start": 109.37999725341797,
      "temperature": 0.0,
      "text": " And you can see here, this corresponds to the edits we saw exactly in the log,",
      "tokens": [
        50365,
        400,
        291,
        393,
        536,
        510,
        11,
        341,
        23249,
        281,
        264,
        41752,
        321,
        1866,
        2293,
        294,
        264,
        3565,
        11,
        50611
      ]
    },
    {
      "id": 44,
      "avg_logprob": -0.2541390657424927,
      "compression_ratio": 1.7692307233810425,
      "end": 118.33999633789062,
      "no_speech_prob": 0.0009546919609420002,
      "seek": 10936,
      "start": 114.33999633789062,
      "temperature": 0.0,
      "text": " the prompt chain, it allows us to keep track of the state, the original slice,",
      "tokens": [
        50613,
        264,
        12391,
        5021,
        11,
        309,
        4045,
        505,
        281,
        1066,
        2837,
        295,
        264,
        1785,
        11,
        264,
        3380,
        13153,
        11,
        50813
      ]
    },
    {
      "id": 45,
      "avg_logprob": -0.2541390657424927,
      "compression_ratio": 1.7692307233810425,
      "end": 122.77999877929688,
      "no_speech_prob": 0.0009546919609420002,
      "seek": 10936,
      "start": 118.41999816894531,
      "temperature": 0.0,
      "text": " the work that it's done and the work that it's doing that's iteration two.",
      "tokens": [
        50817,
        264,
        589,
        300,
        309,
        311,
        1096,
        293,
        264,
        589,
        300,
        309,
        311,
        884,
        300,
        311,
        24784,
        732,
        13,
        51035
      ]
    },
    {
      "id": 46,
      "avg_logprob": -0.2541390657424927,
      "compression_ratio": 1.7692307233810425,
      "end": 124.73999786376953,
      "no_speech_prob": 0.0009546919609420002,
      "seek": 10936,
      "start": 122.77999877929688,
      "temperature": 0.0,
      "text": " And then of course we have the third iteration.",
      "tokens": [
        51035,
        400,
        550,
        295,
        1164,
        321,
        362,
        264,
        2636,
        24784,
        13,
        51133
      ]
    },
    {
      "id": 47,
      "avg_logprob": -0.2541390657424927,
      "compression_ratio": 1.7692307233810425,
      "end": 129.33999633789062,
      "no_speech_prob": 0.0009546919609420002,
      "seek": 10936,
      "start": 124.81999969482422,
      "temperature": 0.0,
      "text": " You can see here, same deal, except we have an additional current deletion, right?",
      "tokens": [
        51137,
        509,
        393,
        536,
        510,
        11,
        912,
        2028,
        11,
        3993,
        321,
        362,
        364,
        4497,
        2190,
        1103,
        302,
        313,
        11,
        558,
        30,
        51363
      ]
    },
    {
      "id": 48,
      "avg_logprob": -0.2541390657424927,
      "compression_ratio": 1.7692307233810425,
      "end": 130.5800018310547,
      "no_speech_prob": 0.0009546919609420002,
      "seek": 10936,
      "start": 129.33999633789062,
      "temperature": 0.0,
      "text": " So we have two deletions in here.",
      "tokens": [
        51363,
        407,
        321,
        362,
        732,
        1103,
        302,
        626,
        294,
        510,
        13,
        51425
      ]
    },
    {
      "id": 49,
      "avg_logprob": -0.2541390657424927,
      "compression_ratio": 1.7692307233810425,
      "end": 131.13999938964844,
      "no_speech_prob": 0.0009546919609420002,
      "seek": 10936,
      "start": 130.5800018310547,
      "temperature": 0.0,
      "text": " We can go ahead.",
      "tokens": [
        51425,
        492,
        393,
        352,
        2286,
        13,
        51453
      ]
    }
  ],
  "language": "english",
  "duration": 131.05999755859375,
  "timestamp": 1742835385.2385979
}