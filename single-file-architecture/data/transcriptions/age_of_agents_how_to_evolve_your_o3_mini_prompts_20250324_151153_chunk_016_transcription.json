{
  "audio_path": "data/chunks/age_of_agents_how_to_evolve_your_o3_mini_prompts_20250324_151153_chunk_016.mp3",
  "text": "this agents that just have a bunch of tools, you give them a problem, you give the right context, and then you just say, go solve my problem. You know, under the hood, I think what's really going to happen is that we're going to see tons and tons of agentic workflows, right? Or put simply, prompt chains, right? There are many very interesting, very powerful ways to use prompt chains, right? And Anthropic has detailed a lot of them here. We have routing, we have parallelization. I like to call this the fusion chain. We put a video out on this in the past. And then we have orchestrators. There's definitely a good call for using an orchestrator for a cut, evaluator, optimizer. And then we can go down the line, right? All the way down to the AI agent. And the AI agent workflow is the most interesting because in a certain way, it's the most hands-off. I think there's a lot of value to be created here, but the trick is always benchmarking the performance of your AI agent versus a predefined series of steps that call prompts and tools, AKA prompt chains. A couple of improvements that I'm thinking about making for a cut. It's pretty clear that this super harsh, yes, no response framework that I have here, where we just have correct, true, or false, isn't going to be accurate enough to create comprehensive benchmarks. Rolling out something like Levenstein distance to allow for a five to 10 character difference is probably going to be a big win for improving these benchmarks going forward. You know, another big thing that piggybacks off this is that, you know, video editing, even when you're just editing out transcripts, it's a very subjective experience. Certain editing decisions that a model will make are totally valid and legit. And it might not match up with the exact target, even within the five or 10 characters using the Levenstein distance that could actually work. So I think a more concrete way to improve that is to add more comprehensive examples into the prompts themselves. You know, why is that important? How can that change the outcome? That's super important because, you know, when we add examples to our prompts to, you know, give us an idea about what types of edits we wanted to make, it'll pick up on the taste of our editing and what words we like to keep in and keep out, so on and so forth. So more work to be done on that, adding in examples.",
  "segments": [
    {
      "id": 0,
      "avg_logprob": -0.22714844346046448,
      "compression_ratio": 1.7337461709976196,
      "end": 2.240000009536743,
      "no_speech_prob": 0.13474483788013458,
      "seek": 0,
      "start": 0.0,
      "temperature": 0.0,
      "text": " this agents that just have a bunch of tools,",
      "tokens": [
        50364,
        341,
        12554,
        300,
        445,
        362,
        257,
        3840,
        295,
        3873,
        11,
        50476
      ]
    },
    {
      "id": 1,
      "avg_logprob": -0.22714844346046448,
      "compression_ratio": 1.7337461709976196,
      "end": 4.960000038146973,
      "no_speech_prob": 0.13474483788013458,
      "seek": 0,
      "start": 2.240000009536743,
      "temperature": 0.0,
      "text": " you give them a problem, you give the right context,",
      "tokens": [
        50476,
        291,
        976,
        552,
        257,
        1154,
        11,
        291,
        976,
        264,
        558,
        4319,
        11,
        50612
      ]
    },
    {
      "id": 2,
      "avg_logprob": -0.22714844346046448,
      "compression_ratio": 1.7337461709976196,
      "end": 7.0,
      "no_speech_prob": 0.13474483788013458,
      "seek": 0,
      "start": 4.960000038146973,
      "temperature": 0.0,
      "text": " and then you just say, go solve my problem.",
      "tokens": [
        50612,
        293,
        550,
        291,
        445,
        584,
        11,
        352,
        5039,
        452,
        1154,
        13,
        50714
      ]
    },
    {
      "id": 3,
      "avg_logprob": -0.22714844346046448,
      "compression_ratio": 1.7337461709976196,
      "end": 8.0,
      "no_speech_prob": 0.13474483788013458,
      "seek": 0,
      "start": 7.0,
      "temperature": 0.0,
      "text": " You know, under the hood,",
      "tokens": [
        50714,
        509,
        458,
        11,
        833,
        264,
        13376,
        11,
        50764
      ]
    },
    {
      "id": 4,
      "avg_logprob": -0.22714844346046448,
      "compression_ratio": 1.7337461709976196,
      "end": 10.420000076293945,
      "no_speech_prob": 0.13474483788013458,
      "seek": 0,
      "start": 8.0,
      "temperature": 0.0,
      "text": " I think what's really going to happen",
      "tokens": [
        50764,
        286,
        519,
        437,
        311,
        534,
        516,
        281,
        1051,
        50885
      ]
    },
    {
      "id": 5,
      "avg_logprob": -0.22714844346046448,
      "compression_ratio": 1.7337461709976196,
      "end": 12.380000114440918,
      "no_speech_prob": 0.13474483788013458,
      "seek": 0,
      "start": 10.420000076293945,
      "temperature": 0.0,
      "text": " is that we're going to see tons and tons",
      "tokens": [
        50885,
        307,
        300,
        321,
        434,
        516,
        281,
        536,
        9131,
        293,
        9131,
        50983
      ]
    },
    {
      "id": 6,
      "avg_logprob": -0.22714844346046448,
      "compression_ratio": 1.7337461709976196,
      "end": 14.140000343322754,
      "no_speech_prob": 0.13474483788013458,
      "seek": 0,
      "start": 12.380000114440918,
      "temperature": 0.0,
      "text": " of agentic workflows, right?",
      "tokens": [
        50983,
        295,
        9461,
        299,
        43461,
        11,
        558,
        30,
        51071
      ]
    },
    {
      "id": 7,
      "avg_logprob": -0.22714844346046448,
      "compression_ratio": 1.7337461709976196,
      "end": 16.3799991607666,
      "no_speech_prob": 0.13474483788013458,
      "seek": 0,
      "start": 14.140000343322754,
      "temperature": 0.0,
      "text": " Or put simply, prompt chains, right?",
      "tokens": [
        51071,
        1610,
        829,
        2935,
        11,
        12391,
        12626,
        11,
        558,
        30,
        51183
      ]
    },
    {
      "id": 8,
      "avg_logprob": -0.22714844346046448,
      "compression_ratio": 1.7337461709976196,
      "end": 17.920000076293945,
      "no_speech_prob": 0.13474483788013458,
      "seek": 0,
      "start": 16.3799991607666,
      "temperature": 0.0,
      "text": " There are many very interesting,",
      "tokens": [
        51183,
        821,
        366,
        867,
        588,
        1880,
        11,
        51260
      ]
    },
    {
      "id": 9,
      "avg_logprob": -0.22714844346046448,
      "compression_ratio": 1.7337461709976196,
      "end": 20.940000534057617,
      "no_speech_prob": 0.13474483788013458,
      "seek": 0,
      "start": 17.920000076293945,
      "temperature": 0.0,
      "text": " very powerful ways to use prompt chains, right?",
      "tokens": [
        51260,
        588,
        4005,
        2098,
        281,
        764,
        12391,
        12626,
        11,
        558,
        30,
        51411
      ]
    },
    {
      "id": 10,
      "avg_logprob": -0.22714844346046448,
      "compression_ratio": 1.7337461709976196,
      "end": 23.200000762939453,
      "no_speech_prob": 0.13474483788013458,
      "seek": 0,
      "start": 20.940000534057617,
      "temperature": 0.0,
      "text": " And Anthropic has detailed a lot of them here.",
      "tokens": [
        51411,
        400,
        12727,
        39173,
        575,
        9942,
        257,
        688,
        295,
        552,
        510,
        13,
        51524
      ]
    },
    {
      "id": 11,
      "avg_logprob": -0.22714844346046448,
      "compression_ratio": 1.7337461709976196,
      "end": 25.540000915527344,
      "no_speech_prob": 0.13474483788013458,
      "seek": 0,
      "start": 23.200000762939453,
      "temperature": 0.0,
      "text": " We have routing, we have parallelization.",
      "tokens": [
        51524,
        492,
        362,
        32722,
        11,
        321,
        362,
        8952,
        2144,
        13,
        51641
      ]
    },
    {
      "id": 12,
      "avg_logprob": -0.22714844346046448,
      "compression_ratio": 1.7337461709976196,
      "end": 27.700000762939453,
      "no_speech_prob": 0.13474483788013458,
      "seek": 0,
      "start": 25.540000915527344,
      "temperature": 0.0,
      "text": " I like to call this the fusion chain.",
      "tokens": [
        51641,
        286,
        411,
        281,
        818,
        341,
        264,
        23100,
        5021,
        13,
        51749
      ]
    },
    {
      "id": 13,
      "avg_logprob": -0.22714844346046448,
      "compression_ratio": 1.7337461709976196,
      "end": 29.899999618530273,
      "no_speech_prob": 0.13474483788013458,
      "seek": 0,
      "start": 27.700000762939453,
      "temperature": 0.0,
      "text": " We put a video out on this in the past.",
      "tokens": [
        51749,
        492,
        829,
        257,
        960,
        484,
        322,
        341,
        294,
        264,
        1791,
        13,
        51859
      ]
    },
    {
      "id": 14,
      "avg_logprob": -0.2555289566516876,
      "compression_ratio": 1.7250858545303345,
      "end": 31.700000762939453,
      "no_speech_prob": 0.00012148063979111612,
      "seek": 2990,
      "start": 30.719999313354492,
      "temperature": 0.0,
      "text": " And then we have orchestrators.",
      "tokens": [
        50405,
        400,
        550,
        321,
        362,
        14161,
        34886,
        13,
        50454
      ]
    },
    {
      "id": 15,
      "avg_logprob": -0.2555289566516876,
      "compression_ratio": 1.7250858545303345,
      "end": 34.5,
      "no_speech_prob": 0.00012148063979111612,
      "seek": 2990,
      "start": 31.700000762939453,
      "temperature": 0.0,
      "text": " There's definitely a good call for using an orchestrator",
      "tokens": [
        50454,
        821,
        311,
        2138,
        257,
        665,
        818,
        337,
        1228,
        364,
        14161,
        19802,
        50594
      ]
    },
    {
      "id": 16,
      "avg_logprob": -0.2555289566516876,
      "compression_ratio": 1.7250858545303345,
      "end": 36.599998474121094,
      "no_speech_prob": 0.00012148063979111612,
      "seek": 2990,
      "start": 34.5,
      "temperature": 0.0,
      "text": " for a cut, evaluator, optimizer.",
      "tokens": [
        50594,
        337,
        257,
        1723,
        11,
        6133,
        1639,
        11,
        5028,
        6545,
        13,
        50699
      ]
    },
    {
      "id": 17,
      "avg_logprob": -0.2555289566516876,
      "compression_ratio": 1.7250858545303345,
      "end": 37.7400016784668,
      "no_speech_prob": 0.00012148063979111612,
      "seek": 2990,
      "start": 36.599998474121094,
      "temperature": 0.0,
      "text": " And then we can go down the line, right?",
      "tokens": [
        50699,
        400,
        550,
        321,
        393,
        352,
        760,
        264,
        1622,
        11,
        558,
        30,
        50756
      ]
    },
    {
      "id": 18,
      "avg_logprob": -0.2555289566516876,
      "compression_ratio": 1.7250858545303345,
      "end": 39.439998626708984,
      "no_speech_prob": 0.00012148063979111612,
      "seek": 2990,
      "start": 37.7400016784668,
      "temperature": 0.0,
      "text": " All the way down to the AI agent.",
      "tokens": [
        50756,
        1057,
        264,
        636,
        760,
        281,
        264,
        7318,
        9461,
        13,
        50841
      ]
    },
    {
      "id": 19,
      "avg_logprob": -0.2555289566516876,
      "compression_ratio": 1.7250858545303345,
      "end": 41.41999816894531,
      "no_speech_prob": 0.00012148063979111612,
      "seek": 2990,
      "start": 39.439998626708984,
      "temperature": 0.0,
      "text": " And the AI agent workflow is the most interesting",
      "tokens": [
        50841,
        400,
        264,
        7318,
        9461,
        20993,
        307,
        264,
        881,
        1880,
        50940
      ]
    },
    {
      "id": 20,
      "avg_logprob": -0.2555289566516876,
      "compression_ratio": 1.7250858545303345,
      "end": 43.7400016784668,
      "no_speech_prob": 0.00012148063979111612,
      "seek": 2990,
      "start": 41.41999816894531,
      "temperature": 0.0,
      "text": " because in a certain way, it's the most hands-off.",
      "tokens": [
        50940,
        570,
        294,
        257,
        1629,
        636,
        11,
        309,
        311,
        264,
        881,
        2377,
        12,
        4506,
        13,
        51056
      ]
    },
    {
      "id": 21,
      "avg_logprob": -0.2555289566516876,
      "compression_ratio": 1.7250858545303345,
      "end": 46.18000030517578,
      "no_speech_prob": 0.00012148063979111612,
      "seek": 2990,
      "start": 43.7400016784668,
      "temperature": 0.0,
      "text": " I think there's a lot of value to be created here,",
      "tokens": [
        51056,
        286,
        519,
        456,
        311,
        257,
        688,
        295,
        2158,
        281,
        312,
        2942,
        510,
        11,
        51178
      ]
    },
    {
      "id": 22,
      "avg_logprob": -0.2555289566516876,
      "compression_ratio": 1.7250858545303345,
      "end": 49.7400016784668,
      "no_speech_prob": 0.00012148063979111612,
      "seek": 2990,
      "start": 46.18000030517578,
      "temperature": 0.0,
      "text": " but the trick is always benchmarking the performance",
      "tokens": [
        51178,
        457,
        264,
        4282,
        307,
        1009,
        18927,
        278,
        264,
        3389,
        51356
      ]
    },
    {
      "id": 23,
      "avg_logprob": -0.2555289566516876,
      "compression_ratio": 1.7250858545303345,
      "end": 54.619998931884766,
      "no_speech_prob": 0.00012148063979111612,
      "seek": 2990,
      "start": 49.7400016784668,
      "temperature": 0.0,
      "text": " of your AI agent versus a predefined series of steps",
      "tokens": [
        51356,
        295,
        428,
        7318,
        9461,
        5717,
        257,
        659,
        37716,
        2638,
        295,
        4439,
        51600
      ]
    },
    {
      "id": 24,
      "avg_logprob": -0.2555289566516876,
      "compression_ratio": 1.7250858545303345,
      "end": 57.86000061035156,
      "no_speech_prob": 0.00012148063979111612,
      "seek": 2990,
      "start": 54.619998931884766,
      "temperature": 0.0,
      "text": " that call prompts and tools, AKA prompt chains.",
      "tokens": [
        51600,
        300,
        818,
        41095,
        293,
        3873,
        11,
        45933,
        12391,
        12626,
        13,
        51762
      ]
    },
    {
      "id": 25,
      "avg_logprob": -0.2177339792251587,
      "compression_ratio": 1.6837060451507568,
      "end": 59.459999084472656,
      "no_speech_prob": 0.017985593527555466,
      "seek": 5786,
      "start": 57.86000061035156,
      "temperature": 0.0,
      "text": " A couple of improvements",
      "tokens": [
        50364,
        316,
        1916,
        295,
        13797,
        50444
      ]
    },
    {
      "id": 26,
      "avg_logprob": -0.2177339792251587,
      "compression_ratio": 1.6837060451507568,
      "end": 61.13999938964844,
      "no_speech_prob": 0.017985593527555466,
      "seek": 5786,
      "start": 59.459999084472656,
      "temperature": 0.0,
      "text": " that I'm thinking about making for a cut.",
      "tokens": [
        50444,
        300,
        286,
        478,
        1953,
        466,
        1455,
        337,
        257,
        1723,
        13,
        50528
      ]
    },
    {
      "id": 27,
      "avg_logprob": -0.2177339792251587,
      "compression_ratio": 1.6837060451507568,
      "end": 63.939998626708984,
      "no_speech_prob": 0.017985593527555466,
      "seek": 5786,
      "start": 61.13999938964844,
      "temperature": 0.0,
      "text": " It's pretty clear that this super harsh,",
      "tokens": [
        50528,
        467,
        311,
        1238,
        1850,
        300,
        341,
        1687,
        14897,
        11,
        50668
      ]
    },
    {
      "id": 28,
      "avg_logprob": -0.2177339792251587,
      "compression_ratio": 1.6837060451507568,
      "end": 67.05999755859375,
      "no_speech_prob": 0.017985593527555466,
      "seek": 5786,
      "start": 63.939998626708984,
      "temperature": 0.0,
      "text": " yes, no response framework that I have here,",
      "tokens": [
        50668,
        2086,
        11,
        572,
        4134,
        8388,
        300,
        286,
        362,
        510,
        11,
        50824
      ]
    },
    {
      "id": 29,
      "avg_logprob": -0.2177339792251587,
      "compression_ratio": 1.6837060451507568,
      "end": 69.5,
      "no_speech_prob": 0.017985593527555466,
      "seek": 5786,
      "start": 67.05999755859375,
      "temperature": 0.0,
      "text": " where we just have correct, true, or false,",
      "tokens": [
        50824,
        689,
        321,
        445,
        362,
        3006,
        11,
        2074,
        11,
        420,
        7908,
        11,
        50946
      ]
    },
    {
      "id": 30,
      "avg_logprob": -0.2177339792251587,
      "compression_ratio": 1.6837060451507568,
      "end": 71.05999755859375,
      "no_speech_prob": 0.017985593527555466,
      "seek": 5786,
      "start": 69.5,
      "temperature": 0.0,
      "text": " isn't going to be accurate enough",
      "tokens": [
        50946,
        1943,
        380,
        516,
        281,
        312,
        8559,
        1547,
        51024
      ]
    },
    {
      "id": 31,
      "avg_logprob": -0.2177339792251587,
      "compression_ratio": 1.6837060451507568,
      "end": 72.81999969482422,
      "no_speech_prob": 0.017985593527555466,
      "seek": 5786,
      "start": 71.05999755859375,
      "temperature": 0.0,
      "text": " to create comprehensive benchmarks.",
      "tokens": [
        51024,
        281,
        1884,
        13914,
        43751,
        13,
        51112
      ]
    },
    {
      "id": 32,
      "avg_logprob": -0.2177339792251587,
      "compression_ratio": 1.6837060451507568,
      "end": 74.66000366210938,
      "no_speech_prob": 0.017985593527555466,
      "seek": 5786,
      "start": 72.81999969482422,
      "temperature": 0.0,
      "text": " Rolling out something like Levenstein distance",
      "tokens": [
        51112,
        36457,
        484,
        746,
        411,
        1456,
        553,
        9089,
        4560,
        51204
      ]
    },
    {
      "id": 33,
      "avg_logprob": -0.2177339792251587,
      "compression_ratio": 1.6837060451507568,
      "end": 77.45999908447266,
      "no_speech_prob": 0.017985593527555466,
      "seek": 5786,
      "start": 74.66000366210938,
      "temperature": 0.0,
      "text": " to allow for a five to 10 character difference",
      "tokens": [
        51204,
        281,
        2089,
        337,
        257,
        1732,
        281,
        1266,
        2517,
        2649,
        51344
      ]
    },
    {
      "id": 34,
      "avg_logprob": -0.2177339792251587,
      "compression_ratio": 1.6837060451507568,
      "end": 79.37999725341797,
      "no_speech_prob": 0.017985593527555466,
      "seek": 5786,
      "start": 77.45999908447266,
      "temperature": 0.0,
      "text": " is probably going to be a big win",
      "tokens": [
        51344,
        307,
        1391,
        516,
        281,
        312,
        257,
        955,
        1942,
        51440
      ]
    },
    {
      "id": 35,
      "avg_logprob": -0.2177339792251587,
      "compression_ratio": 1.6837060451507568,
      "end": 82.18000030517578,
      "no_speech_prob": 0.017985593527555466,
      "seek": 5786,
      "start": 79.37999725341797,
      "temperature": 0.0,
      "text": " for improving these benchmarks going forward.",
      "tokens": [
        51440,
        337,
        11470,
        613,
        43751,
        516,
        2128,
        13,
        51580
      ]
    },
    {
      "id": 36,
      "avg_logprob": -0.2177339792251587,
      "compression_ratio": 1.6837060451507568,
      "end": 85.22000122070312,
      "no_speech_prob": 0.017985593527555466,
      "seek": 5786,
      "start": 82.18000030517578,
      "temperature": 0.0,
      "text": " You know, another big thing that piggybacks off this",
      "tokens": [
        51580,
        509,
        458,
        11,
        1071,
        955,
        551,
        300,
        39349,
        17758,
        766,
        341,
        51732
      ]
    },
    {
      "id": 37,
      "avg_logprob": -0.2177339792251587,
      "compression_ratio": 1.6837060451507568,
      "end": 86.87999725341797,
      "no_speech_prob": 0.017985593527555466,
      "seek": 5786,
      "start": 85.22000122070312,
      "temperature": 0.0,
      "text": " is that, you know, video editing,",
      "tokens": [
        51732,
        307,
        300,
        11,
        291,
        458,
        11,
        960,
        10000,
        11,
        51815
      ]
    },
    {
      "id": 38,
      "avg_logprob": -0.19102443754673004,
      "compression_ratio": 1.6172839403152466,
      "end": 89.36000061035156,
      "no_speech_prob": 0.001032222993671894,
      "seek": 8688,
      "start": 86.87999725341797,
      "temperature": 0.0,
      "text": " even when you're just editing out transcripts,",
      "tokens": [
        50364,
        754,
        562,
        291,
        434,
        445,
        10000,
        484,
        24444,
        82,
        11,
        50488
      ]
    },
    {
      "id": 39,
      "avg_logprob": -0.19102443754673004,
      "compression_ratio": 1.6172839403152466,
      "end": 92.0,
      "no_speech_prob": 0.001032222993671894,
      "seek": 8688,
      "start": 89.36000061035156,
      "temperature": 0.0,
      "text": " it's a very subjective experience.",
      "tokens": [
        50488,
        309,
        311,
        257,
        588,
        25972,
        1752,
        13,
        50620
      ]
    },
    {
      "id": 40,
      "avg_logprob": -0.19102443754673004,
      "compression_ratio": 1.6172839403152466,
      "end": 95.58000183105469,
      "no_speech_prob": 0.001032222993671894,
      "seek": 8688,
      "start": 92.0,
      "temperature": 0.0,
      "text": " Certain editing decisions that a model will make",
      "tokens": [
        50620,
        13407,
        10000,
        5327,
        300,
        257,
        2316,
        486,
        652,
        50799
      ]
    },
    {
      "id": 41,
      "avg_logprob": -0.19102443754673004,
      "compression_ratio": 1.6172839403152466,
      "end": 97.16000366210938,
      "no_speech_prob": 0.001032222993671894,
      "seek": 8688,
      "start": 95.58000183105469,
      "temperature": 0.0,
      "text": " are totally valid and legit.",
      "tokens": [
        50799,
        366,
        3879,
        7363,
        293,
        10275,
        13,
        50878
      ]
    },
    {
      "id": 42,
      "avg_logprob": -0.19102443754673004,
      "compression_ratio": 1.6172839403152466,
      "end": 100.0199966430664,
      "no_speech_prob": 0.001032222993671894,
      "seek": 8688,
      "start": 97.16000366210938,
      "temperature": 0.0,
      "text": " And it might not match up with the exact target,",
      "tokens": [
        50878,
        400,
        309,
        1062,
        406,
        2995,
        493,
        365,
        264,
        1900,
        3779,
        11,
        51021
      ]
    },
    {
      "id": 43,
      "avg_logprob": -0.19102443754673004,
      "compression_ratio": 1.6172839403152466,
      "end": 101.87999725341797,
      "no_speech_prob": 0.001032222993671894,
      "seek": 8688,
      "start": 100.0199966430664,
      "temperature": 0.0,
      "text": " even within the five or 10 characters",
      "tokens": [
        51021,
        754,
        1951,
        264,
        1732,
        420,
        1266,
        4342,
        51114
      ]
    },
    {
      "id": 44,
      "avg_logprob": -0.19102443754673004,
      "compression_ratio": 1.6172839403152466,
      "end": 104.4000015258789,
      "no_speech_prob": 0.001032222993671894,
      "seek": 8688,
      "start": 101.87999725341797,
      "temperature": 0.0,
      "text": " using the Levenstein distance that could actually work.",
      "tokens": [
        51114,
        1228,
        264,
        1456,
        553,
        9089,
        4560,
        300,
        727,
        767,
        589,
        13,
        51240
      ]
    },
    {
      "id": 45,
      "avg_logprob": -0.19102443754673004,
      "compression_ratio": 1.6172839403152466,
      "end": 106.55999755859375,
      "no_speech_prob": 0.001032222993671894,
      "seek": 8688,
      "start": 104.4000015258789,
      "temperature": 0.0,
      "text": " So I think a more concrete way to improve that",
      "tokens": [
        51240,
        407,
        286,
        519,
        257,
        544,
        9859,
        636,
        281,
        3470,
        300,
        51348
      ]
    },
    {
      "id": 46,
      "avg_logprob": -0.19102443754673004,
      "compression_ratio": 1.6172839403152466,
      "end": 110.0999984741211,
      "no_speech_prob": 0.001032222993671894,
      "seek": 8688,
      "start": 106.55999755859375,
      "temperature": 0.0,
      "text": " is to add more comprehensive examples",
      "tokens": [
        51348,
        307,
        281,
        909,
        544,
        13914,
        5110,
        51525
      ]
    },
    {
      "id": 47,
      "avg_logprob": -0.19102443754673004,
      "compression_ratio": 1.6172839403152466,
      "end": 112.4800033569336,
      "no_speech_prob": 0.001032222993671894,
      "seek": 8688,
      "start": 110.0999984741211,
      "temperature": 0.0,
      "text": " into the prompts themselves.",
      "tokens": [
        51525,
        666,
        264,
        41095,
        2969,
        13,
        51644
      ]
    },
    {
      "id": 48,
      "avg_logprob": -0.19102443754673004,
      "compression_ratio": 1.6172839403152466,
      "end": 113.63999938964844,
      "no_speech_prob": 0.001032222993671894,
      "seek": 8688,
      "start": 112.4800033569336,
      "temperature": 0.0,
      "text": " You know, why is that important?",
      "tokens": [
        51644,
        509,
        458,
        11,
        983,
        307,
        300,
        1021,
        30,
        51702
      ]
    },
    {
      "id": 49,
      "avg_logprob": -0.19102443754673004,
      "compression_ratio": 1.6172839403152466,
      "end": 115.27999877929688,
      "no_speech_prob": 0.001032222993671894,
      "seek": 8688,
      "start": 113.63999938964844,
      "temperature": 0.0,
      "text": " How can that change the outcome?",
      "tokens": [
        51702,
        1012,
        393,
        300,
        1319,
        264,
        9700,
        30,
        51784
      ]
    },
    {
      "id": 50,
      "avg_logprob": -0.19102443754673004,
      "compression_ratio": 1.6172839403152466,
      "end": 116.77999877929688,
      "no_speech_prob": 0.001032222993671894,
      "seek": 8688,
      "start": 115.27999877929688,
      "temperature": 0.0,
      "text": " That's super important because, you know,",
      "tokens": [
        51784,
        663,
        311,
        1687,
        1021,
        570,
        11,
        291,
        458,
        11,
        51859
      ]
    },
    {
      "id": 51,
      "avg_logprob": -0.27171802520751953,
      "compression_ratio": 1.559999942779541,
      "end": 119.5,
      "no_speech_prob": 0.01115746796131134,
      "seek": 11678,
      "start": 117.55999755859375,
      "temperature": 0.0,
      "text": " when we add examples to our prompts to, you know,",
      "tokens": [
        50403,
        562,
        321,
        909,
        5110,
        281,
        527,
        41095,
        281,
        11,
        291,
        458,
        11,
        50500
      ]
    },
    {
      "id": 52,
      "avg_logprob": -0.27171802520751953,
      "compression_ratio": 1.559999942779541,
      "end": 122.58000183105469,
      "no_speech_prob": 0.01115746796131134,
      "seek": 11678,
      "start": 119.5,
      "temperature": 0.0,
      "text": " give us an idea about what types of edits we wanted to make,",
      "tokens": [
        50500,
        976,
        505,
        364,
        1558,
        466,
        437,
        3467,
        295,
        41752,
        321,
        1415,
        281,
        652,
        11,
        50654
      ]
    },
    {
      "id": 53,
      "avg_logprob": -0.27171802520751953,
      "compression_ratio": 1.559999942779541,
      "end": 124.54000091552734,
      "no_speech_prob": 0.01115746796131134,
      "seek": 11678,
      "start": 122.58000183105469,
      "temperature": 0.0,
      "text": " it'll pick up on the taste of our editing",
      "tokens": [
        50654,
        309,
        603,
        1888,
        493,
        322,
        264,
        3939,
        295,
        527,
        10000,
        50752
      ]
    },
    {
      "id": 54,
      "avg_logprob": -0.27171802520751953,
      "compression_ratio": 1.559999942779541,
      "end": 126.63999938964844,
      "no_speech_prob": 0.01115746796131134,
      "seek": 11678,
      "start": 124.54000091552734,
      "temperature": 0.0,
      "text": " and what words we like to keep in and keep out,",
      "tokens": [
        50752,
        293,
        437,
        2283,
        321,
        411,
        281,
        1066,
        294,
        293,
        1066,
        484,
        11,
        50857
      ]
    },
    {
      "id": 55,
      "avg_logprob": -0.27171802520751953,
      "compression_ratio": 1.559999942779541,
      "end": 127.4800033569336,
      "no_speech_prob": 0.01115746796131134,
      "seek": 11678,
      "start": 126.63999938964844,
      "temperature": 0.0,
      "text": " so on and so forth.",
      "tokens": [
        50857,
        370,
        322,
        293,
        370,
        5220,
        13,
        50899
      ]
    },
    {
      "id": 56,
      "avg_logprob": -0.27171802520751953,
      "compression_ratio": 1.559999942779541,
      "end": 129.25999450683594,
      "no_speech_prob": 0.01115746796131134,
      "seek": 11678,
      "start": 127.4800033569336,
      "temperature": 0.0,
      "text": " So more work to be done on that,",
      "tokens": [
        50899,
        407,
        544,
        589,
        281,
        312,
        1096,
        322,
        300,
        11,
        50988
      ]
    },
    {
      "id": 57,
      "avg_logprob": -0.27171802520751953,
      "compression_ratio": 1.559999942779541,
      "end": 131.05999755859375,
      "no_speech_prob": 0.01115746796131134,
      "seek": 11678,
      "start": 129.25999450683594,
      "temperature": 0.0,
      "text": " adding in examples.",
      "tokens": [
        50988,
        5127,
        294,
        5110,
        13,
        51078
      ]
    }
  ],
  "language": "english",
  "duration": 131.05999755859375,
  "timestamp": 1742835502.792773
}