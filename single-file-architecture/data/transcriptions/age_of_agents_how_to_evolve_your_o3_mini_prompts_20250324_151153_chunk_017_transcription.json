{
  "audio_path": "data/chunks/age_of_agents_how_to_evolve_your_o3_mini_prompts_20250324_151153_chunk_017.mp3",
  "text": "tag here, it's going to be really important for making the model more consistent with my personal editing style. So something to think about for your prompt chains and AI agents, examples that guide your model in these more creative, more subjective decision making domains is a great way to guide your model and to guide the outcome. And of course, the best way to do this is benchmarking, but also run this against real data and then in a reinforcement learning type of way, take the edits that are working coming out of the model and update the prompt that was feeling that model with those correct edits as examples, right? So more on that in the future, I have some pretty interesting ideas around automatic feedback looped prompts to add more auto self-improving behavior into the prompt. Another interesting problem that I'm running into is that idea of not solving the problem at all. I think one of the strongest signs of a high level problem solver, a senior level engineer is the ability to look at a problem and decide that it doesn't need to be solved at all. And this is tricky to teach or explain to an LLM because these are next token generators, not next token, not generators. For edits, I've run into this several times with this tool already, where the best thing to do is nothing, right? The slice in a real video will sometimes contain perfect script and nothing needs to be changed. So, you know, I think probably more prompt engineering and explicitly mentioning when things need to be edited will be helpful for solving that problem inside of prompts, prompt chains and AI agents. So, you know, the next time you want to solve a problem with generative AI, first start with a prompt, then move to a prompt chain. And only if your prompt chain and your, you know, graph of steps is not giving you the performance that you need, only then should you move to a full on AI agent that can operate in a domain for you automatically. I highly recommend, you know, throughout that process, the more important the problem you're trying to solve, the more important it is to set up benchmarks so that you can know for a fact that your prompt chain is outperforming your prompt and your AI agent is outperforming your prompt chain. Agents give us the ability.",
  "segments": [
    {
      "id": 0,
      "avg_logprob": -0.2589476406574249,
      "compression_ratio": 1.7638888359069824,
      "end": 4.199999809265137,
      "no_speech_prob": 0.06463973969221115,
      "seek": 0,
      "start": 0.0,
      "temperature": 0.0,
      "text": " tag here, it's going to be really important for making the model more consistent with",
      "tokens": [
        50364,
        6162,
        510,
        11,
        309,
        311,
        516,
        281,
        312,
        534,
        1021,
        337,
        1455,
        264,
        2316,
        544,
        8398,
        365,
        50574
      ]
    },
    {
      "id": 1,
      "avg_logprob": -0.2589476406574249,
      "compression_ratio": 1.7638888359069824,
      "end": 6.159999847412109,
      "no_speech_prob": 0.06463973969221115,
      "seek": 0,
      "start": 4.199999809265137,
      "temperature": 0.0,
      "text": " my personal editing style.",
      "tokens": [
        50574,
        452,
        2973,
        10000,
        3758,
        13,
        50672
      ]
    },
    {
      "id": 2,
      "avg_logprob": -0.2589476406574249,
      "compression_ratio": 1.7638888359069824,
      "end": 11.079999923706055,
      "no_speech_prob": 0.06463973969221115,
      "seek": 0,
      "start": 6.159999847412109,
      "temperature": 0.0,
      "text": " So something to think about for your prompt chains and AI agents, examples that guide",
      "tokens": [
        50672,
        407,
        746,
        281,
        519,
        466,
        337,
        428,
        12391,
        12626,
        293,
        7318,
        12554,
        11,
        5110,
        300,
        5934,
        50918
      ]
    },
    {
      "id": 3,
      "avg_logprob": -0.2589476406574249,
      "compression_ratio": 1.7638888359069824,
      "end": 18.360000610351562,
      "no_speech_prob": 0.06463973969221115,
      "seek": 0,
      "start": 11.079999923706055,
      "temperature": 0.0,
      "text": " your model in these more creative, more subjective decision making domains is a great way to",
      "tokens": [
        50918,
        428,
        2316,
        294,
        613,
        544,
        5880,
        11,
        544,
        25972,
        3537,
        1455,
        25514,
        307,
        257,
        869,
        636,
        281,
        51282
      ]
    },
    {
      "id": 4,
      "avg_logprob": -0.2589476406574249,
      "compression_ratio": 1.7638888359069824,
      "end": 21.100000381469727,
      "no_speech_prob": 0.06463973969221115,
      "seek": 0,
      "start": 18.360000610351562,
      "temperature": 0.0,
      "text": " guide your model and to guide the outcome.",
      "tokens": [
        51282,
        5934,
        428,
        2316,
        293,
        281,
        5934,
        264,
        9700,
        13,
        51419
      ]
    },
    {
      "id": 5,
      "avg_logprob": -0.2589476406574249,
      "compression_ratio": 1.7638888359069824,
      "end": 24.760000228881836,
      "no_speech_prob": 0.06463973969221115,
      "seek": 0,
      "start": 21.100000381469727,
      "temperature": 0.0,
      "text": " And of course, the best way to do this is benchmarking, but also run this against real",
      "tokens": [
        51419,
        400,
        295,
        1164,
        11,
        264,
        1151,
        636,
        281,
        360,
        341,
        307,
        18927,
        278,
        11,
        457,
        611,
        1190,
        341,
        1970,
        957,
        51602
      ]
    },
    {
      "id": 6,
      "avg_logprob": -0.2589476406574249,
      "compression_ratio": 1.7638888359069824,
      "end": 29.700000762939453,
      "no_speech_prob": 0.06463973969221115,
      "seek": 0,
      "start": 24.760000228881836,
      "temperature": 0.0,
      "text": " data and then in a reinforcement learning type of way, take the edits that are working",
      "tokens": [
        51602,
        1412,
        293,
        550,
        294,
        257,
        29280,
        2539,
        2010,
        295,
        636,
        11,
        747,
        264,
        41752,
        300,
        366,
        1364,
        51849
      ]
    },
    {
      "id": 7,
      "avg_logprob": -0.2315772920846939,
      "compression_ratio": 1.7426470518112183,
      "end": 34.29999923706055,
      "no_speech_prob": 0.01150767132639885,
      "seek": 2970,
      "start": 29.700000762939453,
      "temperature": 0.0,
      "text": " coming out of the model and update the prompt that was feeling that model with those correct",
      "tokens": [
        50364,
        1348,
        484,
        295,
        264,
        2316,
        293,
        5623,
        264,
        12391,
        300,
        390,
        2633,
        300,
        2316,
        365,
        729,
        3006,
        50594
      ]
    },
    {
      "id": 8,
      "avg_logprob": -0.2315772920846939,
      "compression_ratio": 1.7426470518112183,
      "end": 35.70000076293945,
      "no_speech_prob": 0.01150767132639885,
      "seek": 2970,
      "start": 34.29999923706055,
      "temperature": 0.0,
      "text": " edits as examples, right?",
      "tokens": [
        50594,
        41752,
        382,
        5110,
        11,
        558,
        30,
        50664
      ]
    },
    {
      "id": 9,
      "avg_logprob": -0.2315772920846939,
      "compression_ratio": 1.7426470518112183,
      "end": 39.70000076293945,
      "no_speech_prob": 0.01150767132639885,
      "seek": 2970,
      "start": 35.70000076293945,
      "temperature": 0.0,
      "text": " So more on that in the future, I have some pretty interesting ideas around automatic",
      "tokens": [
        50664,
        407,
        544,
        322,
        300,
        294,
        264,
        2027,
        11,
        286,
        362,
        512,
        1238,
        1880,
        3487,
        926,
        12509,
        50864
      ]
    },
    {
      "id": 10,
      "avg_logprob": -0.2315772920846939,
      "compression_ratio": 1.7426470518112183,
      "end": 45.5,
      "no_speech_prob": 0.01150767132639885,
      "seek": 2970,
      "start": 39.70000076293945,
      "temperature": 0.0,
      "text": " feedback looped prompts to add more auto self-improving behavior into the prompt.",
      "tokens": [
        50864,
        5824,
        6367,
        292,
        41095,
        281,
        909,
        544,
        8399,
        2698,
        12,
        332,
        4318,
        798,
        5223,
        666,
        264,
        12391,
        13,
        51154
      ]
    },
    {
      "id": 11,
      "avg_logprob": -0.2315772920846939,
      "compression_ratio": 1.7426470518112183,
      "end": 50.5,
      "no_speech_prob": 0.01150767132639885,
      "seek": 2970,
      "start": 45.5,
      "temperature": 0.0,
      "text": " Another interesting problem that I'm running into is that idea of not solving the problem",
      "tokens": [
        51154,
        3996,
        1880,
        1154,
        300,
        286,
        478,
        2614,
        666,
        307,
        300,
        1558,
        295,
        406,
        12606,
        264,
        1154,
        51404
      ]
    },
    {
      "id": 12,
      "avg_logprob": -0.2315772920846939,
      "compression_ratio": 1.7426470518112183,
      "end": 51.5,
      "no_speech_prob": 0.01150767132639885,
      "seek": 2970,
      "start": 50.5,
      "temperature": 0.0,
      "text": " at all.",
      "tokens": [
        51404,
        412,
        439,
        13,
        51454
      ]
    },
    {
      "id": 13,
      "avg_logprob": -0.2315772920846939,
      "compression_ratio": 1.7426470518112183,
      "end": 57.099998474121094,
      "no_speech_prob": 0.01150767132639885,
      "seek": 2970,
      "start": 51.5,
      "temperature": 0.0,
      "text": " I think one of the strongest signs of a high level problem solver, a senior level engineer",
      "tokens": [
        51454,
        286,
        519,
        472,
        295,
        264,
        16595,
        7880,
        295,
        257,
        1090,
        1496,
        1154,
        1404,
        331,
        11,
        257,
        7965,
        1496,
        11403,
        51734
      ]
    },
    {
      "id": 14,
      "avg_logprob": -0.23152877390384674,
      "compression_ratio": 1.6254826784133911,
      "end": 63.79999923706055,
      "no_speech_prob": 0.09008479118347168,
      "seek": 5710,
      "start": 57.099998474121094,
      "temperature": 0.0,
      "text": " is the ability to look at a problem and decide that it doesn't need to be solved at all.",
      "tokens": [
        50364,
        307,
        264,
        3485,
        281,
        574,
        412,
        257,
        1154,
        293,
        4536,
        300,
        309,
        1177,
        380,
        643,
        281,
        312,
        13041,
        412,
        439,
        13,
        50699
      ]
    },
    {
      "id": 15,
      "avg_logprob": -0.23152877390384674,
      "compression_ratio": 1.6254826784133911,
      "end": 68.9000015258789,
      "no_speech_prob": 0.09008479118347168,
      "seek": 5710,
      "start": 63.79999923706055,
      "temperature": 0.0,
      "text": " And this is tricky to teach or explain to an LLM because these are next token generators,",
      "tokens": [
        50699,
        400,
        341,
        307,
        12414,
        281,
        2924,
        420,
        2903,
        281,
        364,
        441,
        43,
        44,
        570,
        613,
        366,
        958,
        14862,
        38662,
        11,
        50954
      ]
    },
    {
      "id": 16,
      "avg_logprob": -0.23152877390384674,
      "compression_ratio": 1.6254826784133911,
      "end": 71.45999908447266,
      "no_speech_prob": 0.09008479118347168,
      "seek": 5710,
      "start": 68.9000015258789,
      "temperature": 0.0,
      "text": " not next token, not generators.",
      "tokens": [
        50954,
        406,
        958,
        14862,
        11,
        406,
        38662,
        13,
        51082
      ]
    },
    {
      "id": 17,
      "avg_logprob": -0.23152877390384674,
      "compression_ratio": 1.6254826784133911,
      "end": 75.5199966430664,
      "no_speech_prob": 0.09008479118347168,
      "seek": 5710,
      "start": 71.45999908447266,
      "temperature": 0.0,
      "text": " For edits, I've run into this several times with this tool already, where the best thing",
      "tokens": [
        51082,
        1171,
        41752,
        11,
        286,
        600,
        1190,
        666,
        341,
        2940,
        1413,
        365,
        341,
        2290,
        1217,
        11,
        689,
        264,
        1151,
        551,
        51285
      ]
    },
    {
      "id": 18,
      "avg_logprob": -0.23152877390384674,
      "compression_ratio": 1.6254826784133911,
      "end": 77.58000183105469,
      "no_speech_prob": 0.09008479118347168,
      "seek": 5710,
      "start": 75.5199966430664,
      "temperature": 0.0,
      "text": " to do is nothing, right?",
      "tokens": [
        51285,
        281,
        360,
        307,
        1825,
        11,
        558,
        30,
        51388
      ]
    },
    {
      "id": 19,
      "avg_logprob": -0.23152877390384674,
      "compression_ratio": 1.6254826784133911,
      "end": 83.33999633789062,
      "no_speech_prob": 0.09008479118347168,
      "seek": 5710,
      "start": 77.58000183105469,
      "temperature": 0.0,
      "text": " The slice in a real video will sometimes contain perfect script and nothing needs to be changed.",
      "tokens": [
        51388,
        440,
        13153,
        294,
        257,
        957,
        960,
        486,
        2171,
        5304,
        2176,
        5755,
        293,
        1825,
        2203,
        281,
        312,
        3105,
        13,
        51676
      ]
    },
    {
      "id": 20,
      "avg_logprob": -0.2401713728904724,
      "compression_ratio": 1.8819187879562378,
      "end": 87.9000015258789,
      "no_speech_prob": 0.039045050740242004,
      "seek": 8334,
      "start": 83.33999633789062,
      "temperature": 0.0,
      "text": " So, you know, I think probably more prompt engineering and explicitly mentioning when",
      "tokens": [
        50364,
        407,
        11,
        291,
        458,
        11,
        286,
        519,
        1391,
        544,
        12391,
        7043,
        293,
        20803,
        18315,
        562,
        50592
      ]
    },
    {
      "id": 21,
      "avg_logprob": -0.2401713728904724,
      "compression_ratio": 1.8819187879562378,
      "end": 92.9000015258789,
      "no_speech_prob": 0.039045050740242004,
      "seek": 8334,
      "start": 87.9000015258789,
      "temperature": 0.0,
      "text": " things need to be edited will be helpful for solving that problem inside of prompts, prompt",
      "tokens": [
        50592,
        721,
        643,
        281,
        312,
        23016,
        486,
        312,
        4961,
        337,
        12606,
        300,
        1154,
        1854,
        295,
        41095,
        11,
        12391,
        50842
      ]
    },
    {
      "id": 22,
      "avg_logprob": -0.2401713728904724,
      "compression_ratio": 1.8819187879562378,
      "end": 94.73999786376953,
      "no_speech_prob": 0.039045050740242004,
      "seek": 8334,
      "start": 92.9000015258789,
      "temperature": 0.0,
      "text": " chains and AI agents.",
      "tokens": [
        50842,
        12626,
        293,
        7318,
        12554,
        13,
        50934
      ]
    },
    {
      "id": 23,
      "avg_logprob": -0.2401713728904724,
      "compression_ratio": 1.8819187879562378,
      "end": 100.30000305175781,
      "no_speech_prob": 0.039045050740242004,
      "seek": 8334,
      "start": 94.73999786376953,
      "temperature": 0.0,
      "text": " So, you know, the next time you want to solve a problem with generative AI, first start",
      "tokens": [
        50934,
        407,
        11,
        291,
        458,
        11,
        264,
        958,
        565,
        291,
        528,
        281,
        5039,
        257,
        1154,
        365,
        1337,
        1166,
        7318,
        11,
        700,
        722,
        51212
      ]
    },
    {
      "id": 24,
      "avg_logprob": -0.2401713728904724,
      "compression_ratio": 1.8819187879562378,
      "end": 102.81999969482422,
      "no_speech_prob": 0.039045050740242004,
      "seek": 8334,
      "start": 100.30000305175781,
      "temperature": 0.0,
      "text": " with a prompt, then move to a prompt chain.",
      "tokens": [
        51212,
        365,
        257,
        12391,
        11,
        550,
        1286,
        281,
        257,
        12391,
        5021,
        13,
        51338
      ]
    },
    {
      "id": 25,
      "avg_logprob": -0.2401713728904724,
      "compression_ratio": 1.8819187879562378,
      "end": 106.9800033569336,
      "no_speech_prob": 0.039045050740242004,
      "seek": 8334,
      "start": 102.81999969482422,
      "temperature": 0.0,
      "text": " And only if your prompt chain and your, you know, graph of steps is not giving you the",
      "tokens": [
        51338,
        400,
        787,
        498,
        428,
        12391,
        5021,
        293,
        428,
        11,
        291,
        458,
        11,
        4295,
        295,
        4439,
        307,
        406,
        2902,
        291,
        264,
        51546
      ]
    },
    {
      "id": 26,
      "avg_logprob": -0.2401713728904724,
      "compression_ratio": 1.8819187879562378,
      "end": 112.9800033569336,
      "no_speech_prob": 0.039045050740242004,
      "seek": 8334,
      "start": 106.9800033569336,
      "temperature": 0.0,
      "text": " performance that you need, only then should you move to a full on AI agent that can operate",
      "tokens": [
        51546,
        3389,
        300,
        291,
        643,
        11,
        787,
        550,
        820,
        291,
        1286,
        281,
        257,
        1577,
        322,
        7318,
        9461,
        300,
        393,
        9651,
        51846
      ]
    },
    {
      "id": 27,
      "avg_logprob": -0.2624400854110718,
      "compression_ratio": 1.7889447212219238,
      "end": 115.13999938964844,
      "no_speech_prob": 0.06559734046459198,
      "seek": 11298,
      "start": 113.0199966430664,
      "temperature": 0.0,
      "text": " in a domain for you automatically.",
      "tokens": [
        50366,
        294,
        257,
        9274,
        337,
        291,
        6772,
        13,
        50472
      ]
    },
    {
      "id": 28,
      "avg_logprob": -0.2624400854110718,
      "compression_ratio": 1.7889447212219238,
      "end": 119.26000213623047,
      "no_speech_prob": 0.06559734046459198,
      "seek": 11298,
      "start": 115.13999938964844,
      "temperature": 0.0,
      "text": " I highly recommend, you know, throughout that process, the more important the problem you're",
      "tokens": [
        50472,
        286,
        5405,
        2748,
        11,
        291,
        458,
        11,
        3710,
        300,
        1399,
        11,
        264,
        544,
        1021,
        264,
        1154,
        291,
        434,
        50678
      ]
    },
    {
      "id": 29,
      "avg_logprob": -0.2624400854110718,
      "compression_ratio": 1.7889447212219238,
      "end": 122.81999969482422,
      "no_speech_prob": 0.06559734046459198,
      "seek": 11298,
      "start": 119.26000213623047,
      "temperature": 0.0,
      "text": " trying to solve, the more important it is to set up benchmarks so that you can know",
      "tokens": [
        50678,
        1382,
        281,
        5039,
        11,
        264,
        544,
        1021,
        309,
        307,
        281,
        992,
        493,
        43751,
        370,
        300,
        291,
        393,
        458,
        50856
      ]
    },
    {
      "id": 30,
      "avg_logprob": -0.2624400854110718,
      "compression_ratio": 1.7889447212219238,
      "end": 128.6199951171875,
      "no_speech_prob": 0.06559734046459198,
      "seek": 11298,
      "start": 122.81999969482422,
      "temperature": 0.0,
      "text": " for a fact that your prompt chain is outperforming your prompt and your AI agent is outperforming",
      "tokens": [
        50856,
        337,
        257,
        1186,
        300,
        428,
        12391,
        5021,
        307,
        484,
        26765,
        278,
        428,
        12391,
        293,
        428,
        7318,
        9461,
        307,
        484,
        26765,
        278,
        51146
      ]
    },
    {
      "id": 31,
      "avg_logprob": -0.2624400854110718,
      "compression_ratio": 1.7889447212219238,
      "end": 130.3000030517578,
      "no_speech_prob": 0.06559734046459198,
      "seek": 11298,
      "start": 128.6199951171875,
      "temperature": 0.0,
      "text": " your prompt chain.",
      "tokens": [
        51146,
        428,
        12391,
        5021,
        13,
        51230
      ]
    },
    {
      "id": 32,
      "avg_logprob": -0.2624400854110718,
      "compression_ratio": 1.7889447212219238,
      "end": 131.13999938964844,
      "no_speech_prob": 0.06559734046459198,
      "seek": 11298,
      "start": 130.3000030517578,
      "temperature": 0.0,
      "text": " Agents give us the ability.",
      "tokens": [
        51230,
        2725,
        791,
        976,
        505,
        264,
        3485,
        13,
        51272
      ]
    }
  ],
  "language": "english",
  "duration": 131.05999755859375,
  "timestamp": 1742835514.949495
}