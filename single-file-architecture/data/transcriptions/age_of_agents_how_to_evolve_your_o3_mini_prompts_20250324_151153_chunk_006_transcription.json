{
  "audio_path": "data/chunks/age_of_agents_how_to_evolve_your_o3_mini_prompts_20250324_151153_chunk_006.mp3",
  "text": "this, right? So we have the original text and then we have the predicted final text coming out of our language model and then we have the target text. So that's us answering the problem successfully and incorrectly. So I'll comment this out and let's go ahead and look at the prompt chain. By looking at the log you'll understand exactly how it works. So let's kick this off. What we're doing here with the prompt chain is we're taking that same problem and we're throwing more compute at the problem. You can see here GPG 4.0 got this problem correct with a prompt chain. So let's go ahead and look at the log and then let's break down how the prompt chain worked. Let's go ahead and open up V2. This is the a cut prompt chain and let's look at how a prompt chain outperforms the single prompt, okay? So once again we have that exact same problem, right? Of this scratchpad kind of blah blah blah. So basically we have the starting text and then we have the ground truth, right? This is the end result we're looking to get to. And then we have this new piece of information. When you're using prompt chains and AI agents you'll always want to limit the amount of compute or loops that your AI tooling can run. You don't want it to run infinitely. You might have a bug, something might go wrong and there is nearly always a sweet spot or a perfect range for the problem that you're trying to solve. So you can see here for this prompt chain I'm allowing eight compute loops, eight iterations, okay? So we start with this one deletion here. This, the scratchpad kind of act um. And then if we scroll down here you can see, you know, that made a pretty good edit. Then we can scroll down and then you can see it only took out um. So it's taking another shot. It sees that, you know, after this first run that I made I still need to edit more, okay? So now it's just taking out um. Third compute loop, right? So it has, you know, five shots left. And so now it's telling us that no deletions were generated. So it's going to exit the loop. So inside of this prompt, which we'll look at in a moment, there is a condition that says if you're done return an empty list, okay? It created two deletions. Here's the first one with several words in the deletion. And then we have in a follow-up loop because it didn't create the perfect edit the first time. You can see here we have the words um edited that out. And then we have the final iteration slice. So this is what our final slice looks like.",
  "segments": [
    {
      "id": 0,
      "avg_logprob": -0.23724918067455292,
      "compression_ratio": 1.8228346109390259,
      "end": 4.360000133514404,
      "no_speech_prob": 0.07583343982696533,
      "seek": 0,
      "start": 0.0,
      "temperature": 0.0,
      "text": " this, right? So we have the original text and then we have the predicted final",
      "tokens": [
        50364,
        341,
        11,
        558,
        30,
        407,
        321,
        362,
        264,
        3380,
        2487,
        293,
        550,
        321,
        362,
        264,
        19147,
        2572,
        50582
      ]
    },
    {
      "id": 1,
      "avg_logprob": -0.23724918067455292,
      "compression_ratio": 1.8228346109390259,
      "end": 8.0,
      "no_speech_prob": 0.07583343982696533,
      "seek": 0,
      "start": 4.360000133514404,
      "temperature": 0.0,
      "text": " text coming out of our language model and then we have the target text. So",
      "tokens": [
        50582,
        2487,
        1348,
        484,
        295,
        527,
        2856,
        2316,
        293,
        550,
        321,
        362,
        264,
        3779,
        2487,
        13,
        407,
        50764
      ]
    },
    {
      "id": 2,
      "avg_logprob": -0.23724918067455292,
      "compression_ratio": 1.8228346109390259,
      "end": 10.800000190734863,
      "no_speech_prob": 0.07583343982696533,
      "seek": 0,
      "start": 8.0,
      "temperature": 0.0,
      "text": " that's us answering the problem successfully and incorrectly. So I'll",
      "tokens": [
        50764,
        300,
        311,
        505,
        13430,
        264,
        1154,
        10727,
        293,
        42892,
        13,
        407,
        286,
        603,
        50904
      ]
    },
    {
      "id": 3,
      "avg_logprob": -0.23724918067455292,
      "compression_ratio": 1.8228346109390259,
      "end": 14.920000076293945,
      "no_speech_prob": 0.07583343982696533,
      "seek": 0,
      "start": 10.800000190734863,
      "temperature": 0.0,
      "text": " comment this out and let's go ahead and look at the prompt chain. By looking at",
      "tokens": [
        50904,
        2871,
        341,
        484,
        293,
        718,
        311,
        352,
        2286,
        293,
        574,
        412,
        264,
        12391,
        5021,
        13,
        3146,
        1237,
        412,
        51110
      ]
    },
    {
      "id": 4,
      "avg_logprob": -0.23724918067455292,
      "compression_ratio": 1.8228346109390259,
      "end": 21.399999618530273,
      "no_speech_prob": 0.07583343982696533,
      "seek": 0,
      "start": 14.920000076293945,
      "temperature": 0.0,
      "text": " the log you'll understand exactly how it works. So let's kick this off. What we're",
      "tokens": [
        51110,
        264,
        3565,
        291,
        603,
        1223,
        2293,
        577,
        309,
        1985,
        13,
        407,
        718,
        311,
        4437,
        341,
        766,
        13,
        708,
        321,
        434,
        51434
      ]
    },
    {
      "id": 5,
      "avg_logprob": -0.23724918067455292,
      "compression_ratio": 1.8228346109390259,
      "end": 24.399999618530273,
      "no_speech_prob": 0.07583343982696533,
      "seek": 0,
      "start": 21.399999618530273,
      "temperature": 0.0,
      "text": " doing here with the prompt chain is we're taking that same problem and we're",
      "tokens": [
        51434,
        884,
        510,
        365,
        264,
        12391,
        5021,
        307,
        321,
        434,
        1940,
        300,
        912,
        1154,
        293,
        321,
        434,
        51584
      ]
    },
    {
      "id": 6,
      "avg_logprob": -0.243210569024086,
      "compression_ratio": 1.8695652484893799,
      "end": 30.079999923706055,
      "no_speech_prob": 0.024421392008662224,
      "seek": 2440,
      "start": 24.399999618530273,
      "temperature": 0.0,
      "text": " throwing more compute at the problem. You can see here GPG 4.0 got this problem",
      "tokens": [
        50364,
        10238,
        544,
        14722,
        412,
        264,
        1154,
        13,
        509,
        393,
        536,
        510,
        26039,
        38,
        1017,
        13,
        15,
        658,
        341,
        1154,
        50648
      ]
    },
    {
      "id": 7,
      "avg_logprob": -0.243210569024086,
      "compression_ratio": 1.8695652484893799,
      "end": 33.720001220703125,
      "no_speech_prob": 0.024421392008662224,
      "seek": 2440,
      "start": 30.079999923706055,
      "temperature": 0.0,
      "text": " correct with a prompt chain. So let's go ahead and look at the log and then let's",
      "tokens": [
        50648,
        3006,
        365,
        257,
        12391,
        5021,
        13,
        407,
        718,
        311,
        352,
        2286,
        293,
        574,
        412,
        264,
        3565,
        293,
        550,
        718,
        311,
        50830
      ]
    },
    {
      "id": 8,
      "avg_logprob": -0.243210569024086,
      "compression_ratio": 1.8695652484893799,
      "end": 37.439998626708984,
      "no_speech_prob": 0.024421392008662224,
      "seek": 2440,
      "start": 33.720001220703125,
      "temperature": 0.0,
      "text": " break down how the prompt chain worked. Let's go ahead and open up V2. This is",
      "tokens": [
        50830,
        1821,
        760,
        577,
        264,
        12391,
        5021,
        2732,
        13,
        961,
        311,
        352,
        2286,
        293,
        1269,
        493,
        691,
        17,
        13,
        639,
        307,
        51016
      ]
    },
    {
      "id": 9,
      "avg_logprob": -0.243210569024086,
      "compression_ratio": 1.8695652484893799,
      "end": 41.52000045776367,
      "no_speech_prob": 0.024421392008662224,
      "seek": 2440,
      "start": 37.439998626708984,
      "temperature": 0.0,
      "text": " the a cut prompt chain and let's look at how a prompt chain outperforms the",
      "tokens": [
        51016,
        264,
        257,
        1723,
        12391,
        5021,
        293,
        718,
        311,
        574,
        412,
        577,
        257,
        12391,
        5021,
        484,
        26765,
        82,
        264,
        51220
      ]
    },
    {
      "id": 10,
      "avg_logprob": -0.243210569024086,
      "compression_ratio": 1.8695652484893799,
      "end": 45.959999084472656,
      "no_speech_prob": 0.024421392008662224,
      "seek": 2440,
      "start": 41.52000045776367,
      "temperature": 0.0,
      "text": " single prompt, okay? So once again we have that exact same problem, right? Of this",
      "tokens": [
        51220,
        2167,
        12391,
        11,
        1392,
        30,
        407,
        1564,
        797,
        321,
        362,
        300,
        1900,
        912,
        1154,
        11,
        558,
        30,
        2720,
        341,
        51442
      ]
    },
    {
      "id": 11,
      "avg_logprob": -0.243210569024086,
      "compression_ratio": 1.8695652484893799,
      "end": 49.400001525878906,
      "no_speech_prob": 0.024421392008662224,
      "seek": 2440,
      "start": 45.959999084472656,
      "temperature": 0.0,
      "text": " scratchpad kind of blah blah blah. So basically we have the starting text and",
      "tokens": [
        51442,
        8459,
        13647,
        733,
        295,
        12288,
        12288,
        12288,
        13,
        407,
        1936,
        321,
        362,
        264,
        2891,
        2487,
        293,
        51614
      ]
    },
    {
      "id": 12,
      "avg_logprob": -0.243210569024086,
      "compression_ratio": 1.8695652484893799,
      "end": 52.52000045776367,
      "no_speech_prob": 0.024421392008662224,
      "seek": 2440,
      "start": 49.400001525878906,
      "temperature": 0.0,
      "text": " then we have the ground truth, right? This is the end result we're looking to get",
      "tokens": [
        51614,
        550,
        321,
        362,
        264,
        2727,
        3494,
        11,
        558,
        30,
        639,
        307,
        264,
        917,
        1874,
        321,
        434,
        1237,
        281,
        483,
        51770
      ]
    },
    {
      "id": 13,
      "avg_logprob": -0.21672198176383972,
      "compression_ratio": 1.7142857313156128,
      "end": 56.15999984741211,
      "no_speech_prob": 0.03514304384589195,
      "seek": 5252,
      "start": 52.52000045776367,
      "temperature": 0.0,
      "text": " to. And then we have this new piece of information. When you're using prompt",
      "tokens": [
        50364,
        281,
        13,
        400,
        550,
        321,
        362,
        341,
        777,
        2522,
        295,
        1589,
        13,
        1133,
        291,
        434,
        1228,
        12391,
        50546
      ]
    },
    {
      "id": 14,
      "avg_logprob": -0.21672198176383972,
      "compression_ratio": 1.7142857313156128,
      "end": 60.52000045776367,
      "no_speech_prob": 0.03514304384589195,
      "seek": 5252,
      "start": 56.15999984741211,
      "temperature": 0.0,
      "text": " chains and AI agents you'll always want to limit the amount of compute or loops",
      "tokens": [
        50546,
        12626,
        293,
        7318,
        12554,
        291,
        603,
        1009,
        528,
        281,
        4948,
        264,
        2372,
        295,
        14722,
        420,
        16121,
        50764
      ]
    },
    {
      "id": 15,
      "avg_logprob": -0.21672198176383972,
      "compression_ratio": 1.7142857313156128,
      "end": 65.04000091552734,
      "no_speech_prob": 0.03514304384589195,
      "seek": 5252,
      "start": 60.52000045776367,
      "temperature": 0.0,
      "text": " that your AI tooling can run. You don't want it to run infinitely. You might have",
      "tokens": [
        50764,
        300,
        428,
        7318,
        46593,
        393,
        1190,
        13,
        509,
        500,
        380,
        528,
        309,
        281,
        1190,
        36227,
        13,
        509,
        1062,
        362,
        50990
      ]
    },
    {
      "id": 16,
      "avg_logprob": -0.21672198176383972,
      "compression_ratio": 1.7142857313156128,
      "end": 69.80000305175781,
      "no_speech_prob": 0.03514304384589195,
      "seek": 5252,
      "start": 65.04000091552734,
      "temperature": 0.0,
      "text": " a bug, something might go wrong and there is nearly always a sweet spot or a",
      "tokens": [
        50990,
        257,
        7426,
        11,
        746,
        1062,
        352,
        2085,
        293,
        456,
        307,
        6217,
        1009,
        257,
        3844,
        4008,
        420,
        257,
        51228
      ]
    },
    {
      "id": 17,
      "avg_logprob": -0.21672198176383972,
      "compression_ratio": 1.7142857313156128,
      "end": 73.5999984741211,
      "no_speech_prob": 0.03514304384589195,
      "seek": 5252,
      "start": 69.80000305175781,
      "temperature": 0.0,
      "text": " perfect range for the problem that you're trying to solve. So you can see",
      "tokens": [
        51228,
        2176,
        3613,
        337,
        264,
        1154,
        300,
        291,
        434,
        1382,
        281,
        5039,
        13,
        407,
        291,
        393,
        536,
        51418
      ]
    },
    {
      "id": 18,
      "avg_logprob": -0.21672198176383972,
      "compression_ratio": 1.7142857313156128,
      "end": 77.5999984741211,
      "no_speech_prob": 0.03514304384589195,
      "seek": 5252,
      "start": 73.5999984741211,
      "temperature": 0.0,
      "text": " here for this prompt chain I'm allowing eight compute loops, eight iterations,",
      "tokens": [
        51418,
        510,
        337,
        341,
        12391,
        5021,
        286,
        478,
        8293,
        3180,
        14722,
        16121,
        11,
        3180,
        36540,
        11,
        51618
      ]
    },
    {
      "id": 19,
      "avg_logprob": -0.2511487305164337,
      "compression_ratio": 1.8441064357757568,
      "end": 83.23999786376953,
      "no_speech_prob": 0.268891304731369,
      "seek": 7760,
      "start": 77.5999984741211,
      "temperature": 0.0,
      "text": " okay? So we start with this one deletion here. This, the scratchpad kind of act",
      "tokens": [
        50364,
        1392,
        30,
        407,
        321,
        722,
        365,
        341,
        472,
        1103,
        302,
        313,
        510,
        13,
        639,
        11,
        264,
        8459,
        13647,
        733,
        295,
        605,
        50646
      ]
    },
    {
      "id": 20,
      "avg_logprob": -0.2511487305164337,
      "compression_ratio": 1.8441064357757568,
      "end": 87.31999969482422,
      "no_speech_prob": 0.268891304731369,
      "seek": 7760,
      "start": 83.23999786376953,
      "temperature": 0.0,
      "text": " um. And then if we scroll down here you can see, you know, that made a pretty good",
      "tokens": [
        50646,
        1105,
        13,
        400,
        550,
        498,
        321,
        11369,
        760,
        510,
        291,
        393,
        536,
        11,
        291,
        458,
        11,
        300,
        1027,
        257,
        1238,
        665,
        50850
      ]
    },
    {
      "id": 21,
      "avg_logprob": -0.2511487305164337,
      "compression_ratio": 1.8441064357757568,
      "end": 91.44000244140625,
      "no_speech_prob": 0.268891304731369,
      "seek": 7760,
      "start": 87.31999969482422,
      "temperature": 0.0,
      "text": " edit. Then we can scroll down and then you can see it only took out um. So it's",
      "tokens": [
        50850,
        8129,
        13,
        1396,
        321,
        393,
        11369,
        760,
        293,
        550,
        291,
        393,
        536,
        309,
        787,
        1890,
        484,
        1105,
        13,
        407,
        309,
        311,
        51056
      ]
    },
    {
      "id": 22,
      "avg_logprob": -0.2511487305164337,
      "compression_ratio": 1.8441064357757568,
      "end": 95.76000213623047,
      "no_speech_prob": 0.268891304731369,
      "seek": 7760,
      "start": 91.44000244140625,
      "temperature": 0.0,
      "text": " taking another shot. It sees that, you know, after this first run that I made I",
      "tokens": [
        51056,
        1940,
        1071,
        3347,
        13,
        467,
        8194,
        300,
        11,
        291,
        458,
        11,
        934,
        341,
        700,
        1190,
        300,
        286,
        1027,
        286,
        51272
      ]
    },
    {
      "id": 23,
      "avg_logprob": -0.2511487305164337,
      "compression_ratio": 1.8441064357757568,
      "end": 100.27999877929688,
      "no_speech_prob": 0.268891304731369,
      "seek": 7760,
      "start": 95.76000213623047,
      "temperature": 0.0,
      "text": " still need to edit more, okay? So now it's just taking out um. Third compute loop,",
      "tokens": [
        51272,
        920,
        643,
        281,
        8129,
        544,
        11,
        1392,
        30,
        407,
        586,
        309,
        311,
        445,
        1940,
        484,
        1105,
        13,
        12548,
        14722,
        6367,
        11,
        51498
      ]
    },
    {
      "id": 24,
      "avg_logprob": -0.2511487305164337,
      "compression_ratio": 1.8441064357757568,
      "end": 104.76000213623047,
      "no_speech_prob": 0.268891304731369,
      "seek": 7760,
      "start": 100.27999877929688,
      "temperature": 0.0,
      "text": " right? So it has, you know, five shots left. And so now it's telling us that no",
      "tokens": [
        51498,
        558,
        30,
        407,
        309,
        575,
        11,
        291,
        458,
        11,
        1732,
        8305,
        1411,
        13,
        400,
        370,
        586,
        309,
        311,
        3585,
        505,
        300,
        572,
        51722
      ]
    },
    {
      "id": 25,
      "avg_logprob": -0.2335233986377716,
      "compression_ratio": 1.7560137510299683,
      "end": 108.4000015258789,
      "no_speech_prob": 0.000410838722018525,
      "seek": 10476,
      "start": 104.80000305175781,
      "temperature": 0.0,
      "text": " deletions were generated. So it's going to exit the loop. So inside of this",
      "tokens": [
        50366,
        1103,
        302,
        626,
        645,
        10833,
        13,
        407,
        309,
        311,
        516,
        281,
        11043,
        264,
        6367,
        13,
        407,
        1854,
        295,
        341,
        50546
      ]
    },
    {
      "id": 26,
      "avg_logprob": -0.2335233986377716,
      "compression_ratio": 1.7560137510299683,
      "end": 111.87999725341797,
      "no_speech_prob": 0.000410838722018525,
      "seek": 10476,
      "start": 108.4000015258789,
      "temperature": 0.0,
      "text": " prompt, which we'll look at in a moment, there is a condition that says if you're",
      "tokens": [
        50546,
        12391,
        11,
        597,
        321,
        603,
        574,
        412,
        294,
        257,
        1623,
        11,
        456,
        307,
        257,
        4188,
        300,
        1619,
        498,
        291,
        434,
        50720
      ]
    },
    {
      "id": 27,
      "avg_logprob": -0.2335233986377716,
      "compression_ratio": 1.7560137510299683,
      "end": 117.4000015258789,
      "no_speech_prob": 0.000410838722018525,
      "seek": 10476,
      "start": 111.87999725341797,
      "temperature": 0.0,
      "text": " done return an empty list, okay? It created two deletions. Here's the first",
      "tokens": [
        50720,
        1096,
        2736,
        364,
        6707,
        1329,
        11,
        1392,
        30,
        467,
        2942,
        732,
        1103,
        302,
        626,
        13,
        1692,
        311,
        264,
        700,
        50996
      ]
    },
    {
      "id": 28,
      "avg_logprob": -0.2335233986377716,
      "compression_ratio": 1.7560137510299683,
      "end": 121.63999938964844,
      "no_speech_prob": 0.000410838722018525,
      "seek": 10476,
      "start": 117.4000015258789,
      "temperature": 0.0,
      "text": " one with several words in the deletion. And then we have in a follow-up loop",
      "tokens": [
        50996,
        472,
        365,
        2940,
        2283,
        294,
        264,
        1103,
        302,
        313,
        13,
        400,
        550,
        321,
        362,
        294,
        257,
        1524,
        12,
        1010,
        6367,
        51208
      ]
    },
    {
      "id": 29,
      "avg_logprob": -0.2335233986377716,
      "compression_ratio": 1.7560137510299683,
      "end": 125.08000183105469,
      "no_speech_prob": 0.000410838722018525,
      "seek": 10476,
      "start": 121.63999938964844,
      "temperature": 0.0,
      "text": " because it didn't create the perfect edit the first time. You can see here we",
      "tokens": [
        51208,
        570,
        309,
        994,
        380,
        1884,
        264,
        2176,
        8129,
        264,
        700,
        565,
        13,
        509,
        393,
        536,
        510,
        321,
        51380
      ]
    },
    {
      "id": 30,
      "avg_logprob": -0.2335233986377716,
      "compression_ratio": 1.7560137510299683,
      "end": 129.55999755859375,
      "no_speech_prob": 0.000410838722018525,
      "seek": 10476,
      "start": 125.08000183105469,
      "temperature": 0.0,
      "text": " have the words um edited that out. And then we have the final iteration slice.",
      "tokens": [
        51380,
        362,
        264,
        2283,
        1105,
        23016,
        300,
        484,
        13,
        400,
        550,
        321,
        362,
        264,
        2572,
        24784,
        13153,
        13,
        51604
      ]
    },
    {
      "id": 31,
      "avg_logprob": -0.2335233986377716,
      "compression_ratio": 1.7560137510299683,
      "end": 133.0800018310547,
      "no_speech_prob": 0.000410838722018525,
      "seek": 10476,
      "start": 129.55999755859375,
      "temperature": 0.0,
      "text": " So this is what our final slice looks like.",
      "tokens": [
        51604,
        407,
        341,
        307,
        437,
        527,
        2572,
        13153,
        1542,
        411,
        13,
        51780
      ]
    }
  ],
  "language": "english",
  "duration": 131.05999755859375,
  "timestamp": 1742835371.572617
}