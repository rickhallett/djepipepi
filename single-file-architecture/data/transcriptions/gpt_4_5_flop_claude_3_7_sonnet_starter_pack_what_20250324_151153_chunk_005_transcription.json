{
  "audio_path": "data/chunks/gpt_4_5_flop_claude_3_7_sonnet_starter_pack_what_20250324_151153_chunk_005.mp3",
  "text": "thinking with streaming. Let me go ahead and run that again. So I'm going to paste this again and then check this out. So we have the prompt tokens and then you can see the answer getting streamed in here, right? So you can see the live thinking process of our model and now here's the response bam and there's the output. Let's go ahead and check it against the actual answer 489.24. That's the exact correct answer, right? So this is really cool, right? We have both the final answer but we can also see how our model got there and you can see this one actually took up some thinking tokens. This was actually a math problem that required some real thought from the model. So you can see here the output tokens was just 60 but it took 232 tokens to actually think through the answer and give you a solid response. So to be 100% clear this is what you know these powerful reasoning models are all about. And now you have full control over the scale of intelligence of the model with the thinking budget tokens. Let's just go ahead and run another one. This is a simple you know logical puzzle. A says that B is lying. B says that C is lying. C says that A and B are both lying. Who is telling the truth, right? So we can give Claude 3.7 some time to think here and we can see that thinking getting streamed in. You know really working through this problem with 2,000 tokens and now we have the answer. So now it's formatting the actual output for us. Bam! Therefore B is the only one telling the truth and you know I've worked through this problem. B is telling the truth. So this is the correct answer. Really powerful, really capable and you can see here again I just love the visibility here and I love the fact that we can peer into the model and you know this is something that Anthropic mentioned is going to be really important for model safety moving forward and I think it's just a great approach for that, right? When you can see the model thinking it's a lot easier to understand how it's deriving the actual answer for you, right? So this is awesome. I think you can kind of get a good idea of the primary capabilities of the extended thinking. Let's go ahead and continue working through some examples but let's fire up Claude first. If we clear the terminal...",
  "segments": [
    {
      "id": 0,
      "avg_logprob": -0.2263789027929306,
      "compression_ratio": 1.8289963006973267,
      "end": 3.8399999141693115,
      "no_speech_prob": 0.23639677464962006,
      "seek": 0,
      "start": 0.0,
      "temperature": 0.0,
      "text": " thinking with streaming. Let me go ahead and run that again. So I'm going to paste this again and",
      "tokens": [
        50364,
        1953,
        365,
        11791,
        13,
        961,
        385,
        352,
        2286,
        293,
        1190,
        300,
        797,
        13,
        407,
        286,
        478,
        516,
        281,
        9163,
        341,
        797,
        293,
        50556
      ]
    },
    {
      "id": 1,
      "avg_logprob": -0.2263789027929306,
      "compression_ratio": 1.8289963006973267,
      "end": 8.0,
      "no_speech_prob": 0.23639677464962006,
      "seek": 0,
      "start": 3.8399999141693115,
      "temperature": 0.0,
      "text": " then check this out. So we have the prompt tokens and then you can see the answer getting streamed",
      "tokens": [
        50556,
        550,
        1520,
        341,
        484,
        13,
        407,
        321,
        362,
        264,
        12391,
        22667,
        293,
        550,
        291,
        393,
        536,
        264,
        1867,
        1242,
        4309,
        292,
        50764
      ]
    },
    {
      "id": 2,
      "avg_logprob": -0.2263789027929306,
      "compression_ratio": 1.8289963006973267,
      "end": 12.880000114440918,
      "no_speech_prob": 0.23639677464962006,
      "seek": 0,
      "start": 8.0,
      "temperature": 0.0,
      "text": " in here, right? So you can see the live thinking process of our model and now here's the response",
      "tokens": [
        50764,
        294,
        510,
        11,
        558,
        30,
        407,
        291,
        393,
        536,
        264,
        1621,
        1953,
        1399,
        295,
        527,
        2316,
        293,
        586,
        510,
        311,
        264,
        4134,
        51008
      ]
    },
    {
      "id": 3,
      "avg_logprob": -0.2263789027929306,
      "compression_ratio": 1.8289963006973267,
      "end": 20.239999771118164,
      "no_speech_prob": 0.23639677464962006,
      "seek": 0,
      "start": 12.880000114440918,
      "temperature": 0.0,
      "text": " bam and there's the output. Let's go ahead and check it against the actual answer 489.24. That's",
      "tokens": [
        51008,
        18132,
        293,
        456,
        311,
        264,
        5598,
        13,
        961,
        311,
        352,
        2286,
        293,
        1520,
        309,
        1970,
        264,
        3539,
        1867,
        11174,
        24,
        13,
        7911,
        13,
        663,
        311,
        51376
      ]
    },
    {
      "id": 4,
      "avg_logprob": -0.2263789027929306,
      "compression_ratio": 1.8289963006973267,
      "end": 26.0,
      "no_speech_prob": 0.23639677464962006,
      "seek": 0,
      "start": 20.239999771118164,
      "temperature": 0.0,
      "text": " the exact correct answer, right? So this is really cool, right? We have both the final answer but we",
      "tokens": [
        51376,
        264,
        1900,
        3006,
        1867,
        11,
        558,
        30,
        407,
        341,
        307,
        534,
        1627,
        11,
        558,
        30,
        492,
        362,
        1293,
        264,
        2572,
        1867,
        457,
        321,
        51664
      ]
    },
    {
      "id": 5,
      "avg_logprob": -0.19080403447151184,
      "compression_ratio": 1.6767241954803467,
      "end": 32.720001220703125,
      "no_speech_prob": 0.010488685220479965,
      "seek": 2600,
      "start": 26.0,
      "temperature": 0.0,
      "text": " can also see how our model got there and you can see this one actually took up some thinking tokens.",
      "tokens": [
        50364,
        393,
        611,
        536,
        577,
        527,
        2316,
        658,
        456,
        293,
        291,
        393,
        536,
        341,
        472,
        767,
        1890,
        493,
        512,
        1953,
        22667,
        13,
        50700
      ]
    },
    {
      "id": 6,
      "avg_logprob": -0.19080403447151184,
      "compression_ratio": 1.6767241954803467,
      "end": 37.52000045776367,
      "no_speech_prob": 0.010488685220479965,
      "seek": 2600,
      "start": 32.720001220703125,
      "temperature": 0.0,
      "text": " This was actually a math problem that required some real thought from the model. So you can see",
      "tokens": [
        50700,
        639,
        390,
        767,
        257,
        5221,
        1154,
        300,
        4739,
        512,
        957,
        1194,
        490,
        264,
        2316,
        13,
        407,
        291,
        393,
        536,
        50940
      ]
    },
    {
      "id": 7,
      "avg_logprob": -0.19080403447151184,
      "compression_ratio": 1.6767241954803467,
      "end": 44.31999969482422,
      "no_speech_prob": 0.010488685220479965,
      "seek": 2600,
      "start": 37.52000045776367,
      "temperature": 0.0,
      "text": " here the output tokens was just 60 but it took 232 tokens to actually think through the answer and",
      "tokens": [
        50940,
        510,
        264,
        5598,
        22667,
        390,
        445,
        4060,
        457,
        309,
        1890,
        6673,
        17,
        22667,
        281,
        767,
        519,
        807,
        264,
        1867,
        293,
        51280
      ]
    },
    {
      "id": 8,
      "avg_logprob": -0.19080403447151184,
      "compression_ratio": 1.6767241954803467,
      "end": 50.400001525878906,
      "no_speech_prob": 0.010488685220479965,
      "seek": 2600,
      "start": 44.31999969482422,
      "temperature": 0.0,
      "text": " give you a solid response. So to be 100% clear this is what you know these powerful reasoning",
      "tokens": [
        51280,
        976,
        291,
        257,
        5100,
        4134,
        13,
        407,
        281,
        312,
        2319,
        4,
        1850,
        341,
        307,
        437,
        291,
        458,
        613,
        4005,
        21577,
        51584
      ]
    },
    {
      "id": 9,
      "avg_logprob": -0.18242187798023224,
      "compression_ratio": 1.6654930114746094,
      "end": 56.880001068115234,
      "no_speech_prob": 0.07263334095478058,
      "seek": 5040,
      "start": 50.400001525878906,
      "temperature": 0.0,
      "text": " models are all about. And now you have full control over the scale of intelligence of the",
      "tokens": [
        50364,
        5245,
        366,
        439,
        466,
        13,
        400,
        586,
        291,
        362,
        1577,
        1969,
        670,
        264,
        4373,
        295,
        7599,
        295,
        264,
        50688
      ]
    },
    {
      "id": 10,
      "avg_logprob": -0.18242187798023224,
      "compression_ratio": 1.6654930114746094,
      "end": 62.15999984741211,
      "no_speech_prob": 0.07263334095478058,
      "seek": 5040,
      "start": 56.880001068115234,
      "temperature": 0.0,
      "text": " model with the thinking budget tokens. Let's just go ahead and run another one. This is a simple",
      "tokens": [
        50688,
        2316,
        365,
        264,
        1953,
        4706,
        22667,
        13,
        961,
        311,
        445,
        352,
        2286,
        293,
        1190,
        1071,
        472,
        13,
        639,
        307,
        257,
        2199,
        50952
      ]
    },
    {
      "id": 11,
      "avg_logprob": -0.18242187798023224,
      "compression_ratio": 1.6654930114746094,
      "end": 69.12000274658203,
      "no_speech_prob": 0.07263334095478058,
      "seek": 5040,
      "start": 62.15999984741211,
      "temperature": 0.0,
      "text": " you know logical puzzle. A says that B is lying. B says that C is lying. C says that A and B are",
      "tokens": [
        50952,
        291,
        458,
        14978,
        12805,
        13,
        316,
        1619,
        300,
        363,
        307,
        8493,
        13,
        363,
        1619,
        300,
        383,
        307,
        8493,
        13,
        383,
        1619,
        300,
        316,
        293,
        363,
        366,
        51300
      ]
    },
    {
      "id": 12,
      "avg_logprob": -0.18242187798023224,
      "compression_ratio": 1.6654930114746094,
      "end": 75.76000213623047,
      "no_speech_prob": 0.07263334095478058,
      "seek": 5040,
      "start": 69.12000274658203,
      "temperature": 0.0,
      "text": " both lying. Who is telling the truth, right? So we can give Claude 3.7 some time to think here",
      "tokens": [
        51300,
        1293,
        8493,
        13,
        2102,
        307,
        3585,
        264,
        3494,
        11,
        558,
        30,
        407,
        321,
        393,
        976,
        12947,
        2303,
        805,
        13,
        22,
        512,
        565,
        281,
        519,
        510,
        51632
      ]
    },
    {
      "id": 13,
      "avg_logprob": -0.18242187798023224,
      "compression_ratio": 1.6654930114746094,
      "end": 80.16000366210938,
      "no_speech_prob": 0.07263334095478058,
      "seek": 5040,
      "start": 75.76000213623047,
      "temperature": 0.0,
      "text": " and we can see that thinking getting streamed in. You know really working through this problem",
      "tokens": [
        51632,
        293,
        321,
        393,
        536,
        300,
        1953,
        1242,
        4309,
        292,
        294,
        13,
        509,
        458,
        534,
        1364,
        807,
        341,
        1154,
        51852
      ]
    },
    {
      "id": 14,
      "avg_logprob": -0.21458333730697632,
      "compression_ratio": 1.7968254089355469,
      "end": 85.44000244140625,
      "no_speech_prob": 0.0004044684465043247,
      "seek": 8016,
      "start": 80.16000366210938,
      "temperature": 0.0,
      "text": " with 2,000 tokens and now we have the answer. So now it's formatting the actual output for us.",
      "tokens": [
        50364,
        365,
        568,
        11,
        1360,
        22667,
        293,
        586,
        321,
        362,
        264,
        1867,
        13,
        407,
        586,
        309,
        311,
        39366,
        264,
        3539,
        5598,
        337,
        505,
        13,
        50628
      ]
    },
    {
      "id": 15,
      "avg_logprob": -0.21458333730697632,
      "compression_ratio": 1.7968254089355469,
      "end": 90.08000183105469,
      "no_speech_prob": 0.0004044684465043247,
      "seek": 8016,
      "start": 85.44000244140625,
      "temperature": 0.0,
      "text": " Bam! Therefore B is the only one telling the truth and you know I've worked through this problem.",
      "tokens": [
        50628,
        26630,
        0,
        7504,
        363,
        307,
        264,
        787,
        472,
        3585,
        264,
        3494,
        293,
        291,
        458,
        286,
        600,
        2732,
        807,
        341,
        1154,
        13,
        50860
      ]
    },
    {
      "id": 16,
      "avg_logprob": -0.21458333730697632,
      "compression_ratio": 1.7968254089355469,
      "end": 95.19999694824219,
      "no_speech_prob": 0.0004044684465043247,
      "seek": 8016,
      "start": 90.95999908447266,
      "temperature": 0.0,
      "text": " B is telling the truth. So this is the correct answer. Really powerful, really capable and you",
      "tokens": [
        50904,
        363,
        307,
        3585,
        264,
        3494,
        13,
        407,
        341,
        307,
        264,
        3006,
        1867,
        13,
        4083,
        4005,
        11,
        534,
        8189,
        293,
        291,
        51116
      ]
    },
    {
      "id": 17,
      "avg_logprob": -0.21458333730697632,
      "compression_ratio": 1.7968254089355469,
      "end": 100.95999908447266,
      "no_speech_prob": 0.0004044684465043247,
      "seek": 8016,
      "start": 95.19999694824219,
      "temperature": 0.0,
      "text": " can see here again I just love the visibility here and I love the fact that we can peer into",
      "tokens": [
        51116,
        393,
        536,
        510,
        797,
        286,
        445,
        959,
        264,
        19883,
        510,
        293,
        286,
        959,
        264,
        1186,
        300,
        321,
        393,
        15108,
        666,
        51404
      ]
    },
    {
      "id": 18,
      "avg_logprob": -0.21458333730697632,
      "compression_ratio": 1.7968254089355469,
      "end": 104.4000015258789,
      "no_speech_prob": 0.0004044684465043247,
      "seek": 8016,
      "start": 100.95999908447266,
      "temperature": 0.0,
      "text": " the model and you know this is something that Anthropic mentioned is going to be really",
      "tokens": [
        51404,
        264,
        2316,
        293,
        291,
        458,
        341,
        307,
        746,
        300,
        12727,
        39173,
        2835,
        307,
        516,
        281,
        312,
        534,
        51576
      ]
    },
    {
      "id": 19,
      "avg_logprob": -0.21458333730697632,
      "compression_ratio": 1.7968254089355469,
      "end": 108.23999786376953,
      "no_speech_prob": 0.0004044684465043247,
      "seek": 8016,
      "start": 104.4000015258789,
      "temperature": 0.0,
      "text": " important for model safety moving forward and I think it's just a great approach for that, right?",
      "tokens": [
        51576,
        1021,
        337,
        2316,
        4514,
        2684,
        2128,
        293,
        286,
        519,
        309,
        311,
        445,
        257,
        869,
        3109,
        337,
        300,
        11,
        558,
        30,
        51768
      ]
    },
    {
      "id": 20,
      "avg_logprob": -0.1997971385717392,
      "compression_ratio": 1.559999942779541,
      "end": 113.5199966430664,
      "no_speech_prob": 3.705188282765448e-05,
      "seek": 10824,
      "start": 108.23999786376953,
      "temperature": 0.0,
      "text": " When you can see the model thinking it's a lot easier to understand how it's deriving the actual",
      "tokens": [
        50364,
        1133,
        291,
        393,
        536,
        264,
        2316,
        1953,
        309,
        311,
        257,
        688,
        3571,
        281,
        1223,
        577,
        309,
        311,
        1163,
        2123,
        264,
        3539,
        50628
      ]
    },
    {
      "id": 21,
      "avg_logprob": -0.1997971385717392,
      "compression_ratio": 1.559999942779541,
      "end": 118.95999908447266,
      "no_speech_prob": 3.705188282765448e-05,
      "seek": 10824,
      "start": 113.5199966430664,
      "temperature": 0.0,
      "text": " answer for you, right? So this is awesome. I think you can kind of get a good idea of the",
      "tokens": [
        50628,
        1867,
        337,
        291,
        11,
        558,
        30,
        407,
        341,
        307,
        3476,
        13,
        286,
        519,
        291,
        393,
        733,
        295,
        483,
        257,
        665,
        1558,
        295,
        264,
        50900
      ]
    },
    {
      "id": 22,
      "avg_logprob": -0.1997971385717392,
      "compression_ratio": 1.559999942779541,
      "end": 123.19999694824219,
      "no_speech_prob": 3.705188282765448e-05,
      "seek": 10824,
      "start": 118.95999908447266,
      "temperature": 0.0,
      "text": " primary capabilities of the extended thinking. Let's go ahead and continue",
      "tokens": [
        50900,
        6194,
        10862,
        295,
        264,
        10913,
        1953,
        13,
        961,
        311,
        352,
        2286,
        293,
        2354,
        51112
      ]
    },
    {
      "id": 23,
      "avg_logprob": -0.1997971385717392,
      "compression_ratio": 1.559999942779541,
      "end": 126.63999938964844,
      "no_speech_prob": 3.705188282765448e-05,
      "seek": 10824,
      "start": 123.19999694824219,
      "temperature": 0.0,
      "text": " working through some examples but let's fire up Claude first.",
      "tokens": [
        51112,
        1364,
        807,
        512,
        5110,
        457,
        718,
        311,
        2610,
        493,
        12947,
        2303,
        700,
        13,
        51284
      ]
    },
    {
      "id": 24,
      "avg_logprob": -0.1997971385717392,
      "compression_ratio": 1.559999942779541,
      "end": 130.9600067138672,
      "no_speech_prob": 3.705188282765448e-05,
      "seek": 10824,
      "start": 130.0800018310547,
      "temperature": 0.0,
      "text": " If we clear the terminal...",
      "tokens": [
        51456,
        759,
        321,
        1850,
        264,
        14709,
        485,
        51500
      ]
    }
  ],
  "language": "english",
  "duration": 131.05999755859375,
  "timestamp": 1742835607.026057
}