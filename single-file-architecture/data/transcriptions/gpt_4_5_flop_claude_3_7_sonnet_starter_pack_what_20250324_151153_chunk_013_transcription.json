{
  "audio_path": "data/chunks/gpt_4_5_flop_claude_3_7_sonnet_starter_pack_what_20250324_151153_chunk_013.mp3",
  "text": "and I'm enabling extended output, right? 13K tokens out. This is gonna go above the 8K output that Claude has here, right? So we need to boost this up to a potential 128K out. So we can copy this and fire this off. This is gonna be really cool. So here it is. Here's our live stream, right? Our live stream of the thinking process. You can see Claude thinking and now it has the answer. And now it's just going to run through these tokens. All right, so what did we ask for here? We said 10,000 words. So you can see there, you know, 500. This is gonna just keep ticking up. It's scrolling out of view here. So we're just gonna wait for the response words to come in here. But this is pretty incredible, right? We're getting insane model instruction following. Again, we have other examples here. Be careful when you fire these off. This will, you know, hit the API and likely, you know, consume the tokens it takes to respond with the prompt, right? The extended output token flag really enables you to push the output of the model far beyond, you know, any model we've really seen before. So you can see here, it's still pushing, right? 3K tokens, you can see it's about 10% of the total available token usage. That's great. You can see it only used, you know, 300 tokens to think far below the thinking budget we gave it of what did we give it here? 8,000, yeah, just, this is not a reasoning problem to solve, right? So we could have easily dropped this down to 1K. While this is running, you know, let me give you my quick guide here on intelligence, right? How much intelligence do you really need to solve any problem? A big takeaway here is that we now have fine grain control over the reasoning capabilities of this powerful model. So this is the way I like to slice it up. Basically, we have extra small intelligence all the way up to 4XL. You basically will never need this. You basically will never need to push the model this far, this hard. It is a lot more likely that you'll push into these ranges, you know, given that you're solving an interesting enough problem where the model actually needs to think. As, and.",
  "segments": [
    {
      "id": 0,
      "avg_logprob": -0.24817708134651184,
      "compression_ratio": 1.6566523313522339,
      "end": 3.4800000190734863,
      "no_speech_prob": 0.11433182656764984,
      "seek": 0,
      "start": 0.0,
      "temperature": 0.0,
      "text": " and I'm enabling extended output, right?",
      "tokens": [
        50364,
        293,
        286,
        478,
        23148,
        10913,
        5598,
        11,
        558,
        30,
        50538
      ]
    },
    {
      "id": 1,
      "avg_logprob": -0.24817708134651184,
      "compression_ratio": 1.6566523313522339,
      "end": 5.0,
      "no_speech_prob": 0.11433182656764984,
      "seek": 0,
      "start": 3.4800000190734863,
      "temperature": 0.0,
      "text": " 13K tokens out.",
      "tokens": [
        50538,
        3705,
        42,
        22667,
        484,
        13,
        50614
      ]
    },
    {
      "id": 2,
      "avg_logprob": -0.24817708134651184,
      "compression_ratio": 1.6566523313522339,
      "end": 9.239999771118164,
      "no_speech_prob": 0.11433182656764984,
      "seek": 0,
      "start": 5.0,
      "temperature": 0.0,
      "text": " This is gonna go above the 8K output",
      "tokens": [
        50614,
        639,
        307,
        799,
        352,
        3673,
        264,
        1649,
        42,
        5598,
        50826
      ]
    },
    {
      "id": 3,
      "avg_logprob": -0.24817708134651184,
      "compression_ratio": 1.6566523313522339,
      "end": 11.680000305175781,
      "no_speech_prob": 0.11433182656764984,
      "seek": 0,
      "start": 9.239999771118164,
      "temperature": 0.0,
      "text": " that Claude has here, right?",
      "tokens": [
        50826,
        300,
        12947,
        2303,
        575,
        510,
        11,
        558,
        30,
        50948
      ]
    },
    {
      "id": 4,
      "avg_logprob": -0.24817708134651184,
      "compression_ratio": 1.6566523313522339,
      "end": 16.68000030517578,
      "no_speech_prob": 0.11433182656764984,
      "seek": 0,
      "start": 11.680000305175781,
      "temperature": 0.0,
      "text": " So we need to boost this up to a potential 128K out.",
      "tokens": [
        50948,
        407,
        321,
        643,
        281,
        9194,
        341,
        493,
        281,
        257,
        3995,
        29810,
        42,
        484,
        13,
        51198
      ]
    },
    {
      "id": 5,
      "avg_logprob": -0.24817708134651184,
      "compression_ratio": 1.6566523313522339,
      "end": 19.31999969482422,
      "no_speech_prob": 0.11433182656764984,
      "seek": 0,
      "start": 17.280000686645508,
      "temperature": 0.0,
      "text": " So we can copy this and fire this off.",
      "tokens": [
        51228,
        407,
        321,
        393,
        5055,
        341,
        293,
        2610,
        341,
        766,
        13,
        51330
      ]
    },
    {
      "id": 6,
      "avg_logprob": -0.24817708134651184,
      "compression_ratio": 1.6566523313522339,
      "end": 20.15999984741211,
      "no_speech_prob": 0.11433182656764984,
      "seek": 0,
      "start": 19.31999969482422,
      "temperature": 0.0,
      "text": " This is gonna be really cool.",
      "tokens": [
        51330,
        639,
        307,
        799,
        312,
        534,
        1627,
        13,
        51372
      ]
    },
    {
      "id": 7,
      "avg_logprob": -0.24817708134651184,
      "compression_ratio": 1.6566523313522339,
      "end": 21.0,
      "no_speech_prob": 0.11433182656764984,
      "seek": 0,
      "start": 20.15999984741211,
      "temperature": 0.0,
      "text": " So here it is.",
      "tokens": [
        51372,
        407,
        510,
        309,
        307,
        13,
        51414
      ]
    },
    {
      "id": 8,
      "avg_logprob": -0.24817708134651184,
      "compression_ratio": 1.6566523313522339,
      "end": 24.15999984741211,
      "no_speech_prob": 0.11433182656764984,
      "seek": 0,
      "start": 21.0,
      "temperature": 0.0,
      "text": " Here's our live stream, right?",
      "tokens": [
        51414,
        1692,
        311,
        527,
        1621,
        4309,
        11,
        558,
        30,
        51572
      ]
    },
    {
      "id": 9,
      "avg_logprob": -0.24817708134651184,
      "compression_ratio": 1.6566523313522339,
      "end": 26.299999237060547,
      "no_speech_prob": 0.11433182656764984,
      "seek": 0,
      "start": 24.15999984741211,
      "temperature": 0.0,
      "text": " Our live stream of the thinking process.",
      "tokens": [
        51572,
        2621,
        1621,
        4309,
        295,
        264,
        1953,
        1399,
        13,
        51679
      ]
    },
    {
      "id": 10,
      "avg_logprob": -0.24817708134651184,
      "compression_ratio": 1.6566523313522339,
      "end": 29.479999542236328,
      "no_speech_prob": 0.11433182656764984,
      "seek": 0,
      "start": 26.299999237060547,
      "temperature": 0.0,
      "text": " You can see Claude thinking and now it has the answer.",
      "tokens": [
        51679,
        509,
        393,
        536,
        12947,
        2303,
        1953,
        293,
        586,
        309,
        575,
        264,
        1867,
        13,
        51838
      ]
    },
    {
      "id": 11,
      "avg_logprob": -0.20154310762882233,
      "compression_ratio": 1.5780141353607178,
      "end": 33.13999938964844,
      "no_speech_prob": 2.5071050913538784e-05,
      "seek": 2948,
      "start": 29.479999542236328,
      "temperature": 0.0,
      "text": " And now it's just going to run through these tokens.",
      "tokens": [
        50364,
        400,
        586,
        309,
        311,
        445,
        516,
        281,
        1190,
        807,
        613,
        22667,
        13,
        50547
      ]
    },
    {
      "id": 12,
      "avg_logprob": -0.20154310762882233,
      "compression_ratio": 1.5780141353607178,
      "end": 35.86000061035156,
      "no_speech_prob": 2.5071050913538784e-05,
      "seek": 2948,
      "start": 33.13999938964844,
      "temperature": 0.0,
      "text": " All right, so what did we ask for here?",
      "tokens": [
        50547,
        1057,
        558,
        11,
        370,
        437,
        630,
        321,
        1029,
        337,
        510,
        30,
        50683
      ]
    },
    {
      "id": 13,
      "avg_logprob": -0.20154310762882233,
      "compression_ratio": 1.5780141353607178,
      "end": 37.34000015258789,
      "no_speech_prob": 2.5071050913538784e-05,
      "seek": 2948,
      "start": 35.86000061035156,
      "temperature": 0.0,
      "text": " We said 10,000 words.",
      "tokens": [
        50683,
        492,
        848,
        1266,
        11,
        1360,
        2283,
        13,
        50757
      ]
    },
    {
      "id": 14,
      "avg_logprob": -0.20154310762882233,
      "compression_ratio": 1.5780141353607178,
      "end": 41.02000045776367,
      "no_speech_prob": 2.5071050913538784e-05,
      "seek": 2948,
      "start": 37.34000015258789,
      "temperature": 0.0,
      "text": " So you can see there, you know, 500.",
      "tokens": [
        50757,
        407,
        291,
        393,
        536,
        456,
        11,
        291,
        458,
        11,
        5923,
        13,
        50941
      ]
    },
    {
      "id": 15,
      "avg_logprob": -0.20154310762882233,
      "compression_ratio": 1.5780141353607178,
      "end": 42.29999923706055,
      "no_speech_prob": 2.5071050913538784e-05,
      "seek": 2948,
      "start": 41.02000045776367,
      "temperature": 0.0,
      "text": " This is gonna just keep ticking up.",
      "tokens": [
        50941,
        639,
        307,
        799,
        445,
        1066,
        33999,
        493,
        13,
        51005
      ]
    },
    {
      "id": 16,
      "avg_logprob": -0.20154310762882233,
      "compression_ratio": 1.5780141353607178,
      "end": 44.880001068115234,
      "no_speech_prob": 2.5071050913538784e-05,
      "seek": 2948,
      "start": 42.29999923706055,
      "temperature": 0.0,
      "text": " It's scrolling out of view here.",
      "tokens": [
        51005,
        467,
        311,
        29053,
        484,
        295,
        1910,
        510,
        13,
        51134
      ]
    },
    {
      "id": 17,
      "avg_logprob": -0.20154310762882233,
      "compression_ratio": 1.5780141353607178,
      "end": 46.02000045776367,
      "no_speech_prob": 2.5071050913538784e-05,
      "seek": 2948,
      "start": 44.880001068115234,
      "temperature": 0.0,
      "text": " So we're just gonna wait",
      "tokens": [
        51134,
        407,
        321,
        434,
        445,
        799,
        1699,
        51191
      ]
    },
    {
      "id": 18,
      "avg_logprob": -0.20154310762882233,
      "compression_ratio": 1.5780141353607178,
      "end": 48.81999969482422,
      "no_speech_prob": 2.5071050913538784e-05,
      "seek": 2948,
      "start": 46.02000045776367,
      "temperature": 0.0,
      "text": " for the response words to come in here.",
      "tokens": [
        51191,
        337,
        264,
        4134,
        2283,
        281,
        808,
        294,
        510,
        13,
        51331
      ]
    },
    {
      "id": 19,
      "avg_logprob": -0.20154310762882233,
      "compression_ratio": 1.5780141353607178,
      "end": 50.380001068115234,
      "no_speech_prob": 2.5071050913538784e-05,
      "seek": 2948,
      "start": 48.81999969482422,
      "temperature": 0.0,
      "text": " But this is pretty incredible, right?",
      "tokens": [
        51331,
        583,
        341,
        307,
        1238,
        4651,
        11,
        558,
        30,
        51409
      ]
    },
    {
      "id": 20,
      "avg_logprob": -0.20154310762882233,
      "compression_ratio": 1.5780141353607178,
      "end": 54.540000915527344,
      "no_speech_prob": 2.5071050913538784e-05,
      "seek": 2948,
      "start": 50.380001068115234,
      "temperature": 0.0,
      "text": " We're getting insane model instruction following.",
      "tokens": [
        51409,
        492,
        434,
        1242,
        10838,
        2316,
        10951,
        3480,
        13,
        51617
      ]
    },
    {
      "id": 21,
      "avg_logprob": -0.20154310762882233,
      "compression_ratio": 1.5780141353607178,
      "end": 56.13999938964844,
      "no_speech_prob": 2.5071050913538784e-05,
      "seek": 2948,
      "start": 54.540000915527344,
      "temperature": 0.0,
      "text": " Again, we have other examples here.",
      "tokens": [
        51617,
        3764,
        11,
        321,
        362,
        661,
        5110,
        510,
        13,
        51697
      ]
    },
    {
      "id": 22,
      "avg_logprob": -0.20154310762882233,
      "compression_ratio": 1.5780141353607178,
      "end": 57.540000915527344,
      "no_speech_prob": 2.5071050913538784e-05,
      "seek": 2948,
      "start": 56.13999938964844,
      "temperature": 0.0,
      "text": " Be careful when you fire these off.",
      "tokens": [
        51697,
        879,
        5026,
        562,
        291,
        2610,
        613,
        766,
        13,
        51767
      ]
    },
    {
      "id": 23,
      "avg_logprob": -0.22931812703609467,
      "compression_ratio": 1.7460317611694336,
      "end": 60.31999969482422,
      "no_speech_prob": 0.0010649508330971003,
      "seek": 5754,
      "start": 57.560001373291016,
      "temperature": 0.0,
      "text": " This will, you know, hit the API",
      "tokens": [
        50365,
        639,
        486,
        11,
        291,
        458,
        11,
        2045,
        264,
        9362,
        50503
      ]
    },
    {
      "id": 24,
      "avg_logprob": -0.22931812703609467,
      "compression_ratio": 1.7460317611694336,
      "end": 62.47999954223633,
      "no_speech_prob": 0.0010649508330971003,
      "seek": 5754,
      "start": 60.31999969482422,
      "temperature": 0.0,
      "text": " and likely, you know, consume the tokens it takes",
      "tokens": [
        50503,
        293,
        3700,
        11,
        291,
        458,
        11,
        14732,
        264,
        22667,
        309,
        2516,
        50611
      ]
    },
    {
      "id": 25,
      "avg_logprob": -0.22931812703609467,
      "compression_ratio": 1.7460317611694336,
      "end": 64.55999755859375,
      "no_speech_prob": 0.0010649508330971003,
      "seek": 5754,
      "start": 62.47999954223633,
      "temperature": 0.0,
      "text": " to respond with the prompt, right?",
      "tokens": [
        50611,
        281,
        4196,
        365,
        264,
        12391,
        11,
        558,
        30,
        50715
      ]
    },
    {
      "id": 26,
      "avg_logprob": -0.22931812703609467,
      "compression_ratio": 1.7460317611694336,
      "end": 68.68000030517578,
      "no_speech_prob": 0.0010649508330971003,
      "seek": 5754,
      "start": 64.55999755859375,
      "temperature": 0.0,
      "text": " The extended output token flag really enables you",
      "tokens": [
        50715,
        440,
        10913,
        5598,
        14862,
        7166,
        534,
        17077,
        291,
        50921
      ]
    },
    {
      "id": 27,
      "avg_logprob": -0.22931812703609467,
      "compression_ratio": 1.7460317611694336,
      "end": 71.94000244140625,
      "no_speech_prob": 0.0010649508330971003,
      "seek": 5754,
      "start": 68.68000030517578,
      "temperature": 0.0,
      "text": " to push the output of the model far beyond, you know,",
      "tokens": [
        50921,
        281,
        2944,
        264,
        5598,
        295,
        264,
        2316,
        1400,
        4399,
        11,
        291,
        458,
        11,
        51084
      ]
    },
    {
      "id": 28,
      "avg_logprob": -0.22931812703609467,
      "compression_ratio": 1.7460317611694336,
      "end": 73.76000213623047,
      "no_speech_prob": 0.0010649508330971003,
      "seek": 5754,
      "start": 71.94000244140625,
      "temperature": 0.0,
      "text": " any model we've really seen before.",
      "tokens": [
        51084,
        604,
        2316,
        321,
        600,
        534,
        1612,
        949,
        13,
        51175
      ]
    },
    {
      "id": 29,
      "avg_logprob": -0.22931812703609467,
      "compression_ratio": 1.7460317611694336,
      "end": 76.83999633789062,
      "no_speech_prob": 0.0010649508330971003,
      "seek": 5754,
      "start": 73.76000213623047,
      "temperature": 0.0,
      "text": " So you can see here, it's still pushing, right?",
      "tokens": [
        51175,
        407,
        291,
        393,
        536,
        510,
        11,
        309,
        311,
        920,
        7380,
        11,
        558,
        30,
        51329
      ]
    },
    {
      "id": 30,
      "avg_logprob": -0.22931812703609467,
      "compression_ratio": 1.7460317611694336,
      "end": 79.87999725341797,
      "no_speech_prob": 0.0010649508330971003,
      "seek": 5754,
      "start": 76.83999633789062,
      "temperature": 0.0,
      "text": " 3K tokens, you can see it's about 10%",
      "tokens": [
        51329,
        805,
        42,
        22667,
        11,
        291,
        393,
        536,
        309,
        311,
        466,
        1266,
        4,
        51481
      ]
    },
    {
      "id": 31,
      "avg_logprob": -0.22931812703609467,
      "compression_ratio": 1.7460317611694336,
      "end": 83.27999877929688,
      "no_speech_prob": 0.0010649508330971003,
      "seek": 5754,
      "start": 79.87999725341797,
      "temperature": 0.0,
      "text": " of the total available token usage.",
      "tokens": [
        51481,
        295,
        264,
        3217,
        2435,
        14862,
        14924,
        13,
        51651
      ]
    },
    {
      "id": 32,
      "avg_logprob": -0.22931812703609467,
      "compression_ratio": 1.7460317611694336,
      "end": 84.12000274658203,
      "no_speech_prob": 0.0010649508330971003,
      "seek": 5754,
      "start": 83.27999877929688,
      "temperature": 0.0,
      "text": " That's great.",
      "tokens": [
        51651,
        663,
        311,
        869,
        13,
        51693
      ]
    },
    {
      "id": 33,
      "avg_logprob": -0.22931812703609467,
      "compression_ratio": 1.7460317611694336,
      "end": 86.12000274658203,
      "no_speech_prob": 0.0010649508330971003,
      "seek": 5754,
      "start": 84.12000274658203,
      "temperature": 0.0,
      "text": " You can see it only used, you know, 300 tokens",
      "tokens": [
        51693,
        509,
        393,
        536,
        309,
        787,
        1143,
        11,
        291,
        458,
        11,
        6641,
        22667,
        51793
      ]
    },
    {
      "id": 34,
      "avg_logprob": -0.21291667222976685,
      "compression_ratio": 1.725000023841858,
      "end": 89.5999984741211,
      "no_speech_prob": 0.0006666972767561674,
      "seek": 8612,
      "start": 86.12000274658203,
      "temperature": 0.0,
      "text": " to think far below the thinking budget we gave it",
      "tokens": [
        50364,
        281,
        519,
        1400,
        2507,
        264,
        1953,
        4706,
        321,
        2729,
        309,
        50538
      ]
    },
    {
      "id": 35,
      "avg_logprob": -0.21291667222976685,
      "compression_ratio": 1.725000023841858,
      "end": 91.44000244140625,
      "no_speech_prob": 0.0006666972767561674,
      "seek": 8612,
      "start": 89.5999984741211,
      "temperature": 0.0,
      "text": " of what did we give it here?",
      "tokens": [
        50538,
        295,
        437,
        630,
        321,
        976,
        309,
        510,
        30,
        50630
      ]
    },
    {
      "id": 36,
      "avg_logprob": -0.21291667222976685,
      "compression_ratio": 1.725000023841858,
      "end": 93.76000213623047,
      "no_speech_prob": 0.0006666972767561674,
      "seek": 8612,
      "start": 91.44000244140625,
      "temperature": 0.0,
      "text": " 8,000, yeah, just, this is not a reasoning problem",
      "tokens": [
        50630,
        1649,
        11,
        1360,
        11,
        1338,
        11,
        445,
        11,
        341,
        307,
        406,
        257,
        21577,
        1154,
        50746
      ]
    },
    {
      "id": 37,
      "avg_logprob": -0.21291667222976685,
      "compression_ratio": 1.725000023841858,
      "end": 94.5999984741211,
      "no_speech_prob": 0.0006666972767561674,
      "seek": 8612,
      "start": 93.76000213623047,
      "temperature": 0.0,
      "text": " to solve, right?",
      "tokens": [
        50746,
        281,
        5039,
        11,
        558,
        30,
        50788
      ]
    },
    {
      "id": 38,
      "avg_logprob": -0.21291667222976685,
      "compression_ratio": 1.725000023841858,
      "end": 97.04000091552734,
      "no_speech_prob": 0.0006666972767561674,
      "seek": 8612,
      "start": 94.5999984741211,
      "temperature": 0.0,
      "text": " So we could have easily dropped this down to 1K.",
      "tokens": [
        50788,
        407,
        321,
        727,
        362,
        3612,
        8119,
        341,
        760,
        281,
        502,
        42,
        13,
        50910
      ]
    },
    {
      "id": 39,
      "avg_logprob": -0.21291667222976685,
      "compression_ratio": 1.725000023841858,
      "end": 98.31999969482422,
      "no_speech_prob": 0.0006666972767561674,
      "seek": 8612,
      "start": 97.04000091552734,
      "temperature": 0.0,
      "text": " While this is running, you know,",
      "tokens": [
        50910,
        3987,
        341,
        307,
        2614,
        11,
        291,
        458,
        11,
        50974
      ]
    },
    {
      "id": 40,
      "avg_logprob": -0.21291667222976685,
      "compression_ratio": 1.725000023841858,
      "end": 101.55999755859375,
      "no_speech_prob": 0.0006666972767561674,
      "seek": 8612,
      "start": 98.31999969482422,
      "temperature": 0.0,
      "text": " let me give you my quick guide here on intelligence, right?",
      "tokens": [
        50974,
        718,
        385,
        976,
        291,
        452,
        1702,
        5934,
        510,
        322,
        7599,
        11,
        558,
        30,
        51136
      ]
    },
    {
      "id": 41,
      "avg_logprob": -0.21291667222976685,
      "compression_ratio": 1.725000023841858,
      "end": 102.77999877929688,
      "no_speech_prob": 0.0006666972767561674,
      "seek": 8612,
      "start": 101.55999755859375,
      "temperature": 0.0,
      "text": " How much intelligence do you really need",
      "tokens": [
        51136,
        1012,
        709,
        7599,
        360,
        291,
        534,
        643,
        51197
      ]
    },
    {
      "id": 42,
      "avg_logprob": -0.21291667222976685,
      "compression_ratio": 1.725000023841858,
      "end": 104.16000366210938,
      "no_speech_prob": 0.0006666972767561674,
      "seek": 8612,
      "start": 102.77999877929688,
      "temperature": 0.0,
      "text": " to solve any problem?",
      "tokens": [
        51197,
        281,
        5039,
        604,
        1154,
        30,
        51266
      ]
    },
    {
      "id": 43,
      "avg_logprob": -0.21291667222976685,
      "compression_ratio": 1.725000023841858,
      "end": 107.95999908447266,
      "no_speech_prob": 0.0006666972767561674,
      "seek": 8612,
      "start": 104.16000366210938,
      "temperature": 0.0,
      "text": " A big takeaway here is that we now have fine grain control",
      "tokens": [
        51266,
        316,
        955,
        30681,
        510,
        307,
        300,
        321,
        586,
        362,
        2489,
        12837,
        1969,
        51456
      ]
    },
    {
      "id": 44,
      "avg_logprob": -0.21291667222976685,
      "compression_ratio": 1.725000023841858,
      "end": 111.55999755859375,
      "no_speech_prob": 0.0006666972767561674,
      "seek": 8612,
      "start": 107.95999908447266,
      "temperature": 0.0,
      "text": " over the reasoning capabilities of this powerful model.",
      "tokens": [
        51456,
        670,
        264,
        21577,
        10862,
        295,
        341,
        4005,
        2316,
        13,
        51636
      ]
    },
    {
      "id": 45,
      "avg_logprob": -0.21291667222976685,
      "compression_ratio": 1.725000023841858,
      "end": 113.5199966430664,
      "no_speech_prob": 0.0006666972767561674,
      "seek": 8612,
      "start": 111.55999755859375,
      "temperature": 0.0,
      "text": " So this is the way I like to slice it up.",
      "tokens": [
        51636,
        407,
        341,
        307,
        264,
        636,
        286,
        411,
        281,
        13153,
        309,
        493,
        13,
        51734
      ]
    },
    {
      "id": 46,
      "avg_logprob": -0.21291667222976685,
      "compression_ratio": 1.725000023841858,
      "end": 115.5199966430664,
      "no_speech_prob": 0.0006666972767561674,
      "seek": 8612,
      "start": 113.5199966430664,
      "temperature": 0.0,
      "text": " Basically, we have extra small intelligence",
      "tokens": [
        51734,
        8537,
        11,
        321,
        362,
        2857,
        1359,
        7599,
        51834
      ]
    },
    {
      "id": 47,
      "avg_logprob": -0.28739285469055176,
      "compression_ratio": 1.6378378868103027,
      "end": 117.31999969482422,
      "no_speech_prob": 0.03308422118425369,
      "seek": 11552,
      "start": 115.91999816894531,
      "temperature": 0.0,
      "text": " all the way up to 4XL.",
      "tokens": [
        50384,
        439,
        264,
        636,
        493,
        281,
        1017,
        55,
        43,
        13,
        50454
      ]
    },
    {
      "id": 48,
      "avg_logprob": -0.28739285469055176,
      "compression_ratio": 1.6378378868103027,
      "end": 119.62000274658203,
      "no_speech_prob": 0.03308422118425369,
      "seek": 11552,
      "start": 117.31999969482422,
      "temperature": 0.0,
      "text": " You basically will never need this.",
      "tokens": [
        50454,
        509,
        1936,
        486,
        1128,
        643,
        341,
        13,
        50569
      ]
    },
    {
      "id": 49,
      "avg_logprob": -0.28739285469055176,
      "compression_ratio": 1.6378378868103027,
      "end": 121.68000030517578,
      "no_speech_prob": 0.03308422118425369,
      "seek": 11552,
      "start": 119.62000274658203,
      "temperature": 0.0,
      "text": " You basically will never need to push the model",
      "tokens": [
        50569,
        509,
        1936,
        486,
        1128,
        643,
        281,
        2944,
        264,
        2316,
        50672
      ]
    },
    {
      "id": 50,
      "avg_logprob": -0.28739285469055176,
      "compression_ratio": 1.6378378868103027,
      "end": 123.0,
      "no_speech_prob": 0.03308422118425369,
      "seek": 11552,
      "start": 121.68000030517578,
      "temperature": 0.0,
      "text": " this far, this hard.",
      "tokens": [
        50672,
        341,
        1400,
        11,
        341,
        1152,
        13,
        50738
      ]
    },
    {
      "id": 51,
      "avg_logprob": -0.28739285469055176,
      "compression_ratio": 1.6378378868103027,
      "end": 126.04000091552734,
      "no_speech_prob": 0.03308422118425369,
      "seek": 11552,
      "start": 123.0,
      "temperature": 0.0,
      "text": " It is a lot more likely that you'll push into these ranges,",
      "tokens": [
        50738,
        467,
        307,
        257,
        688,
        544,
        3700,
        300,
        291,
        603,
        2944,
        666,
        613,
        22526,
        11,
        50890
      ]
    },
    {
      "id": 52,
      "avg_logprob": -0.28739285469055176,
      "compression_ratio": 1.6378378868103027,
      "end": 127.5199966430664,
      "no_speech_prob": 0.03308422118425369,
      "seek": 11552,
      "start": 126.04000091552734,
      "temperature": 0.0,
      "text": " you know, given that you're solving",
      "tokens": [
        50890,
        291,
        458,
        11,
        2212,
        300,
        291,
        434,
        12606,
        50964
      ]
    },
    {
      "id": 53,
      "avg_logprob": -0.28739285469055176,
      "compression_ratio": 1.6378378868103027,
      "end": 128.9199981689453,
      "no_speech_prob": 0.03308422118425369,
      "seek": 11552,
      "start": 127.5199966430664,
      "temperature": 0.0,
      "text": " an interesting enough problem",
      "tokens": [
        50964,
        364,
        1880,
        1547,
        1154,
        51034
      ]
    },
    {
      "id": 54,
      "avg_logprob": -0.28739285469055176,
      "compression_ratio": 1.6378378868103027,
      "end": 130.52000427246094,
      "no_speech_prob": 0.03308422118425369,
      "seek": 11552,
      "start": 128.9199981689453,
      "temperature": 0.0,
      "text": " where the model actually needs to think.",
      "tokens": [
        51034,
        689,
        264,
        2316,
        767,
        2203,
        281,
        519,
        13,
        51114
      ]
    },
    {
      "id": 55,
      "avg_logprob": -0.28739285469055176,
      "compression_ratio": 1.6378378868103027,
      "end": 131.36000061035156,
      "no_speech_prob": 0.03308422118425369,
      "seek": 11552,
      "start": 130.52000427246094,
      "temperature": 0.0,
      "text": " As, and.",
      "tokens": [
        51114,
        1018,
        11,
        293,
        13,
        51156
      ]
    }
  ],
  "language": "english",
  "duration": 131.05999755859375,
  "timestamp": 1742835697.834472
}